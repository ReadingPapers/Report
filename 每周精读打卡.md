
######### 模版 #############

> [日期]-[总结人]-[论文标题]-[来源]（附带链接，链接可选，时间倒序）\
> 
> > 论文框架图（有助于一眼就能想起论文内容）
> 
> > 论文简述 （一两句话总结精华，切勿过长）
> >

######### 模版 #############


> ##########------------------------------ - 周分界线 - -------------------------------##########

> ==> 时间倒序，本次更新插入位置 <==

> [2024-4-21]-[聂建涛]-[Face2Exp: Combating Data Biases for Facial Expression Recognition](https://openaccess.thecvf.com/content/CVPR2022/papers/Zeng_Face2Exp_Combating_Data_Biases_for_Facial_Expression_Recognition_CVPR_2022_paper.pdf)
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/FaceChain-ImagineID%20Freely%20Crafting%20High-Fidelity%20Diverse%20Talking%20Faces%20from%20Disentangled%20Audio.png">
> 
> >论文简述:
> >
> >文章提出了Meta-Face2Exp框架，用于面部表情识别任务，通过元优化框架利用未标记的FR数据来增强FER。作者观察到FER数据存在类别不平衡问题，以及FR数据和FER数据之间存在分布不匹配。关键部分是基础网络和适配网络通过电路反馈范式不断相互补充，提取去偏置的知识。特别是去偏机制可以有效产生低方差和高均值的准确性。
> 
>  [2024-4-26]-[崔凯]-[Self-supervised Group Meiosis Contrastive Learning for EEG-Based Emotion Recognition]-[arXiv](https://arxiv.org/pdf/2208.00877)
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/assets/127180964/79214a91-1b66-40f4-afd4-b3deafb4dddc">
>
> >论文简述:
> >
> >本文提出了一种基于人类脑电信号刺激一致的自监督群体减数分裂对比学习框架（SGMC）。 SGMC 开发了一种新颖的受遗传学启发的数据增强方法，称为减数分裂。它利用组中脑电图样本之间的刺激对齐，通过配对、交叉交换和分离来生成增强组。该模型采用群体投影仪从相同情感视频刺激触发的群体脑电图样本中提取群体级特征表示。然后采用对比学习来最大化具有相同刺激的增强组的组级表示的相似性。
>
>> [2024-4-25]-[陈银]-[Multi-Task Multi-Modal Self-Supervised Learning for Facial Expression Recognition]-[CVPRW2024](https://arxiv.org/abs/2404.10904), [知乎](https://zhuanlan.zhihu.com/p/694434136),[博客](https://www.yuque.com/g/chenxiaoyin-ymwpp/gxgvdt/fv3qqmox5gazbzxc/collaborator/join?token=kXqqTUspDeJLODAQ&source=doc_collaborator#)
> 
> > 论文框架图
> <img src="https://raw.githubusercontent.com/cyinen/imge/master/20240424203748.png" width="512">
> 
> > 论文简述： 使用多任务多模态自监督的方法做面部表情识别，减少对标注数据的依赖。主要使用了三个损失函数：多模态对比损失对齐不同模态的特征；多模态聚类损失，保留空间输入数据的语义结构；多模态数据重建损失，增强encoder 表征能力。
> 
> [2024-4-24]-程浩-[INTER-MODALITY AND INTRA-SAMPLE ALIGNMENT FOR MULTI-MODAL EMOTION RECOGNITION]-[ICASSP](https://ieeexplore.ieee.org/abstract/document/10446571)\
>论文框架图：
> ![image](https://github.com/ReadingPapers/Report/assets/90198143/174740da-caf0-4678-b196-aaf4a192670a)
> 
> >论文简述：\
> >仍然是AV两个模态的模态对齐的工作，考虑到在在保持模态内部多样性的同时对齐两个模态，模型与cav-mae比较相似，但在对比阶段多了模态内的对比和一个监督对比。总体会有三个对比损失。



> [2024-4-21]-[聂建涛]-[FaceChain-ImagineID: Freely Crafting High-Fidelity Diverse Talking Faces from Disentangled Audio](https://arxiv.org/pdf/2403.01901.pdf)
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/FaceChain-ImagineID%20Freely%20Crafting%20High-Fidelity%20Diverse%20Talking%20Faces%20from%20Disentangled%20Audio.png">
> 
> >论文简述:
> >
> >这篇文章提出了一个新的说话人脸生成任务，即直接从音频中想象出符合音频特征的多样化动态说话人脸，而常规的该任务需要给定一张参考人脸。具体来说，该任务涉及到两个核心的挑战，首先如何从音频中解耦出说话人的身份（性别、年龄等语义信息以及脸型等结构信息）、说话内容以及说话人传递的情绪，其次是如何根据这些信息生成多样化的符合条件的视频，同时保持视频内的一致性。文章首先挖掘了三个人脸相关要素之间的联系，设计了一个渐进式音频解耦模块，以此降低解耦难度，并且提高了各个解耦因子的准确性。同时基于Latent DIffusion Models (LDMs)提出了一个可控一致帧生成模块，继承了LDMs的多样化生成能力，并设计了相应模块将音频中的信息准确的表达在生成的动态人脸上，缓解了LDMs可控性差的局限。

> > [2024-4-21]-[张雪松]-[Diagnosing the Environment Bias in Vision-and-Language Navigation](https://dl.acm.org/doi/abs/10.5555/3491440.3491564)    
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/assets/68311986/e9bf1f83-5c80-4f0e-8796-ada1b541255a">   
> >论文简述:
> >
> >之前的研究发现vln模型倾向于拟合训练集，在unseen场景中的表现差，但是这种关于环境bias的发现没有被清楚的分析和验证。在这项工作中，作者通过环境重新分割设计了新的诊断实验和特征替换，探讨了环境偏差的可能原因。作者观察到，影响代理模型并导致结果中的环境偏见的既不是语言，也不是底层的导航图，而是通过ResNet特征传达的low-level视觉表现，作者还注意到有噪声但更高层次的语义特征可以有效地减少性能差距，同时在不同的VLN数据集上保持适度的性能。根据这一观察结果，作者探索了几种包含较少的低级视觉信息的语义表示，因此，利用这些特征学习到的代理可以更好地推广到看不见的测试环境中。
 
>> [2024-4.21]-[陈银]-[PTH-Net: Dynamic Facial Expression Recognition without Face Detection and Alignment]-[IEEE]([PTH-Net: Dynamic Facial Expression Recognition without Face Detection and Alignmen](https://www.techrxiv.org/users/711863/articles/702382-pth-net-dynamic-facial-expression-recognition-without-face-detection-and-alignment), [知乎](https://zhuanlan.zhihu.com/p/693939726)
> 
> > 论文框架图
> <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/assets/56926538/7f4e8582-97a7-4908-be87-a1d79eae0b57">
> 
> > 论文简述： 本文提出了一个金字塔时间层次网络（PTH-Net）， 可以直接应用于原始视频，无需进行面部检测和对齐。传统的先检测再对齐的方法，可能会丢失一些有用的关键信息， PTH-Net优势在于它在特征层面区分背景和目标，保留了更多关键信息，并且是一个端到端的网络，更加灵活。
> 
> >
> [2024-4-20]-[何艺超]-[COGMEN: COntextualized GNN based Multimodal Emotion recognitioN]-[NAACL 2022](https://arxiv.org/abs/2205.02455)-[知乎](https://zhuanlan.zhihu.com/p/692989727)
>
> >论文简述：在多人交互式对话多模态任务中，考虑到说话者与说话者之间、说话者与说话者本身会相互影响，作者提出使用Transformer+RGNN建模一段对话中的复杂依赖（局部（local）和全局（global）信息），并在IEMOCAP和MOSEI数据集中取得了SOTA结果。
> 
>  [2024-4-20]-[张宇]-[TransFER: Learning Relation-aware Facial Expression Representations with Transformers](https://arxiv.org/abs/2108.11116)
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/TransFER.png">
>
> >论文简述:
> >
> >本文指出不同的面部表情可能在某些部位存在相同的动作，而使用标准的Transformer对Patch相同注意力计算时候会误导模型的判断，固提出了MAD模块随机丢弃某些注意力图（对一个图片多次提取得到多个注意力图随机设置一些branch为0），并且利用MASD模块（包含MAD）随机丢弃自我注意模块，通过对标准的Transformer结构做改动实现对FER任务的适应。
> 
>  [2024-4-20]-[崔凯]-[EEG Conformer: Convolutional Transformer for EEG Decoding and Visualization]-[2023 IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING](https://ieeexplore.ieee.org/document/9991178/)
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/assets/127180964/085e11fe-c4be-4a31-84f1-20dfab1b2738">
>
> >论文简述:
> >
> >本文提出了一种紧凑的卷积Transformer，名为 EEG Conformer，将局部和全局特征封装在统一的 EEG 分类框架中。具体来说，卷积模块学习整个一维时间和空间卷积层的低级局部特征。自注意力模块直接连接以提取局部时间特征内的全局相关性。随后，基于全连接层的简单分类器模块来预测脑电图信号的类别。为了增强可解释性，还设计了一种可视化策略，将类激活映射投影到大脑地形上。

##########------------------------------ - 周分界线 - -------------------------------##########

> > [2024-4-14]-[张雪松]-[Depth-guided adain and shift attention network for vision-and-language navigation](https://ieeexplore.ieee.org/abstract/document/9428422)
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/assets/68311986/bde42231-98ab-44dc-a116-66e4e5d10edd">
> 
> >论文简述:
> >
> >本文首次在VLN任务中显示地引入深度信息，从而提高智能体在unseen场景的鲁棒性。并进一步提出了一个转移注意力模块来模拟注意地图中的相对方向。这种简单增加深度信息的方式并没有显著提升agent的成功率，后续[研究](https://ieeexplore.ieee.org/abstract/document/9811921)对这种方法进行了改进。值得一提的是，和当时的rgb图像一样，本文的深度图像也是通过Resnet152（在imagenet上预训练）提取。最近的[论文](https://github.com/cshizhe/onav_rim)提出使用Resnet50（在Gibson上预训练），但是这种方式主要应用在连续的导航任务中。


> [2024-4-14]-[聂建涛]-[Fine-Grained Face Swapping via Regional GAN Inversion](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Fine-Grained_Face_Swapping_via_Regional_GAN_Inversion_CVPR_2023_paper.pdf)
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/Fine-Grained%20Face%20Swapping%20via%20Regional%20GAN%20Inversion.png">
> 
> >论文简述:
> >
> >在这篇文章中，提出了一种新的人脸交换框架E4S，该框架明确地分解了每个人脸成分的形状和纹理，并将人脸交换重新定义为一个简化的纹理和形状交换问题。同时为了寻求这种解纠缠以及高分辨率和高保真，提出了一种新的区域GAN反演(RGI)方法。 具体来说，多尺度mask引导编码器将输入脸投射到每个区域样式的代码中。 此外，mask引导注入模块使用样式码根据给定的masks操作生成器中的特征映射。

> [2024-4-13]-[陈银]-[Landmark-based Facial Self-supervised Learning for Face Recognition]-[CVPR2024](https://arxiv.org/abs/2403.08161) 
> <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/assets/56926538/5c958a29-8840-4c04-b847-5ded7c69087b">
> 
>  论文简述：
> 
> 本文提出了一种名为LAFS（基于landmark的面部自监督学习）的自监督学习策略，利用未标记的面部图像来学习并提高人脸识别性能。框架使用的是 DINO的学习方法，与在预训练中使用随机裁剪的图像块构建增强不同，LAFS利用通过提取的面部landmark 提取 patchs, 并引入了两种Landmark-based Augmantions（随机打乱 patchs 的顺序，和加入扰动），显著提升了人脸识别的性能。
> 
> [2024-4-13]-[张宇]-[Rethinking the Learning Paradigm for Dynamic Facial Expression Recognition]-[CVPR 2023](https://ieeexplore.ieee.org/document/10204167)
><img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/M3DFEL.png">
> >论文简述:
> >
> >本文的思想在于处理动态表情一系列视频帧序列时，认为将非目标帧视为嘈杂帧太过绝对，应当被归结为弱监督问题，所以提出了多实例学习（MIL）来处理不准确的标签，利用3DCNN提取特征，利用动态长期实例聚合模块（DLIAM）模拟时间关系并且动态聚合实例，从而能解决不同样本中目标帧出现的时间长短不一造成的影响，在DFER任务中引入多实例学习的方法属于比较有新意的学习范式，该方法在当时DFER数据集上达到了SOTA。
>
>  [2024-4-13]-[崔凯]-[A Multitask Framework for Emotion Recognition Using EEG and Eye Movement Signals with Adversarial Training and Attention Mechanism]-[2023 IEEE International Conference on Bioinformatics and Biomedicine](https://ieeexplore.ieee.org/abstract/document/10385505)
>
> >论文简述:
> >在本文提出了一种具有对抗性训练和注意机制（ATAM）的多任务框架，用于使用脑电图和眼动信号进行情绪识别。对抗性训练方案是通过最大化相同模态内的互信息损失并最小化不同模态之间的余弦相似性损失来设计的。通过这种设计，所提出的 ATAM 不仅保留了脑电图和眼动信号中的情感信息，而且还保留了这两种模式之间的互补特性。再使用注意力机制自适应地融合多模态特征。
>
> [2024-4-12]-[何艺超]-[Unsupervised Feature Learning for Speech Emotion Recognition Based on Autoencoder]-[Electronics](https://www.mdpi.com/2079-9292/10/17/2086)
>
> >论文简述:
> >作者设计了一种语音情感数据分割和增强方法，通过不同的时间偏移来分割情感语音数据，使得每个分割的数据集在情感类别上分布更加均衡。分析了不同自编码器包括简单自编码器、去噪自编码器和对抗自编码器在IEMOCAP数据集上的分类结果，并发现对抗自编码器模型在特征细节重建方面表现更好。


> [2024-4-11]-[程浩]-[Beyond Text: Frozen Large Language Models in Visual Signal Comprehension]-[CVPR 2024](https://arxiv.org/pdf/2403.07874.pdf)
>
> >论文简述:\
> >idea为将图片视作一个外语,从而利用冻结的LLM去做多模态任务,如VQA,Image Caption. 具体来说，这篇论文提出V2L模型, 类似一个分词器,把图片转换到LLM的"token空间" (即把图片转化成LLM词汇表里面的Token), 然后把这些Token输入到冻结的LLM模型,从而可以让LLM模型理解图片. 接着在每种下游任务上都有小设计,但也都基于llm对图片的理解.

##########------------------------------ - 周分界线 - -------------------------------##########

> [2024-4-7]-[聂建涛]-[Reinforced Disentanglement for Face Swapping without Skip Connection](https://arxiv.org/ftp/arxiv/papers/2307/2307.07928.pdf)
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/Reinforced%20Disentanglement%20for%20Face%20Swapping%20without%20Skip%20Connection.png">
> 
> >论文简述:
> >
> >这篇论文的主要方法是提出了一个新的框架，通过网络结构和正则化损失的视角来解决之前工作中ID和非ID表示纠缠的问题。具体来说，作者提出了两个编码器，分别捕获目标图像的面部非ID属性和语义级非面部属性，每个编码器都被设计为专门捕获其期望的表示，而不需要相互妥协。同时，使用目标身份移除损失和几个非ID属性保留损失来补偿由于缺少跳过连接而丢失的细节。

> > [2024-4-6]-[张雪松]-[DELAN: Dual-Level Alignment for Vision-and-Language Navigation
by Cross-Modal Contrastive Learning](https://arxiv.org/html/2404.01994v1)
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/assets/68311986/36e3c921-8734-4165-a47c-bf33fbec4ec9">
>
> >论文简述:
> >
> >本文在两个level上将跨模态的对比学习应用到VLN任务中。具体来说，作者在视觉和文本指令在融合前，进行指令-历史level的对齐以及landmark-observation level上的对比学习，从而进行语义关联。实验在R2R，RxR等lowlevel的指令数据集上进行。

> [2024-4-5]-[崔凯]-[Spatial-frequency convolutional self-attention network for EEG emotion recognition](https://www.sciencedirect.com/science/article/abs/pii/S1568494622001843)
>
> >论文简述：
> >
> >提出了一种空间频率卷积自注意力网络（SFCSAN）来整合脑电信号空间和频域的特征学习。在该模型中，采用频带内自注意力来学习每个频带的频率信息，频带间映射进一步将它们映射到最终的注意力表示中，以学习它们的互补频率信息。此外，还使用并行卷积神经网络（PCNN）层来挖掘脑电图信号的空间信息。
> 
> [2024-4-5]-[何艺超]-[VIVIT：A video vision transformer](https://arxiv.org/abs/2103.15691)
>
> >论文简述：
> >
> >相对general的文章，情感计算领域可用。其主要提出了4种图像ViT扩展到视频的范式，分别是1.embedding to tokens中的token拼接 2.空间encoder输出至时间encoder 3.修改block结构，先计算空间MSA再计算时间MSA 4.修改block结构，将多头注意力分解成空间和时间两部分。除此之外，还提出了图像预训练ViT模型迁移到上述VideoViT上的方法。

> [2024-4-5]-[张宇]-[Intensity-Aware Loss for Dynamic Facial Expression Recognition in the Wild](https://arxiv.org/abs/2208.10335)
> >
> > 论文框架图
> <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/GCA%2BIAL.png">
>
> > 论文简述：
> 针对于DFER任务大多数据集忽略了同一类样本之间表达强度的差异这一问题，本文设计了一个新颖的全局注意力块GCA（重新缩放通道抑制不相关的目标）和一种新的强度感知损失IAL，从而帮助网络从硬标签DFER数据集（DFEW,FERv39K，MAFW等）获得更多的信息，减少类内差异，增加类间差异，并在当时在各个流行的DFER数据集上达到了SOTA。


> [2024-4-5]-[陈银】-Cluster-Guided Asymmetric Contrastive Learning for Unsupervised Person Re-Identification, [TIP](https://ieeexplore.ieee.org/document/9775582/)
> >
> > 论文框架图
> <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/assets/56926538/65c9f78c-3dd6-451d-97d4-77a68c9d4d64">
>
> > 论文简述：
> 行人重识别的基于实例的对比学习容易让模型学习到RGB颜色特征上去，对任务造成干扰。本文提出了通过利用聚类信息在对比学习框架中指导特征学习，有效地利用了不同数据增强视图之间的不变性，学习到了超越RGB主导的有效特征。此外还提出来一种聚类信息精细化的方法，进一步提高模型的性能。


> [2024-4-5]-[王会雅]-[Spontaneous Facial Micro-Expression Recognition using 3D Spatiotemporal Convolutional Neural
Networks](https://arxiv.org/pdf/1904.01390) ,[知乎](https://zhuanlan.zhihu.com/p/690021847)
>>
>>论文简述：
>>作者提出了两种三维卷积神经网络（MicroExpSTCNN和MicroExpFuseNet）用于视频微表情识别，MicroExpSTCNN考虑的是面部全部区域，而MicroExpFuseNet考虑的则是眼睛和嘴巴区域。通过实验可知，MicroExpSTCNN的性能要优于MicroExpFuseNet，也就是除了眼睛和嘴巴区域外，其他一些显著的面部区域也有助于微表情识别
>>

> [2024-4-1]-[聂建涛]-[DPE: Disentanglement of Pose and Expression for General Video Portrait Editing]-[Paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Pang_DPE_Disentanglement_of_Pose_and_Expression_for_General_Video_Portrait_CVPR_2023_paper.pdf)
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/DPE%EF%BC%9A%20Disentanglement%20of%20Pose%20and%20Expression.png">
> 
> >论文简述:
> >
> >本文提出了一种新的自监督解耦框架，用于在不需要3DMM和配对数据的情况下解耦姿态和表达。通过将人脸投影到一个可分离姿态运动和表达运动的强大可编辑潜在空间中，该方法可以通过附加操作方便地在该空间中执行姿势或表情转移。此外，该方法还提出了一种双向循环训练策略，通过精心设计的约束来保证解耦的实现。通过这种训练策略，可以在没有配对数据的情况下实现姿态和表达的解耦。

> 2024-4-1-程浩-MAViL: Masked Audio-Video Learners-[NuerIPS 2023](https://proceedings.neurips.cc/paper_files/paper/2023/file/40b60852a4abdaa696b5a1a78da34635-Paper-Conference.pdf)
>
> <img width="512" alt="image" src='https://github.com/ReadingPapers/Report/assets/90198143/54f17edc-359c-4b85-a169-a112d981b8e5'>
> 
> >论文简述: \
> > 在MAE和对比学习结合的CAV-MAE的基础上，借鉴知识蒸馏的思想，把原始MAE的代理任务由重建raw data转变为重建教师模型的表征。State-1对教师模型的训练用raw data作为target，State-2利用教师模型来对学生模型进行训练，两阶段掩码率都可达到80%。最终在Event Classification，Retrieval和单模态的任务上都达到SOTA。


> ##########------------------------------ - 周分界线 - -------------------------------##########

> >[2024-3-30]-[崔凯]-[Dynamic Confidence-Aware Multi-Modal Emotion Recognition](https://ieeexplore.ieee.org/document/10349925)
> 
> >论文简述
> >
> >提出了一个具有注意机制的多通道 LSTM 特征网络，可以同时对齐异构多模态特征并自适应预测时间层面的情绪不确定性。同时又设计了一种基于真实类别概率的置信回归网络来估计模态层面的情绪预测不确定性，它允许通过置信权重进行更可解释和更可靠的多模式情感融合识别。 在进行融合网络的优化过程中，采用自定进度学习来提高所提出模型的鲁棒性。


> > [2024-3-30]-[王会雅]-[Micro-expression recognition based on 3D flow convolutional neural network](https://www.researchgate.net/profile/Yandan-Wang-2/publication/328818759_Micro-expression_recognition_based_on_3D_flow_convolutional_neural_network/links/5be63e4a299bf1124fc78539/Micro-expression-recognition-based-on-3D-flow-convolutional-neural-network.pdf)
>
> > 论文简述：
> >
> > 本文提出了具有12层（5对卷积层和池化层，一个完全连接层，一个softmax层）的3D FCNN模型，将光流（动态信息）和标准灰度帧（外观信息）作为网络的输入数据，对所有池化层的输入应用零填充，并使用更小的三维卷积核来表示局部区域的微小变化。
> > 
> > [2024-3-29]-[张宇]-[NR-DFERNet: Noise-Robust Network for Dynamic Facial Expression Recognition](https://arxiv.org/abs/2206.04975)
> > <img width="512" alt="image" src=https://github.com/ReadingPapers/Report/blob/main/Images/NR-DFERNet.png>
> > 
> >论文简述:
> >
> >在处理DFER任务上，本文没有遵循直接对序列整体特征的学习这一方式，而是提出了可以区分噪声帧和关键帧的网络NR-DFERNet，设计了一个动态-静态融合模块（DSF）以学习更加有效的特征，并且在时空阶段引入新的动态token(DCT)抑制无关帧（非neutral序列上过多neutral序列），最后通过SF过滤器对判别结果进行加强，在DFEW和AFEW上达到了当时的SOTA。
> 
> > [2024-3-29]-[何艺超]-[Tailor Versatile Multi-modal Learning for Multi-label Emotion Recognition](https://arxiv.org/abs/2201.05834)
>
> > 论文简述：
> >
> > 提出用于多标签识别的TAILOR，通过单模态特征抽取（Transformer编码器）、对抗多模态提取公有和私有表征（公有特征用于加噪、私有特征用于判别器识别原模态从而获得不同模态之间的共性和特有表示）、标签-模态对齐三个步骤，旨在细化多标签多模态表示。

> > [2024-3-28]-[张雪松]-[Are You Looking? Grounding to Multiple Modalities in
Vision-and-Language Navigation](https://aclanthology.org/P19-1655/)
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/assets/68311986/dcdb9cef-07e4-472d-9f3b-a304c25e37b1">
> 
> >论文简述:
> >
> >本文在调查VLN的SOTA方法时有两个惊人的发现，(1)视觉模态对VLN任务会伤害模型导航的泛化表现，不使用视觉输入（视觉模态置为0）模型在unseen场景下的表现更好。（2）作者使用MoE（mixture-of-experts）的方法解耦视觉模态，路径结构和目标检测的特征，鼓励模型对齐每一种模态，并极大的提高了模型的导航表现。（本文来自2019ACL，视觉模态使用ResNet提取的特征，且仅在R2R数据集上进行实验，作者在文中指出类似的情况在VQA任务也有出现）

> 
>>[2023-3-27]-[陈银]-[Vi2CLR: Video and Image for Visual Contrastive Learning of Representation](https://doi.org/10.1109/ICCV48922.2021.00153.) ,[博客](https://demo.hedgedoc.org/-UWwKvnOSpmIHMu9bggyLQ)
>>
>><img width="512" alt="image" src="https://github.com/ReadingPapers/Report/assets/56926538/71447695-027c-4e76-94a7-4cdf52d6c2f2">
>
>>论文简述：
>>
>>核心还是使用对比学习，采用聚类的方法来构建正负样本对。
>>以往的对比学习正样本对构建都是通过不同的 view, 使用momentum Encoder来提取特征，或者采用memory bank。而本文采用了聚类的形式，将 Image 和 Video Encoder 输出的 concat 在一起，然后在特征空间做聚类。


> ##########------------------------------ - 周分界线 - -------------------------------##########

> > [2024-3-24]-[聂建涛]-[High-resolution face swapping via latent semantics disentanglement]-[Paper](https://arxiv.org/pdf/2203.15958.pdf)
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/High-resolution%20face%20swapping%20via%20latent%20semantics%20disentanglement.png">
> 
> >论文简述:
> >
> >本文提出了一种基于预训练的GAN模型的高分辨率面部交换方法。该方法通过分离源图像和目标图像在生成器中的潜在语义，明确地分离了潜在的语义，从浅层获得结构属性，从深层获得外观属性，实现结构属性和外观属性的单独处理。

>[2023-3-24]-[何艺超]-[MERBench: A Unified Evaluation Benchmark for Multimodal Emotion Recognition]-[Paper](https://arxiv.org/abs/2401.03429),[知乎](https://zhuanlan.zhihu.com/p/688678130)
>
> >论文简述：
> >
> >多模态情感识别领域模型方法研究的不一致性（数据集不一致，模型不一致，实验设置不一致）带来的难以公平评测不同模型能力的问题，后来的研究者难以轻松地选择适合自己的模型研究，这种不一致限制了多模态情感识别的发展，于是制作了MER2023 benchmark数据集，和一套模型测试方法，并在众多已有的模型上测试，给出了多指标的测试结果，方便后期工作者了解不同模型的性能。

> [2023-3-23]-[崔凯]-[PR-PL: A Novel Prototypical Representation based Pairwise Learning Framework for Emotion Recognition Using EEG Signals]-[Paper](https://ieeexplore.ieee.org/abstract/document/10160130)， [GitHub](https://github.com/KAZABANA/PR-PL)
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/assets/127180964/2564838f-603d-4d52-9bab-86edecee3564">
> 
> >论文简述：
> >
> >我们提出了一种新颖的迁移学习框架，该框架具有基于原型表示的成对学习（PR-PL）。学习区分性和广义的脑电图特征以揭示个体之间的情绪，并将情绪识别任务制定为成对学习，以提高模型对噪声标签的耐受性。更具体地说，开发了一种原型学习来编码脑电图数据固有的与情感相关的语义结构，并在考虑源域和目标域的特征可分离性的情况下将个体的脑电图特征对齐到共享的公共特征空间。

> [2023-3-23]-[王会雅]-[Facial micro-expression spotting and recognition using time contrasted feature with visual memory]-[Arxiv](https://arxiv.org/pdf/1902.03514) 
>
> >论文简述:
> >
> >本文使用时间网络与使用DeepLab v1的空间网络给出的更高层次微表情图像编码进行对比得出微表情相关的特征，GRU模块利用这种对比特征来记忆和预测微表情片段时间框架中的微表情的类别和强度，此方法不仅提高了从不明显的面部运动中识别微表情的能力，相对于传统识别方法还具有更高的准确性。
> >
> >
> [2023-3-23]-[陈银]-[Unmasked Teacher: Towards Training-Efficient Video Foundation Model]-[Paper](https://arxiv.org/abs/2303.16058)， [GitHub](https://github.com/OpenGVLab/unmasked_teacher) 
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/assets/56926538/80a93820-cdf3-4ba7-b690-23374bf454f0">
> >
> >论文简述:
> >
> > 基于像素重建的 VideoMAE预训练方式学习到的特征与下游任务存在较大的 gap, 而 CLIP 基于图像文本预训练包含更 hifh-level 的语义信息。本文提出一个新的框架，将 CLIP 预训练的 Image encoder 作为 Teacher, 指导 video model (Student)的学习， 使student model 既具备了时空捕捉能力，又学到了更接近下游任务的 high-level 知识。在下游的多个任务上达到 sota, 并且 student的性能超过了 teacher.

> [2023-3-22]-[张雪松]-[Mind the Gap: Improving Success Rate of Vision-and-Language
Navigation by Revisiting Oracle Success Routes]-[Paper](https://researchers.mq.edu.au/en/publications/mind-the-gap-improving-success-rate-of-vision-and-language-naviga)， [知乎](https://zhuanlan.zhihu.com/p/688058784](https://zhuanlan.zhihu.com/p/687858582))
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/assets/68311986/74ed96aa-e4c5-417b-aaac-0dc0fdd1ca64">
> 
> >论文简述:
> > 本文基于已有方法提供的指令和轨迹(不是智能体主动采样得到的)，设计了一个基于tansformer多模块的框架，旨在找到轨迹中和指令描述匹配的目标位置，从而提高导航成功率，减少SR和OSR之间的差距。

> [2023-3-20]-[张宇]-[EmoCLIP: A Vision-Language Method for Zero-Shot Video Facial Expression Recognition]-[Arxiv](https://arxiv.org/abs/2310.16640) [知乎](https://zhuanlan.zhihu.com/p/688058784)
>
> >论文简述:
> >
> >在CLIP的基础上进行改进实现对DFER进行zero-shot，加入ViT模型模拟时间维度，首个在DFER上提出使用样本级视频-文本数据集进行训练，在其余流行的DFER数据集上进行zero-shot进行测试，区别于VLM在DFER上CLIPER和DFER-CLIP的类级学习范式，微调了CLIP的image端和text端，使其对人脸表情以及表情描述文本更加敏感。

> [2023-3-19]-[程浩]-[Self-supervised Cross-modal Pretraining for Speech Emotion Recognition and Sentiment Analysis]-[EMNLP 2022](https://aclanthology.org/2022.findings-emnlp.375.pdf)
>
> >论文简述:\
> >利用Textual modality现成的一个类Bert但更General的模型RoBERTa和Acoustic modality中一个较General的模型HuBERT分别作为两个模态的Embedding, 接着在特征层面进行Mask and Reconstruction的学习，模型对于两个模态分别设计了两个Transformer模块，并利用Cross-attention机制进行模态间的特征融合，最终在IEMOCAP和MOSEI两个数据集上达到了SOTA。

> ##########------------------------------ - 周分界线 - -------------------------------##########

>  [2023-3-17]-[崔凯]-[TSception: Capturing Temporal Dynamics and Spatial Asymmetry From EEG for Emotion Recognition]-[ieee.org](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9762054)
> >[知乎](https://zhuanlan.zhihu.com/p/689915437/preview?comment=0&catalog=0)
>

> > 论文简述
> >
> > 针对EEG信号的high temporal resolution和the asymmetric spatial activations属性，作者设计了具有多尺度卷积核Dynamic Temporal Layer以及Asymmetric Spatial Layer，以此来捕捉EEG的时间动态性和空间不对称性，实现更准确和更具备泛化的情绪识别。

> [2023-3-13]-[陈银]-[A $^3$ lign-DFER: Pioneering Comprehensive Dynamic Affective Alignment for Dynamic Facial Expression Recognition with CLIP]-[Arxiv](https://arxiv.org/abs/2403.04294) [知乎](https://zhuanlan.zhihu.com/p/686840722)
>  

> > 论文简述：
> > 
> > 在CLIP的基础上，提出多个模块，从affective, dynamic和bidirectional三个角度实现了动态情感对齐，达到了较高的performance。相比于之前的prompt learning ， 把text embeding增加了一个时间上的维度。在clip-based模型里达到了SOTA。







