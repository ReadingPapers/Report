
######### 模版 #############

> [日期]-[总结人]-[论文标题]-[来源]（附带链接，链接可选，时间倒序）
>  
> > 论文框架图（有助于一眼就能想起论文内容）
> 
> > 论文简述 （一两句话总结精华，切勿过长）
> >

######### 模版 #############

> 时间倒序，本次更新插入位置 <==

> [2023-3-22]-[张雪松]-[Mind the Gap: Improving Success Rate of Vision-and-Language
Navigation by Revisiting Oracle Success Routes]-[Paper](https://researchers.mq.edu.au/en/publications/mind-the-gap-improving-success-rate-of-vision-and-language-naviga)， [知乎](https://zhuanlan.zhihu.com/p/688058784](https://zhuanlan.zhihu.com/p/687858582))
>
> >论文简述:
> >
> > 本文基于已有方法提供的指令和轨迹(不是智能体主动采样得到的)，设计了一个基于tansformer多模块的框架，旨在找到轨迹中和指令描述匹配的目标位置，从而提高导航成功率，减少SR和OSR之间的差距。

> [2023-3-20]-[张宇]-[EmoCLIP: A Vision-Language Method for Zero-Shot Video Facial Expression Recognition]-[Arxiv](https://arxiv.org/abs/2310.16640) [知乎](https://zhuanlan.zhihu.com/p/688058784)
>
> >论文简述:
> >
> >在CLIP的基础上进行改进实现对DFER进行zero-shot，加入ViT模型模拟时间维度，首个在DFER上提出使用样本级视频-文本数据集进行训练，在其余流行的DFER数据集上进行zero-shot进行测试，区别于VLM在DFER上CLIPER和DFER-CLIP的类级学习范式，微调了CLIP的image端和text端，使其对人脸表情以及表情描述文本更加敏感。

> [2023-3-19]-[程浩]-[Self-supervised Cross-modal Pretraining for Speech Emotion Recognition and Sentiment Analysis]-[EMNLP 2022](https://aclanthology.org/2022.findings-emnlp.375.pdf)
>
> >论文简述:\
> >利用Textual modality现成的一个类Bert但更General的模型RoBERTa和Acoustic modality中一个较General的模型HuBERT分别作为两个模态的Embedding, 接着在特征层面进行Mask and Reconstruction的学习，模型对于两个模态分别设计了两个Transformer模块，并利用Cross-attention机制进行模态间的特征融合，最终在IEMOCAP和MOSEI两个数据集上达到了SOTA。


>  [2023-3-17]-[崔凯]-[TSception: Capturing Temporal Dynamics and Spatial Asymmetry From EEG for Emotion Recognition]-[ieee.org](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9762054)
>

> > 论文简述
> >
> > 针对EEG信号的high temporal resolution和the asymmetric spatial activations属性，作者设计了具有多尺度卷积核Dynamic Temporal Layer以及Asymmetric Spatial Layer，以此来捕捉EEG的时间动态性和空间不对称性，实现更准确和更具备泛化的情绪识别。

> [2023-3-13]-[陈银]-[A $^3$ lign-DFER: Pioneering Comprehensive Dynamic Affective Alignment for Dynamic Facial Expression Recognition with CLIP]-[Arxiv](https://arxiv.org/abs/2403.04294) [知乎](https://zhuanlan.zhihu.com/p/686840722)
>  

> > 论文简述：
> > 
> > 在CLIP的基础上，提出多个模块，从affective, dynamic和bidirectional三个角度实现了动态情感对齐，达到了较高的performance。相比于之前的prompt learning ， 把text embeding增加了一个时间上的维度。在clip-based模型里达到了SOTA。







