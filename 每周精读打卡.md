######### 模版 #############

> week1 同一周在同一个level的bar上
> 
> [日期]-[first]-[论文标题]-[来源]（附带链接，链接可选，时间倒序）\
> >
> > 论文框架图（有助于一眼就能想起论文内容），宽度设置为512
> >
> > 论文简述 （一两句话总结精华，切勿过长）
> 
> [日期]-[second]-[论文标题]-[来源]（附带链接，链接可选，时间倒序）\
> >
> > 论文框架图（有助于一眼就能想起论文内容）
> >
> > 论文简述 （一两句话总结精华，切勿过长）
> 

> week2
> 
######### 模版 #############

## 必须是【全文】精读的论文，如实记录

> ==> 时间倒序，本次更新插入位置 <==
>
>>[2025-03-29]-张宇-[PE-CLIP: A Parameter-Efficient Fine-Tuning of Vision Language Models for Dynamic Facial Expression Recognition]-[arxiv]-(https://arxiv.org/abs/2503.16945)
><img width='512' alt='image' src="https://raw.githubusercontent.com/cyinen/imge/master/20250328095606.png"/>
>
>论文简述：目前用clip做dfer任务的通常需全参数微调，会破坏原有参数导致严重记忆遗忘问题。本篇工作也就是针对这一问题，设计了一种参数高效微调框架，通过时序动态adapter（TDA）和共享adapter（ShA）增强CLIP的时序建模能力与跨模态对齐。TDA通过GRU的动态缩放机制自动学习时序变化（也就是通过常规的adapter（但其中的激活函数用的是GELU）在clip的内部将多帧融合，不需要再经过时序模型），ShA以轻量瓶颈结构同步优化视觉与文本特征（这个adapter通过架构图也能看出是共享参数的）。最终通过面部动作单元（AU）的语义描述作为监督信号实现分类。最终实验表明，PE-CLIP在DFEW和FERV39K数据集上只有9M可训练参数（<6%总参数量），而且表现也不错，DFEW上的WAR达到了75.35%（目前在FER任务上对clip进行高效参数微调是一个不错的想法）。
>
>>[2025-03-29]-陈银-[CoPL:Parameter-Efficient Collaborative Prompt Learning for Audio-Visual Tasks] MM 2024
><img width='512' alt='image' src="https://raw.githubusercontent.com/cyinen/imge/master/20250328095606.png"/>
>
>论文简述： 将预训练的uni-modal模型迁移到多模态的任务能够减少训练的cost, 但是现有的方法主要关注的是多模态的融合，忽视了忽视模态特定的微调环节。为了解决这个，本文提出了高效的协作式提示学习（CoPL）来微调 uni-modal和多模态特征，包括模态特定的prompt与任务耦合的prompts, 并通过prompt bank实现基于实例的特征提取。最后使用MSE做模态对齐。在下游的几个音视频任务上以更少的参数实现了更好的性能。
>
>>[2025-03-22]-陈银-[Align before Adapt: Leveraging Entity-to-Region Alignments for Generalizable Video Action Recognition] CVPR 2024
><img width='512' alt='image' src="https://raw.githubusercontent.com/cyinen/imge/master/20250312181304.png"/>
>
>论文简述：本文提出了 Align Befor Adapt， 在Adapt 视频表征学习之前，先利用每一帧与实体-区域对齐。通过对区域感知图像嵌入与离线构建的文本语料库进行匹配，实现了实体对齐。利用已对齐的实体，将其文本嵌入作为查询输入到基于Transformer的视频适配器中，这有助于将视频中最重要的实体语义提取为向量表示。<font color="#366092">该范式在适配过程中重用了视觉语言预训练（VLP）的视觉-语言对齐机制，并尝试通过底层实体来解释动作。这种方法通过弥合复杂活动语义的差距来促进对动作的理解，特别是在面对不熟悉或未见过的类别时。</font>
>
>>[2025-03-22]-张雪松-[ETPNav: Evolving Topological Planning for
Vision-Language Navigation in Continuous
Environments](https://ieeexplore.ieee.org/abstract/document/10495141/)（TPAMI2024）
><img width="512" alt="image" src="https://github.com/user-attachments/assets/3d838dcf-6172-4b37-9737-f535762dc538" />
>
> 论文简述：连续和离散环境的VLN任务最主要的区别在于。连续环境中的agent无法依赖prior的graph map的结构进行high-level的导航，只能进行low-level的导航。为此，[之前的工作](https://github.com/YicongHong/Discrete-Continuous-VLN)已经设计了一个Candidate Waypoints Predictor来预测下一个可导航点，使得agent在连续的环境中来执行highlevel的导航动作。本文基于此，构建一个graphmap来预测下一个最可能的可导航点，然后使用一个参数化的控制器执行低水平的action从而实现导航任务。除此之外，（1）之前的Waypoints Predictor都是基于RGB-D的，而本文发现仅用depth效果更好。（2）本文对避障进行了简单的处理。最后，本文大幅提升了agent的导航表现，为连续环境下的VLN任务提供了一个strong baseline。

>>[2025-02-23]-张雪松-[NaVILA: Legged Robot Vision-Language-Action-Model for Navigation](https://navila-bot.github.io/)
><img width="512" alt="image" src="https://github.com/user-attachments/assets/37d1283a-2e83-4693-a6d2-b6ccafce7cf6" />
> 
> 论文简述：本文（应该是首次）将视觉语言导航任务成功落地到基于腿式机器人（宇树四足Go2和人形H1机器人）的室内室外导航场景。本文的方法分为两阶段，第一阶段是预训练视觉语言模型VILA（包含视觉encoder，一个projector，和一个LLM），其中预训练资源包括仿真环境中的VLN数据集，处理后的youtube中的视频，具身问答数据集等，有监督的微调阶段不冻结LLM的参数。第二阶段是使用单阶段的基于强化学习的动作学习策略，将第一阶段的语言输出等信息转换为四足的DoFs的低水平动作。实验结果，以单个view作为输入超过了gridmm在仿真环境的表现，在自己造的真实场景下也达到了将近50%的成功率。首次投ICLR2025被拒 https://openreview.net/forum?id=gkDRrvqeWF
> 
> >[2025-02-09]-[何艺超]-本周未精读
> >
>>[2025-02-09]-陈银-[HumanOmni: A Large Vision-Speech Language Model for Human-Centric Video Understanding]
><img width="512" alt="image" src="https://github.com/user-attachments/assets/af0134ac-3211-45a4-b14b-1fc320bd2eb7" />
>
> 论文简述：本文提出了一种以人为中心的视觉语音大模型，并构建了一个包含240万个人中心视频片段和详细caption的数据库，以及超过1400万个指令对。在“情感识别、面部表情描述和动作理解”方面，模型表现优异的性能。数据集的构建流程如下: 场景检测和分割->过滤低分辨率 clip->使用 Qwen2-VL 去掉相似的 clip->最终使用自动标注标注出人脸和身体的 bounding box->然后用两个多模态 VLM 模型进行caption，并使用大预言模型进行汇总共同点,去除幻觉，按照特定模板生成 Instrucitons。除此之外，还手动对其中的 50K 同时包含视觉和语音的video clips 进行了标注，用于后续的微调。模型的训练分为三阶段，视觉对齐，语音对齐，以及多模态交互。最终在DFEW， MAFW 上的 WAR分别达到了 82.46%，68.40%.
> 
> > [2025-01-26]-陈银-[A Multi-Task Learning Framework for Emotion  Recognition Using 2D Continuous Space]-TAFFC
>
>><img width="512" alt="image" src="https://github.com/user-attachments/assets/c41e9e42-aeb2-445d-8893-85fb7f112603" />
> 
> 论文简述：本文提出了一种基于深度信念网络（DBN）框架的多任务学习方法，用于情绪识别。该方法将情绪类别识别作为主要任务，将激活度和情感价的识别作为次要任务，通过两种策略——基于类别水平的分类和基于连续水平的回归——将次要任务整合到情绪识别系统中。实验在IEMOCAP和SEMAINE两个数据库上进行，结果表明，与仅使用基线特征的方法相比，该多任务学习框架在未加权准确率上取得了显著提升，证明了利用激活度和情感价信息在情绪识别中的有效性。
> > [2025-01-26]-程浩-本周未精读
> 
> > [2025-01-26]-[张雪松]-本周未精读
>
> > [2025-01-26]-[聂建涛]-本周未精读
>
> > [2025-01-26]-[于洋晨]-本周未精读
> 
> > [2025-01-26]-[贾朋]-本周未精读
> > 
> > [2025-01-26]-[徐赟博]-本周未精读
> > 
> > [2025-01-26]-[何艺超]-[Omni-Emotion: Extending Video MLLM with Detailed Face and Audio Modeling for Multimodal Emotion Analysis](https://arxiv.org/pdf/2501.09502v1)
> > 
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/Omni-Emotion%20MLLM.png">
> >
> > 论文简述：Emotion-LLaMA的后续工作，和Emotion-LLaMA非常相像，细节上更加精细，并且在MER-OV，EMER，recognition三个任务上都做了实验，十分全面，首先训练过程从二阶段调整成三阶段，即分步把音频对齐，视频对齐和细粒度微调，而不是视频音频一块对齐。在数据集的处理阶段，相较于emotion-llama完全借助大模型和AU知识生成caption，这里没有使用AU知识，而是借鉴了EmoLLM使用到的FaceXFormer针对年龄，性别，以及人脸特征提取，虽然instruction也是借助大模型提取，但是增加了gpt-3.5和人工的审查机制（emotion-llama没有审查），最终得到24137和3500的细粒度，粗粒度数据。然后模型设计这块，没有什么创新，单纯讨论了一下project在针对全画幅特征和人脸特征的融合机制，分别是：逐帧拼接，交叉注意力计算，以及全视频最终特征拼接。
> 
> >  [2025-1-26]-[张宇]-[CRoF: CLIP-based Robust Few-shot Learning on Noisy Labels]-[arxiv](https://arxiv.org/abs/2412.12793)
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/corf.png">
> >
> > 论文简述: 本篇工作是改进CLIP去适应噪声下的小样本学习，主要的出发点对准的是噪声这个特性，考虑到clip会把相似的类别误判的主要原因是：1）标签噪声导致2）clip的硬匹配造成，所以解决这一现状让clip在多噪声的环境下还能有较高得泛化性只能从第二点出发，设置加权平滑的软匹配，即top-k加权策略（目标如果被占cos得分的第一，赋值为1，如果在top-k中，那么通过加权把它的得分赋值到排名的最高，如果超出了范围，那么得分就是0），这也是本文主要的创新点之一，其他两点在我看来没有那么新颖感觉像是拼凑上去的（1.通过gpt-o1来进行prompt的生成，一个类别生成五组2.微调clip的两端，使用adapter+残差连接）这三点后面的消融中也都感觉很独立，所以在我看来，多噪声环境下的小样本学习主要还是靠clip的软匹配来获益的，总的来说这三点可以看成是一种策略，即插即用的训练策略，比较轻盈高效。
> 
> > [2025-1-19]-程浩-赶论文，未精~~度~~读
> > 
> > [2025-1-19]-[何艺超]-加紧多模态自监督的数据集微调数据传输等+家中琐事，本周没有来得及精读
> > 
> > [2025-1-19]-[张宇]-[本周没有精读]
> 
> > [2025-01-19]-[贾朋]-[Emotional Conversation- Empowering Talking Faces with Cohesive Expression, Gaze and Pose Generation](https://arxiv.org/abs/2406.07895)
> > <img width="512" alt="image" src="https://raw.githubusercontent.com/2351548518/images/main/20241219/20250113171534297.png">
> >
> > 论文简述: 提出了一个两阶段音频驱动的说话人脸生成框架，该框架采用 3D 面部标志作为中间变量，通过自监督学习实现了表情、注视和姿势的协调。作者将此任务分解为两个步骤，即语音到landmark的合成和landmaek到人脸的生成。语音到landmark合成：给定参考图像，提取归一化的landmarks、gaze和 head pose，并预测由输入语音和情感标签驱动的每帧运动。landmark到面部生成：每帧的facial landmarks被映射到潜在关键点，然后将其输入到预训练face-vid2vid模型中以生成最终的面部。
> 
> >[2025-1-18]-[徐赟博]-[AeroVerse: UAV-Agent Benchmark Suite for Simulating, Pre-training, Finetuning, and Evaluating Aerospace Embodied World Models]
> >
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/AeroVerse.png">
> >
> >论文简述：本文是构建无人机领域的新尝试。仿真器提供进行连续导航的条件。在两个典型城市（京沪）和学校以及居民区两个场景下搜集了大量的RGB图像和对应的深度图像，多个方向的环境caption。下游涵盖了场景描述，空间推理，导航决策等五个任务。提供了多个不同特点的数据集-以无人机为中心，以环境描述为中心，以空间推理为中心。论文看上去为无人机导航提供了很多可以直接用的插件。
> > 
> > [2025-1-17]-[聂建涛]-[Guided Interpretable Facial Expression Recognition via Spatial Action Unit Cues]-[paper](https://arxiv.org/pdf/2402.00281)
> >
> >  <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/Guided%20Interpretable%20Facial%20Expression%20Recognition%20via%20Spatial%20Action%20Unit%20Cues.png">
> >
> >论文简述：本文提出了一种新的学习策略，通过将空间动作单元（AU）线索显式地融入分类器训练，从而训练出深度可解释的模型。作者采用输入图像、表情标签和面部标记，结合AU代码本构建AU热图，以此作为空间线索，约束分类器的空间层特征与AU热图相关联，使用复合损失函数进行训练，以实现同时正确分类图像并生成与AU图相关的层内可视化注意力。
>
> >[2025-1-17]-[张雪松]-[NAVILA: LEGGED ROBOT VISION-LANGUAGEACTION MODEL FOR NAVIGATION](https://navila-bot.github.io/)
>> <img width="512" alt="image" src="https://github.com/user-attachments/assets/2c909803-ef4c-4f8b-8530-667796075ed7" />
>>
>>论文简述：本文主要关注腿式机器人的视觉语言导航。和之前的VLN任务相比，本任务的主要挑战在于，将自然语言指令迁移(translate)到机器人的腿部动作并不简单。暂时略读，一些demo比较炫酷，而且是面向实际应用场景的，代码似乎可用，准备未来fellow！
> 
>>[2025-1-17]-[陈银]-[Lifting Scheme-Based Implicit Disentanglement of Emotion-Related Facial Dynamics in the Wild]
>><img width="512" alt="image" src="https://github.com/user-attachments/assets/16272c26-1dd5-4807-a938-02079914fdc1" />
>>
>>论文简述：在自然场景的视频中，情感相关的表情往往被时间上和空间上的非情感相关表情和全局背景信息所稀释，这使得情感识别变得困难。本文提出了一种新的隐式面部动态解缠框架，通过小波提升方案（lifting scheme），在不使用显式操作或外部引导的情况下，隐式地将情感相关动态信息从全局背景信息中分离出来，减轻了情感无关帧的负面影响。
>
> 
> >[2025-1-13]-[徐赟博]-[ENVEDIT: Environment Editing for Vision-and-Language Navigation]
> ><img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/Envedit.png">
> >
> >论文简述：本文提出了一种对VLN的数据增强技术，在保留关键语义信息的基础上，改变房间的风格、外观、对象类别来减少训练集和测试集之间的分布迁移问题。通过条件可控生成的技术进行环境的编辑，其中注入生成模型的控制条件是语义分割的mask，这样可以保留到环境中的语义信息（和一定程度的结构信息）。在生成数据和原始数据的可视化对比中，作者发现具有高度的对应关系。在导航能力的定量对比中，这种数据增强技术可以同时在Seen和Unseen场景下提升明显
> >
> >[2025-01-13]-[于洋晨]-上周多次期末考试以及课程报告，未精度
>
> > [2025-01-12]-[贾朋]-[SyncTalk- The Devil is in the Synchronization for Talking Head Synthesis](https://ziqiaopeng.github.io/synctalk/)
> > <img width="512" alt="image" src="https://raw.githubusercontent.com/2351548518/images/main/20241219/20250109155213773.png">
> >
> > 论文简述: 传统的GAN很难保持一致的面部身份，而NeRF方法虽然可以解决这个问题，但通常会产生不匹配的嘴唇运动、面部表情不足和不稳定的头部姿势。作者提出了 SyncTalk这种基于 NeRF 的方法有效地保持了主体身份，增强了头部说话合成的同步性和真实感。SyncTalk 采用Face-Sync Controller将嘴唇运动与语音保持一致，并使用 3DMM 模型来捕捉准确的面部表情。大量的实验和用户研究表明，SyncTalk 在同步性和真实性方面优于最先进的方法。
> 
> > [2025-01-12]-[张雪松]-[Robustness Analysis of Video-Language Models Against Visual and Language Perturbations](https://proceedings.neurips.cc/paper_files/paper/2022/hash/de6ff07cbd222c10d694c2b2f732aceb-Abstract-Datasets_and_Benchmarks.html)
> > <img width="512" alt="image" src="https://github.com/user-attachments/assets/65b0509c-6b61-409a-a64e-63d0ff171650">
> >
> > 论文简述: 本文首次对视频-语言模型在真实世界扰动下的鲁棒性进行了广泛的研究。作者提出了两个大规模基准数据集（MSRVTT-P和YouCook2-P），并利用90种视觉和35种文本扰动进行测试，从而揭示出一些有趣的初步发现：1）当仅文本被扰动时，模型比仅视频被扰动时更为鲁棒；2）经过预训练的模型比从头训练的模型更鲁棒；3）模型更多地关注场景和物体，而不是动作和运动。
> 
> > [2024-1-12]-[张宇]-[Waffling around for Performance: Visual Classification with  Random Words and Broad Concepts]-[ICCV23](https://openaccess.thecvf.com/content/ICCV2023/html/Roth_Waffling_Around_for_Performance_Visual_Classification_with_Random_Words_and_ICCV_2023_paper.html)
> >
> >  <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/wallfclip.png">
> >
> >论文简述：LLM+CLIP的工作，但是探究的点很新颖，是探究性工作，一般LLM+CLIP的工作是使用类名单词作为强约束，多组符合常理的描述句子跟在类名单词后面作为细粒度的区分，但是这样的话很有可能是多组文本平均带来的性能提升，因为可能VLM不会真正理解那些一个类别多个描述的人为认定的细粒度描述语句（后面作者在实验中也证实了，如果它是从语言描述的细粒度程度上收益，那么按理说多组文本取max替代一般工作的mean方式也会表现不错，但是实验显示了换成max性能会大幅下降），这就更证实了作者的要探究的观点（胡乱的随机词作为类名之后的细粒度描述也能表现很好），所以也属于改进zero-shot的工作，prompt形式是“A photo of a {concept}: a {c}, which (is/has/etc) {随机单词或字符}.”所以还是有类名单词作为强约束的（因为后面消融也显示了只有随机字符作为监督性能不佳），为什么随机单词也能有区分呢？->可以把它看做是不同形式的噪声集成，可以理解成数据加噪，性能超越了使用专门设计的细粒度语言描述的工作。
>
> > [2024-1-12]-程浩- $\texttt{赶实验赶论文，未精读}$ \
> > [2025-1-12]-[聂建涛]-[Detecting Facial Action Units From Global-Local Fine-Grained Expressions]-[paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160018)
> >
> >  <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/Detecting%20Facial%20Action%20Units%20From%20Global-Local%20Fine-Grained%20Expressions.png">
> >
> >论文简述：论文提出了一种新的面部动作单位（AU）检测方法GLEE-Net，通过全局-局部面部表情嵌入技术，缓解了因AU注释需要专业知识导致的身份过拟合问题，同时利用了未被标记的的面部表情数据集来辅助AU检测。GLEE-Net框架包含三个分支，分别提取与身份无关的表情特征，其中全局分支消除身份影响建模整体面部表情，局部分支关注特定面部区域。该方法先在面部表情数据集上预训练得到身份无关的表情嵌入，然后在小量的AU数据集上进行微调。此外，引入了3D全局分支通过3D人脸重建提取表情系数以加强2D AU描述，最终使用基于Transformer的多标签分类器融合所有表示进行AU检测。
>
> > [2025-01-11]-何艺超-[Contrastive Instruction Tuning]-[ACL 2024 Findings](http://arxiv.org/abs/2402.11138)
> > <img width="512" alt="image" src=https://github.com/ReadingPapers/Report/blob/main/Images/COIN.png>
> >
> > 论文简述：作者针对指令微调在具体任务中过于依赖指令而在用户指令输入的单词错误，拼写错误，语法错误，语义错误中的表现不佳的问题，提出一种基于对比学习的指令微调，正样本对是原始指令和原始输入输出&原始指令改写指令和原始输入输出，然后负样本对是原始指令和原始输入输出&与任务无关指令和原始输入输出，或者是原始指令和原始输入输出&原始指令和与任务无关输入输出，同时将对比学习损失和LLM生成需要用的交叉熵损失结合，通过超参控制二者结合的比例，这种方法可以增强模型针对同一样本对于不同指令的语义理解，从而提高下游准确性。
> > 
> > [2025-01-11]-陈银-[Label-Guided Dynamic Spatial-Temporal Fusion for  Video-Based Facial Expression Recognition](https://ieeexplore.ieee.org/abstract/document/10552397)-[TMM]
> >
> > <img width="512" alt="image" src=https://raw.githubusercontent.com/cyinen/imge/master/20250108133119.png>
>>
>>论文简评：考虑到标签分布提供了图像的分类保真度，本文使用表情的分布来引导时空的融合。把 video的标签作为 label, 为每一个帧分配视频的 label, 并设计辅助损失函数来监督。利用学习到的标签分布计算一个动态权重，并用把权重用于时空融合。==有一种把监督信号加到中间，打破信息瓶颈的味道。==实验验证了为每一帧分配不同的权重是很重要的，能够让模型把注意力关注到更重要的帧上面，从而利用更有用的信息。**这个方向还有待深入研究**。
> 
> > [2025-1-5]-程浩-放假+实验+写论文=未精读\
> >
> > [2025-1-5]-[何艺超]-准备最后两门考试，未精读
> >
> > [2025-1-05]-[张雪松]-[ AeroVerse: UAV-Agent Benchmark Suite for Simulating, Pre-training, Finetuning, and Evaluating Aerospace Embodied World Models]-[paper](https://arxiv.org/pdf/2408.15511)
> >
> > <img width="512" alt="image" src="https://github.com/user-attachments/assets/10fec248-3237-46da-aeb7-2588ba9525cd">
> >
> > 论文简述：本文提出AeroVerse，作为无人机智能体领域的全新基准套件，包含AeroSimulator仿真平台、首个大规模真实图文预训练数据集AerialAgent-Ego10k与虚拟图文姿态对齐数据集CyberAgent-Ego500k，以及支持五类下游任务（场景感知、空间推理、导航探索、任务规划和动作决策）的微调数据集SkyAgent系列。此外，通过GPT-4开发的SkyAgent-Eval，实现更全面和客观的任务评估，有效揭示了现有2D/3D视觉语言模型在无人机任务中的潜力与局限性.（暂未开源，简单了解不算严格意义上的精读）
> >
> > [2025-1-05]-[徐赟博]-[SAME: Learning Generic Language-Guided Visual Navigation with State-Adaptive Mixture of Experts]
> >
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/SAME.png">
> >
> > 论文简述：本文提出了一种使用MOE的架构统一不同视觉语言导航任务的学习。作者首先使用了单独学习的方式，对不同语言引导程度的导航任务进行学习，发现这种方式会导致不同的任务之间产生负迁移的现象。于是，作者在Cross-Attention层引入MoE来促进多任务学习。但是，作者发现，直接进行moe的效果并不好，分析原因可能发生在不同导航任务的语言引导程度上（Zero/fine/coarse grined），于是作者在MoE层，预测路由概率时加入了与任务以及多模态相关的信息。促进了七种导航任务的多任务学习。实验做得不太完善，进行对比的很多工作都是没有在某个数据集上训练过的，但是也是在VLN中使用MoE的先河。
> >
> > [2025-01-05]-[贾朋]-[LES-Talker: Fine-Grained Emotion Editing for Talking Head Generation in Linear Emotion Space](https://peterfanfan.github.io/LES-Talker/)
> > <img width="512" alt="image" src="https://raw.githubusercontent.com/2351548518/images/main/20241219/20250105215301644.png">
> >
> > 论文简述: 虽然现有的Talking head模型在粗粒度情感编辑方面取得了进展，但仍然缺乏具有高可解释性的细粒度情感编辑模型。作者提出了 LES-Talker，一种具有高可解释性的Talking head生成模型，以实现不同情感类型、情感水平和AU的细粒度情感编辑，提出了基于AU的线性情感空间（LES）定义，将情感变换描述为向量变换。设计了跨维度注意力网络（CDAN）来挖掘 LES 表示和 3D 模型表示之间的相关性，使 LES 表示能够指导 3D 模型的可控变形。
> 
> > [2025-1-5]-[张宇]-准备最后两门考试，未精度
>
> > [2025-1-5]-[聂建涛]-[FaceMixup: Enhancing Facial Expression Recognition through Mixed Face Regularization]-[paper](https://arxiv.org/pdf/2405.20259)
> >
> >  <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/FaceMixup.png">
> >
> >论文简述：论文提出了一种新的面部数据增强方法，名为FaceMixup，旨在通过混合面部组件正则化来提高面部表情识别的性能。其主要思路是通过对不同类别的面部图像进行裁剪和替换操作来创建新的混合面部图像，并在训练过程中考虑这些类别信息。FaceMixup方法通过混合面部组件进行正则化，生成额外的训练样本，以此丰富数据集并提高深度学习模型的泛化能力。
>
> > [2025-01-05]-[陈银]-这周在改论文，未精度


为简化排版，2024年小组精读论文已经保存在[这里](https://github.com/ReadingPapers/Report/blob/main/Archived/2024%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E5%BD%92%E6%A1%A3.md)







