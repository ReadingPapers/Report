######### 模版 #############

> week1 同一周在同一个level的bar上
> 
> [日期]-[first]-[论文标题]-[来源]（附带链接，链接可选，时间倒序）\
> >
> > 论文框架图（有助于一眼就能想起论文内容），宽度设置为512
> >
> > 论文简述 （一两句话总结精华，切勿过长）
> 
> [日期]-[second]-[论文标题]-[来源]（附带链接，链接可选，时间倒序）\
> >
> > 论文框架图（有助于一眼就能想起论文内容）
> >
> > 论文简述 （一两句话总结精华，切勿过长）
> 

> week2
> 
######### 模版 #############

## 必须是【全文】精读的论文，如实记录

> ==> 时间倒序，本次更新插入位置 <==
>
>> [2025-12-28]-[戴逦]-[Collaborative Diffusion for Multi-Modal Face Generation and Editing]
>>
>>论文简述：本文提出一种名为Collaborative Diffusion的多模态人脸生成与编辑框架，解决现有扩散模型通常只能由单一模态（如文本或分割掩码）控制的问题。该方法无需重新训练模型，通过引入一个轻量级的动态扩散器（Dynamic Diffuser），在扩散反向去噪的每个时间步和空间位置上，自适应地分配不同预训练单模态扩散模型（如文本驱动、掩码驱动）的贡献权重，实现多模态条件的有效协同。实验结果表明，该框架在多模态人脸生成和编辑任务中，在图像质量、条件一致性和身份保持方面均优于当时的方法，验证了其灵活性与有效性。
>
> > [2025-12-28]-[于洋晨[-[本周未精读]
> 
> > [2025-12-28]-[李欣雨]-[Multi-modal expressive personality recognition in data non-ideal audiovisual based on multi-scale feature enhancement and modal augment]
> >
> > 论文简述：本文提出了一种名为MsMA-Net的端到端视听多模态人格识别框架，解决实际应用中常见的模态缺失和噪声干扰问题。该方法通过跨注意力机制深度融合视觉与听觉特征，并设计了一个通用的多尺度特征增强模块MSFEM来提升特征表达能力。同时，创新性地引入模态增强策略MAS，在训练中模拟多种非理想数据场景，显著增强了模型的鲁棒性。该方法在ChaLearn First Impression数据集上以0.916平均准确率刷新SOTA，并在模拟的六种非理想场景下均表现出更强的适应性，为多模态人格识别的实际应用提供了有效解决方案。
> > 
> > [2025-12-28]-[张宇]-[Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization]
>>
>>论文简述：野外的DFER任务中，不同数据源、不同人物和不同视频质量差异较大，导致模型很容易学到数据风格而不是表情本身，从而泛化性较低。作者的思路不是简单做数据增强或域对齐，而是从表示学习和训练优化两个方面进行，在表示层，通过一个时频联合的注意力机制，频率上主动模拟画质和风格变化、迫使模型忽略与表情无关的频域差异，时间上则抑制视频中不稳定或噪声帧对决策的干扰，在优化层，再通过一个动态调节的损失设计，避免极端难样本和异构样本主导梯度更新。(引发这篇论文动机的问题，感觉值得被关注)。
>>
>> [2025-12-28]-[何艺超]-[AffectGPT-R1: Leveraging Reinforcement Learning for Open-Vocabulary Multimodal Emotion Recognition]
>>
>> 论文简述：作者认为当前OV-MER任务存在严重的目标错位，即训练用的Token-level loss关注字符匹配，而评测用的情感轮指标关注语义距离且不可导（这导致模型分不清字符相似但语义迥异的情感词）。于是作者设计了AffectGPT-R1，将强化学习引入情感领域的尝试。方法上，模型在输出最终情感预测前需生成一段推理过程，以辅助捕捉细粒度情感语义；同时，为防止模型通过冗长无效的推理进行 reward hacking，作者引入长度惩罚与辅助奖励，对推理质量和最终预测结果进行联合约束。实验结果表明，该方法在MER-UniBench等多个基准上取得了显著提升。
>> 
>> [2025-12-28]-[陈银]-[V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning]
>>
>> <img width="512" height="325" alt="image" src="https://github.com/user-attachments/assets/3fb133d3-ecf2-4d78-a20f-01683aad0659" />
>>
>>这篇论文提出了 V-JEPA 2，一种基于自监督学习的大规模视频模型，旨在让 AI 通过观察（超过 100 万小时的互联网视频）来理解世界，并能通过极少量（约 62 小时）无标签的机器人交互数据进行后训练，从而具备预测未来和在物理世界中规划的能力。V-JEPA 2 不仅在视频理解（如动作分类、问答）和预测（如人类动作预测）任务上达到了最先进水平，更重要的是，它作为一个高效的潜在空间世界模型，能够直接用于机器人的零样本规划，在未见过的环境中成功完成抓取和放置等复杂任务，而无需针对特定任务的训练或奖励。
>
> >[2025-12-27]-[程浩]-[本周未精读]
> >
> >[2025-12-27]-[张雪松[-[本周未精读]
> >
> >[2025-12-21]-[李欣雨]-[本周未精读]
> >
> >[2025-12-21]-[陈乾]-[本周未精度]
> >
> >[2025-12-21]-[贾朋]-[本周未精读]
> >
> >[2025-12-21]-[戴逦]-[本周未精读]
> >
> >[2025-12-21]-[徐赟博]-[本周未精读]
> > 
> >[2025-12-21]-[何艺超]-[本周未精读]
> >
> >[2025-12-21]-[张宇]-[本周未精读]
> >
> >[2025-12-21]-[于洋晨[-[本周未精读]
> >
> >[2025-12-20]-[程浩]-[本周未精读]
> >
> >[2025-12-20]-[陈银[-[本周未精读] \
> >
> >[2025-12-14]-[张雪松[-[本周未精读]
> >
> >[2025-12-14]-[徐赟博]-[本周未精读]
> >
> >[2025-12-14]-[戴逦]-[本周未精读]
> >
> >[2025-12-14]-[李欣雨]-[本周未精读]
> >
> >[2025-12-14]-[贾朋]-[本周未精读]
> >
> >[2025-12-14]-[于洋晨[-[本周未精读]
> >
> >[2025-12-14]-[何艺超]-[Learning What to Attend First: Modality-Importance-Guided Reasoning for Reliable Multimodal Emotion Understanding]
> >
> ><img width="512" height="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/MIGR.png" />
> >
> >论文简述：作者认为当前的MLLM在emotion reasoning任务中存在reasoning drift，具体来说就是模型在推理过程中逐渐依赖自身生成的文本而非多模态证据，从而推理链条不清晰（其实目前正是因为推理链条不够清晰，导致情感识别准确率不高，由于大模型文本是一个个token往外蹦的，所以作者设计方法的核心就是把情感外显模态的描述放在前面，数据集上设计的方法就是如何检测重要模态）。于是作者设计了MI（模态重要性），通过Human-Omni模型（阿里的一个微调情感大模型），分别输入audio-only，video-only，audio&video进行推理，并与GT比较，关注推理正确的模态，认为推理正确的模态是主导模态，然后把他的描述信息放在前面，这就体现了那个模态的重要性。然后除了EMER的322条数据，作者还补充了不一部分，这部分数据的GT通过Emotion-llama中基于经验的AU映射法得到（这种方法已经被相关工作指出存在一些问题，但应该不是什么大问题）。然后在训练阶段，整个arch照搬R1-Omni（阿里的一个做微调情感大模型的工作），在MAFW和DFEW上验证（添加了部分训练数据到预训练中，说明应该也不是按照5折来的）。两阶段训练，第一阶段冷启动SFT对齐模态；第二阶段强化学习，提出模态对齐顺序奖励。消融实验上发现了一个现象：方法在惊讶准确率下降，分析是可能源于惊讶与恐惧之间固有的模糊性，表明这一方面还有进一步改进的空间。个人猜测：即使通过这种方法改善了提升了推理过程的可靠性，但是由于描述性数据的信息量不够，在一些模糊数据没有办法直接通过类似的描述进行区分，说不定认知型的信息可以从数据构建阶段解决这个问题。
> >
> >[2025-12-14]-[张宇]-[FairMT: Fairness for Heterogeneous Multi-Task Learning]
> >
> >论文简述：传统的公平算法一般只针对单一任务，而在多任务、尤其是混合任务的场景下，现有方法很难保证每个任务都公平，该工作提出了一个新的框架，它能在多任务学习中自动平衡每个任务之间的公平性。具体来说，FairMT的核心是AHFDA机制，它的思路是只惩罚那些在特定群体上表现不佳的任务输出，并通过归一化聚合机制在多任务间传递公平约束，让模型自动平衡不同任务与群体之间的差异，也就是只针对那些表现落后的群体进行提升，而不去削弱原本表现好的群体（有点像帕累托最优解的感觉），这样模型是在补短板而不是砍长板，同时，它还考虑了多任务架构中每个任务输出层的结构不同可能带来的不公平，通过一个head-aware优化代理来修正这种几何偏差，从而在优化时保持公平性。
> >
> >[2025-12-14]-[陈乾]-[ECERC: Evidence-Cause Attention Network for Multi-Modal Emotion Recognition in Conversation]
> >
> ><img width="512" height="335" alt="image" src="https://github.com/2003-0224/PapperImage/blob/main/3d706c0e-2d28-419f-9136-34708c049732.png" />
> >
> >论文简述：论文提出了证据-原因注意力网络，一个用于多模态对话情感识别 (MMERC)的新方法，旨在解决现有模型仅关注一般对话依赖性、而未能充分捕获对话中多样化且详细情感原因（如情感传染、他人影响、事件引用等）的局限性 。ECERC通过证据门控跨模态提炼情感证据、原因编码从对话上下文捕获事件和情感因素、证据-原因交互利用多方面注意力机制将证据与四种不同原因（自方/跨方事件，自方/跨方情感）融合生成候选特征、特征门控自适应加权这些候选特征，最终进行情感分类。在 IEMOCAP 和 MELD 这两个主流基准数据集上进行的广泛实验验证了 ECERC 的有效性，其在加权 F1-score 和准确率上均超越了现有基线模型，展现了通过整合情感证据和因果推理来增强 MMERC 性能的潜力
> >
>> [2025-12-14]-[陈银-[Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture]
>>
>> <img width="512" height="335" alt="image" src="https://github.com/user-attachments/assets/3c3b2856-ef7d-4f77-baff-75a9b952a272" />
>>
>> 论文简述：本文提出了图像联合嵌入预测架构（I-JEPA），一种新的自监督学习方法，旨在不依赖手工设计数据增强的情况下学习高度语义的图像表示。I-JEPA的核心机制是使用一个上下文图像块，通过一个预测器网络，去预测同一图像中多个大尺度目标块在目标编码器中的嵌入表示。通过在抽象的嵌入空间而非原始像素空间进行预测，模型被引导去捕获高层语义信息，同时舍弃无关的低级细节。实验表明，I-JEPA具有极高的计算效率和可扩展性，其训练的模型在ImageNet线性评估和半监督学习等任务上性能优于其他非增强方法，并且在对象计数和深度预测等低级任务上也超越了主流的基于数据增强的方法。
>> 
> >[2025-12-13]-[程浩]-[本周未精读] \
> > [2025-12-07]-[陈乾]-[本周一直改代码，未精度]
> > [2025-12-08]-[陈银]-[本周参加学术会议，未精度]
> > 
> > [2025-12-07]-[贾朋]-[Flow Matching for Generative Modeling](https://arxiv.org/abs/2210.02747)
> > 
> > 论文简述: 流匹配(FM)，一种通过回归固定条件概率路径向量场来训练连续归一化流的方法。该技术与通过噪声与数据样本间转换的高斯概率路径通用族兼容，其中现有的扩散路径均可视为其特例。此外，流匹配技术为使用非扩散概率路径训练连续归一化流开辟了新途径，其中值得关注的是采用最优传输(OT)位移插值定义条件概率路径。
> >
> >[2025-12-07]-[戴逦]-[IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models]
> >
> ><img width="512" height="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/IPadapter.PNG" />
> >
> >论文简述：IP-Adapter是预训练文本到图像扩散模型新增“图像提示能力”的轻量级适配器，其核心在于“解耦式交叉注意力”机制：分别为文本特征与图像特征设置独立的cross-attention，从而在完全冻结原UNet的情况下，细腻地注入参考图像信息。IP-Adapter具有高度通用性：无需重新训练即可直接复用到社区自定义模型、兼容ControlNet等结构控制模块，并支持文本与图像的多模态联合生成。
> >
> >[2025-12-07]-[李欣雨]-[Mem-PAL: Towards Memory-based Personalized Dialogue Assistants for Long-term User-Agent Interaction]
> >
> >论文简述：Mem-PAL是一项针对长期个性化对话系统的研究，提出了首个中文用户-智能体交互评估基准PAL-Bench及其支持数据集PAL-Set。该数据集通过多阶段大语言模型合成与人工验证构建，包含100名虚拟用户的多模态交互历史（对话与行为日志）。为了解决个性化建模中的历史信息整合难题，论文创新性地提出了H²Memory——一种层次化异质记忆框架，能够分别对日志和对话进行结构化编码，并提取用户长期背景与抽象偏好原则。实验表明，该方法在需求理解、方案生成及多轮对话等任务上均显著优于现有方法，并在外部数据集上验证了泛化能力。
> >
> >[2025-12-07]-[张宇]-[UNITI: Framework for multi-task learning across datasets to mitigate overfitting]
> >
> ><img width="512" height="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/uniti.png" />
> >
> >论文简述：针对多任务学习中不同任务来自完全不同数据集时，多任务模型容易互相干扰甚至忘记已学任务的问题，这篇工作提出了一个统一的架构。UNITI让模型在各个数据集之间按批次轮流训练，避免一次只学一个任务导致灾难性遗忘，同时利用特征层级的知识蒸馏，让学生模型模仿每个任务的teacher模型的中间特征，这样不同任务就不会把彼此的特征覆盖掉。最后再用结构重参数化把训练时的复杂结构收回成原始模型，保证推理效率。实验结果显示，无论在异构数据集还是在同构数据集中，UNITI 都能维持甚至超过单任务模型的表现，同时只需要训练和部署一个模型。
> >
> >[2025-12-07]-[何艺超]-[CAPE: A Chinese Dataset for Appraisal-based Emotional Generation in Large Language Models]
> >
> ><img width="512" height="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/CAPE.png" />
> >
> >论文简述：这是一篇立足于使用LLM在的对话中生成情感恰当的反应。出发点源于当前通用LLM可能生成的反应虽然在上下文中相关，但与特定情境所需的情感基调不符。并且相关研究主要聚焦于英文。于是作者创建了一个中文为主的数据集CAPE，以认知评估理论作为支撑，先给定一个具体情境，再人为设定一组“人的内心判断条件”，然后让大模型在这些条件限制下生成带有明确情绪的多轮对话，最后再由人工逐条检查和修改，保证对话在逻辑上合理、情绪上自然、表达上像真人说的话。下游任务上主要包括情绪预测任务和下一个子句预测，生成的话语质量反映了模型表达类人情感反应的能力。实验结果表明，作者微调的模型超越了通用模型在CAPE数据集上两个任务的指标，验证了其数据集的有效性。
> >
> >[2025-12-07]-[张雪松]-[Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment]
> >
> > <img width="512" height="512" alt="image" src="https://github.com/user-attachments/assets/d98ed27f-d1a4-4437-b039-ee1ce04146cd" />
> >
> >论文简述：Evo-1 提出一种轻量级视觉—语言—动作（VLA）模型，旨在在不依赖大规模机器人预训练的前提下保留预训练视觉-语言模型（VLM）的语义对齐。作者引入交叉调制扩散 Transformer 与高效集成模块，并采用两阶段训练策略逐步将动作学习与感知對齊，从而在仅 0.77B 参数下于 Meta-World、RoboTwin 等基准取得强劲性能，同时在真实机器人上展现高成功率与低延迟，便于资源受限环境部署。
> >
> >[2025-12-06]-[程浩]-[本周未精读] \
> >  [2025-11-30]-[李欣雨]-[本周未精读]
> > 
> >   [2025-11-30]-[贾朋]-[MoEE: Mixture of Emotion Experts for Audio-Driven Portrait Animation](https://arxiv.org/abs/2501.01808)
> > <img width="512" alt="image" src="https://raw.githubusercontent.com/2351548518/images/main/20251120/20251201002307783.png">
> >
> > 论文简述: 语音驱动说话人。作者提出1）情感专家混合模型(MoEE)，通过解耦六种基本情绪实现单一及复合情感状态的精准合成；2）设计情感-隐空间控制模块，利用多模态输入对齐音频、文本和标签等控制信号，在支持多样化控制输入的同时保留纯音频情感调控能力。
> > 
> >  [2025-11-30]-[张宇]-[本周未精读] 
> > 
> > [2025-11-30]-[戴逦]-[本周未精读]
> >
> > [2025-11-30]-[于洋晨]-[本周未精读]
> >
> > [2025-11-30]-[何艺超]-[本周未精读] \
> >
> > [2025-11-30]-[徐赟博]-[Classifier-Free Diffusion Guidance]
> > 
> > (论文无主图)论文简述：本文提出了一种条件控制技术——Classifier-Free Guidance，用于提升文本到图像扩散模型的生成质量。与传统方法依赖外部分类器强化条件信息不同，CFG通过在训练阶段对条件进行随机dropout，使模型学习区分条件与无条件生成。在推理阶段，通过线性混合有条件和无条件的噪声预测，动态调整生成结果对条件的依赖程度，从而在保持计算效率的同时实现高质量的条件控制生成。这种方法解决了传统分类器指导方法的高计算开销和生成质量不稳定问题，在文本到图像生成任务中取得了显著的性能提升 。
> > 
> >[2025-11-230-[陈银]-[SemMAE: Semantic-Guided Masking for Learning Masked Autoencoders]-NeurIPs
> >
> ><img width="512" height="371" alt="image" src="https://github.com/user-attachments/assets/448dfaff-6089-4d3b-8aa7-1c6ee851f46b" />
>>
> >论文简述：本文旨在缩小掩码语言建模(MLM)与掩码图像建模(MIM)之间的差距。核心思想是探索图像的语义部分(Semantic Parts)作为视觉中类似于“单词”的单元，并提出一种语义引导的掩码策略(Semantic-Guided Masking)来将语义信息整合到掩码自编码器(MAE)的训练中，以学习更好的图像表征。SemMAE通过引入语义部分作为视觉“单词”的类比，并提出一种渐进式的学习方法和语义引导的掩码策略，成功地将高级语义信息整合到MAE的预训练过程中，在多个视觉任务上取得了领先的性能
> >
> >[2025-11-29]-[程浩]-[本周未精读] \
> >
> >[2025-11-29]-[张雪松]-[本周未精读] \
> >
> >[2025-11-23]-[陈乾]-[本周未精读]
> >
> >[2025-11-23]-[贾朋]-[本周未精读]
> >
> >[2025-11-23]-[张宇]-[FaceXFormer: A Unified Transformer for Facial Analysis]-[ICCV25]
> >
> >论文简述：本文针对人脸分析任务多、数据分散且标注各异的问题，提出了一个统一的transformer框架，通过将可学习的任务token与从图像抽取的人脸token共同建模，使任务与视觉表征在attention机制中直接交互。同时引入轻量的解码器(采用双向cross-attention在任务token与人脸token之间高效解耦信息)，使模型能在异构数据源下同时学习多种任务。但是没看到文中具体如何缓解异构数据集bias的问题。总体感觉就是工程感强，新颖的点也就在于那个初始化的任务token和人脸token的交互(其实也就是task-embedding那套)。
> >
> >[2025-11-23]-[李欣雨]-[Rethinking Personality Assessment from Human-Agent Dialogues: Fewer Rounds May Be Better Than More]
> >
> >论文简述：文章构建了中文人格对话数据集 Personality-1260，由42名大学生和5名LLM代理（分别高 O/C/E/A/N）在囚徒困境游戏场景下进行6轮对话，探究“交互轮次”与“代理人格”对 LLM 人格评估的影响。实验发现：1–2轮对话即可达到最优评估精度，继续增加轮次反而降低准确率，同时匹配特定人格代理可显著提升对应维度评估效果。最后得出少而精的交互比冗长对话更有效的结论。
> >
> >[2025-11-23]-[戴逦]-[InstructAvatar: Text-Guided Emotion and Motion Control for Avatar Generation]
> >
> >论文简述：这篇论文提出了一种创新的文本引导统一框架InstructAvatar ，通过GPT-4V自动化标注和AUs构建指令-视频数据集，用于精细控制2D说话数字人的情感和面部动作。其核心是双分支扩散生成器，能同时处理音频和文本指令，通过双分支交叉注意力机制，分别实现对全局情感的风格指导和对动态面部动作的精细控制，显著提升了生成数字人的表现力、唇形同步质量和自然度，并支持纯文本指令驱动面部动作。
> >
>> [2025-11-23]-[徐赟博]-[本周主要改代码，未精读]
> 
> > [2025-11-23]-[于洋晨]-[本周未精读]
> 
>> [2025-11-23]-[何艺超]-[本周梳理之前论文，未精读]
>> 
>> [2025-11-23]-陈银-[Text-Guided Video Masked Autoencoder]-ECCV2024
>>
>> <img width="512" alt="image" src="https://raw.githubusercontent.com/cyinen/imge/master/20251119092203.png">
>>
>> 论文简述：本文提出了一种新颖的文本引导掩码算法（TGM），它掩码与配对文本标题对应度最高的视频区域而无需额外的视觉线索的引导。实验证明，TGM在不使用任何显式视觉线索的情况下，其性能可与基于运动引导的SOTA掩码算法相媲美。此外，为了进一步利用自然语言的语义信息，本文提出了一个统一框架，将MAE与掩码视频-文本对比学习相结合。研究表明，这种统一框架能提升多种掩码算法在下游任务中的性能，尤其是在线性探测任务上。
>>
>> [2025-11-23]-张雪松-[MoManipVLA: Transferring Vision-language-action Models for General Mobile
Manipulation]-CVPR2025
>>
>> <img width="512" alt="image" src="https://github.com/user-attachments/assets/aad33405-c1cd-4d48-842a-4fbfc29fd591" />
>>
>> 论文简述：MoManipVLA 提出了一种策略迁移框架，将预训练的 固定基座 VLA（vision-language-action）模型 用于移动操控任务。它首先利用 VLA 模型预测机械臂**末端执行器的关键 waypoints**；然后通过双层优化 (bi-level) 同时规划机器人底座和机械臂的运动轨迹，以确保轨迹在可达性、平滑性与避碰方面都物理可行。该方法在 OVMM 基准和真实机器人上都显著提升了成功率（比 SOTA 高 4.2%），且真实环境部署只需较少训练样本。 
>>
> >[2025-11-22]-[程浩]-[本周未精读] \
> > [2025-11-16]-[李欣雨]-[本周未精读]
> >
> > [2025-11-16]-[贾朋]-[VGGT: Visual Geometry Grounded Transformer](https://jytime.github.io/data/VGGT_CVPR25.pdf)
> > <img width="512" alt="image" src="https://raw.githubusercontent.com/2351548518/images/main/20250709/20251117104053373.png">
> >
> > 论文简述: 前馈3D重建，可直接从单视角、少数视角或数百视角中推断场景的全部关键三维属性，包括相机参数、点云图、深度图及三维点轨迹。该方法突破了传统三维计算机视觉模型通常受限于单一任务的局面，兼具简洁高效特性，能在1秒内完成图像重建，且性能优于需要后置视觉几何优化处理的替代方案。该网络在相机参数估计、多视角深度估计、密集点云重建和三维点追踪等多项任务中达到最先进水平。
> > 
> >[2025-11-16]-[于洋晨]-[本周未精读]
> 
> >[2025-11-16]-[戴逦]-[本周未精读]
> >
> >[2025-11-16]-[陈乾]-[本周未精读]
> >
> > [2025-11-16]-[何艺超]-[EmoPrefer: Can Large Language Models Understand Human Emotion Preferences?](http://arxiv.org/abs/2507.04278)
> >
> > 论文简述：本文提出EmoPrefer，核心是探索MLLM在识别人类情感偏好方面的能力。提出的新任务，也是从别的领域迁移过来的。简单说就是测试了一下不同的MLLM对于同一个样本，但是不同的描述性文本的偏好，去判断哪种描述更好，并给出了4种判断的方式。样本的收集来自EMER-fine和MER-Caption+的交集，汇总得到一个新的数据集EmoPrefer-Data。然后配套的提出EmoPrefer-Bench，是一个专注于人类情感偏好的数据集和基准测试。从结果上看，不同的MLLM倾向于不同的判断方式（提示策略）。并且总的来说当前的MLLM仍然难以准确预测情绪偏好，也就没法准确判断，哪种情感描述更贴切，揭示了目前情感描述数据集工作的弊端。
> > 
> > [2025-11-16]-[张宇]-[本周未精读]
> >
> > [2025-11-16]-[徐赟博]-[本周未精读]
> > 
> >[2025-11-15]-[程浩]-[本周未精读]
> >
> >[2025-11-15]-[张雪松]-[本周未精读]
> > 
> >[2025-11-15]-[陈银]-[本周未精读]
> > 
> 
> > [2025-11-09]-[贾朋]-[PESTalk: Speech-Driven 3D Facial Animation with Personalized Emotional Styles](https://dl.acm.org/doi/10.1145/3746027.3755190)
> > <img width="512" alt="image" src="https://raw.githubusercontent.com/2351548518/images/main/20250709/20251110102117995.png">
> >
> > 论文简述: 3D面部动画。作者提出PESTalk，可直接从语音输入生成具有个性化情感风格的三维面部动画，显著提升动画真实感。由于声学频域线索包含关键情感信息，首先设计双流情感提取器（DSEE），通过同步捕捉音频信号的时域变化与频域特征，获取细粒度情感特征及微妙情绪差异。开发情感风格建模模块（ESMM）实现个性化情感风格，该模块基于声纹特征为每个对象建立基线表征，并通过逐步整合情感特征不断优化，最终为每个对象在每类情感中构建独特的个性化情感风格表征。
> > 
> >[2025-11-09]-[何艺超]-[本周未精读]
> >
> >[2025-11-09]-[李欣雨]-[本周未精读]
> > 
> >[2025-11-09]-[张雪松]-[ADANAV: ADAPTIVE REASONING WITH UNCER-TAINTY FOR VISION-LANGUAGE NAVIGATION]
> >
> ><img width="512" height="512" alt="image" src="https://github.com/user-attachments/assets/703d2f3e-abe6-4435-969f-fd8d0db85e85" />
> > 
> >论文简述：本文提出 **AdaNav**，一种基于不确定性的自适应推理框架，用于提升视觉语言导航（VLN）中的时序一致性与感知-行动对齐。其核心的 **不确定性自适应推理模块（UAR Block） 通过动作熵动态触发推理，仅在高不确定性时进行思考，从而避免过度推理并提高效率**。配合 启发式到强化学习（Heuristic-to-RL） 的训练策略，模型在无额外标注数据下学习何时及如何推理。实验显示，在仅6K样本下，AdaNav即超越百万规模闭源模型，在R2R与RxR上显著提升成功率，并展现出优越的跨数据集与真实场景泛化能力。

> >
> >[2025-11-09]-[陈乾]-[GRPO-Guided Modality Selection Enhanced LoRA-Tuned LLMs for Multimodal Emotion Recognition]
> >
> ><img width="512" height="512" alt="image" src="https://github.com/2003-0224/PapperImage/blob/main/e5e03dffd48af0a13f824768a684905a.png" />
> >
> >论文简述：传统的MER C 常把所有模态固定融合，但“有时候文本够用、有时需要多模态”。固定融合会在冗余场景引入噪声，因此我们需要“按需融合”。作者提出一个两阶段框架，其中第一阶段主要是用 GRPO 强化学习优化策略，使模型学会在何时保留多模态，GRPO 借助“组内多输出 + 以组内均值和标准差归一化”的奖励设计，让模型在无标签情况下比较不同 reasoning 路径的“质量”，并倾向于产生既合格式又合理长度和语义对齐的 CoT 输出，从而学会在历史窗口中判别何时需要额外的 audio/video 描述。第二阶段根据阶段一的决策构造 prompt（有/无 audio/video 描述），用 LoRA-微调 LLM 做情感标签预测。最终在MELD和IEMOCAP两个数据集上均取得了显著的提升。
> >
> > [2025-11-09]-[徐赟博]-[World-Consistent Data Generation for Vision-and-Language Navigation]
> > 
> > <img width="512" height="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/WCGEN.png" />
> > 
> > 论文简述：这篇文章在PanoGen的基础上，从全景图连贯一致性的角度进行了改进。主要是通过引入了深度信息以及细粒度的语义描述信息作为更加精确的控制信号。提出了两阶段的生成范式。一阶段通过2D坐标和像素值构建3维点云，同一个轨迹中相邻viewpoint的首个初始参考图像由3D点云采用出一个图像质量不高，部分像素值缺失的初始图像，然后通过深度图作为控制条件诱导执行图像修复过程。随后根据初始参考图像，深度图作为控制条件，以及通过prompt提取的较细粒度的语义描述，执行迭代扩图过程生成新的全景图像。
> > 
> >[2025-11-09]-[戴逦]-[EmoEdit: Evoking Emotions through Image Manipulation]
> >
> >论文简述：本文提出一种新的情感图像编辑方法——EmoEdit，在保持原图主体结构的同时，通过修改内容来实现特定情绪编辑。作者构建首个大规模情感图像编辑数据集 EmoEditSet，包含 40,120 对图像数据，利用情感归因与语义生成策略获得高质量、多语义的样本。作者还设计了一个Emotion Adapter模块，使扩散模型具备情感感知能力，并提出Instruction Loss来捕捉语义层面的变化。实验结果显示，EmoEdit 在结构保持、语义一致性和情绪准确度上均优于现有方法，同时能平衡图像保真与情绪表达。
> >
> > [2025-11-09]-[张宇]-[Reading Smiles: Proxy Bias in Foundation Models  for Facial Emotion Recognition]
>>
>> 论文简述：这篇研究探讨了vlm在fer任务中，如何依赖面部特征（牙齿可见性、眉毛位置和嘴巴开合）来预测情绪。研究发现，尽管一些模型在zeroshot情感识别任务中表现优异，但它们往往大多依赖牙齿的可见性，特别影响高情感值的预测从而导致误分类（当牙齿被错误预测为可见时，模型倾向于将情绪归类为快乐）。在affectnet数据集中的测试极为显著，可见的牙齿成为了zeroshot-vlm或监督学习模型的线索。换种角度思考，这种情况本质上是由各个数据集的bias给模型提供了一些捷径，导致训练出的模型缺乏可解释性和泛化性。这篇工作只是发现了这个问题存在，并没有提出解决它的办法。
> >
> >[2025-11-09]-[于洋晨]-[Targeted Distillation for Sentiment Analysis]-EMNLP2025
> ><img width="512" height="413" alt="image" src="https://github.com/yyc-hfut/PaperReading/blob/main/TDSA.png"/>
>>
>> 论文简述：在大模型之间进行情感知识的蒸馏，从多方面多视角的总结知识，然后再去进行知识的迁移。并提出了对应的相关基准数据集。特别是在知识的提取阶段，文中的Multi-perspective Prompting Strategy的设计比较有趣，从四个维度提取出深度的情感信息。通过文中设计的蒸馏框架，可以有效的迁移大模型之间的情感知识，那么假设，这些角度能够完备的总结出大模型所需要的情感推理逻辑，是否也可以作为对大模型进行强化学习决策的一个完备的情感状态呢，比一般的特征提取会更加的可解释，也会更加的全面；或者说是将小模型的特征提取替换成了大模型的特征提取，更适合作为大语言模型的输入（或许可扩展当前的强化学习范式，作为当前工作的大模型版本）
> >
> 
>> [2025-11-09]-[陈银]-[Text-Guided Video Masked Autoencoder]-ECCV2024
> >
>> <img width="512" height="413" alt="image" src="https://github.com/user-attachments/assets/3f8e3b42-7b61-4053-b43d-2cf2884a40bd" />
>>
>> 论文简述： 对VideoMAE的改进工作依赖于视觉线索来mask运动显著的区域，但是这些方法的鲁棒性依赖于视频的输入。本文尝试通过自然语言，隐式的捕捉显著的区域从而进行掩码。在不依赖任何额外视觉线索的情况下，本文的方法与SOTA方法相当，并极大提升了线性探测的性能。
>> 
> >[2025-11-08]-[程浩]-[本周未精读] \
> >[2025-11-02]-[贾朋]-[本周未精读]
> >
> >[2025-11-02]-[戴逦]-[Denoising Diffusion Probabilistic Models]
> >
> >论文简述：论文提出一种基于非平衡热力学思想的扩散概率生成模型，通过在潜变量空间中构建一个逐步加噪与去噪的马尔可夫链，实现从高斯噪声到复杂数据分布的高保真生成。论文在理论上揭示了扩散模型与去噪分数匹配以及退火 Langevin 动力学之间的等价关系，从而将其训练目标形式化为一个加权的变分下界，并进一步提出简化的去噪损失函数以提升训练稳定性与生成质量。
> >
> >[2025-11-02]-[何艺超]-[Emotion-Coherent Reasoning for Multimodal LLMs via Emotional Rationale Verifier]
> >
> ><img width="512" height="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/ERV.png" />
> >
> >论文简述：作者的motivation基于当前MLLM的推理和结果不一致，会削弱模型的稳定输出；作者提出了一种不使用任何新添加的数据集（有歧义）也不改变模型（使用了R1-omni的训练的流程）实现情感分类性能提升的方法。在实际操作中，具体通过一个设计的ERV模块实现，该模块的主要作用是为GRPO的训练提供奖励，这个奖励主要衡量的就是模型指令数据在分割成一句句子句后的情感预测正确的比例。训练流程如下：完全使用HumanOmni的模型，初始化使用EMER的322条数据做SFT，然后初始化的模型和其的一个副本被分别当作ref-model和policy-model，VER是一个事先在情感数据集上预训练的文本情感分类器，但是作者在其基础上又添加了DFEW和MAFW的文本描述进行了再次的训练，policy-model的输出会被ERV计算部分奖励，配合GRPO基本都有的格式输出奖励和结果正确奖励，奖励会在GRPO的训练过程中促进更新模型参数。实验结果上看，效果较优，但是不知道这个结果是DFEW或者是MAFW的第几折出来的结果（这两个数据集不同折的结果差别都挺大的），并且其说了是不添加任何数据集，但是VER的训练中使用了构造伪标签的DFEW和MAFW，并且测试用的也是这两个数据集，如果VER记住了DFEW和MAFW的数据结果，那么这时候再用VER的奖励去train policy-model，就会让policy-model也继承了VER的记忆，那这时候性能的提升就类似于一种高级的数据泄露，存在一定的问题。
> >
> >[2025-11-02]-[张雪松]-[本周未精读]
>  >
> >[2025-11-02]-[徐赟博]-[本周未精读]
> >
> >[2025-11-2]-[李欣雨]-[A Media-Guided Attentive Graphical Network for Personality Recognition Using Physiology]
> >
> ><img width="512" height="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/media_guided_GCN.png" />
> >
> >论文简述：本文提出首个融合视听刺激内容的生理信号人格识别框架。文章将每位受试者的多个生理反应建模为图结构，节点为生理特征，边由视频内容（视觉和听觉）语义相似性决定，再通过图卷积和注意力机制聚合为单一人格预测，在Amigos和Ascertain两大数据集上达SOTA。此外，文章还进行深入分析，揭示“十面埋伏”等关键刺激、EDA tonic与Shannon熵为稳健指标，并发现外向性与视觉厌恶/熟悉度显著相关，为多媒体-生理-人格建模提供可解释性。
> >
> >[2025-11-02]-[陈乾]-[MINOS: A Multimodal Evaluation Model for Bidirectional Generation Between Image and Text](https://arxiv.org/pdf/2506.02494)
> >
> >论文简述：现有多模态生成评估多集中于图像到文本（I2T）任务，忽视了文本到图像（T2I）评估及大规模人工标注数据的价值。本文提出Minos，一种面向双向生成任务的多模态评估模型，并构建了包含12.4万样本的Minos-Corpus，其中4.8万具有人类标注分数。作者提出“数据筛选与平衡”策略以提升数据质量，并结合Mix-SFT混合微调与DPO偏好对齐方法优化模型性能。实验表明，Minos在多项I2T与T2I任务上均实现SOTA表现，特别在T2I评估中超越了GPT-4o等模型。研究验证了高质量人工数据与双向任务联合训练对多模态评估模型泛化性和人类一致性的关键作用。
> >
> >[2025-11-01]-[陈银]-[CrossMAE: Cross-Modality Masked Autoencoders for Region-Aware Audio-Visual Pre-Training ]-CVPR2024
> >
> > <img width="512" alt="image" src="https://raw.githubusercontent.com/cyinen/imge/master/20251031100805.png">
>>
>>论文简述: 现有的AV预训练工作主要关注全局信息而忽略了同样重要的细粒度特征和位置信息，导致在稠密预测的任务上表现不佳。为了解决这个问题，本文更进一步提出了基于区域的AV预训方法CrossMAE, 实现模态交互与区域的对齐。主要提出了两点改进，一个是在像素级重建时引入另一个模态的关键信息（Attentive Tokens）， 另一个方面就是通过跨模态的特征重建（引入可学习的query强化位置信息）。这一机制不仅有效实验了模态之间的交互，更实现了细粒度语义的深度挖掘。在下游的分类，检索，音视频定位任务上都实现了SOTA。
>>
> >[2025-11-01]-[程浩]-[本周未精读] 
> >
> >[2025-11-01]-[张宇]-[赶论文，本周未精读]
> >
> >[2025-10-26]-[陈乾]-[本周一直做实验，未精读]
> >
> >[2025-10-26]-[贾朋]-[ECOFACE: AUDIO-VISUAL EMOTIONAL CODISENTANGLEMENT SPEECH-DRIVEN 3D TALKING  FACE GENERATION](https://proceedings.iclr.cc/paper_files/paper/2025/file/fc014449bb7cf2d2add0f734cbb40006-Paper-Conference.pdf)
> > <img width="512" alt="image" src="https://raw.githubusercontent.com/2351548518/images/main/20250709/20251027162519251.png">
> >
> > 论文简述: 3D面部动画。近期研究探索了语音-情感解耦技术以增强面部表情表现，而非人工标注情感。然而，该方法存在特征混淆、情绪弱化和平均脸等问题。为此，作者提出EcoFace框架：(1) 通过创新协作目标，从说话者的表情动作和生成声音中为情感表征学习提供显式信号，构建与语音内容无关的视听联合协调情感空间；(2) 建立由语音特征决定的通用面部运动分布空间并实现说话者感知生成。本方案能生成更具泛化性且情感表现真实的三维数字人。
> >
> >[2025-10-26]-[张宇]-[本周赶实验论文，没有精读]
> >
> >[2025-10-26]-[李欣雨]-[Personality Recognition by Modelling Person-specific Cognitive Processes using Graph Representation]-ACM2021
> >
> ><img width="512" height="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/Graph.png" />
> >
> >论文简述：文章提出了新的个性识别方法：通过建模个体特定的认知过程来实现对“真实个性”的识别。因此，作者提出：在两人对话中，听者（target subject）的面部表情反应由两个因素驱动，其内部认知机制和说者的外部非语言行为。本文通过神经架构搜索为每个个体构建多模态CNN认知模型，输入为说者的音频和面部行为，输出为听者的面部反应，模拟出听者从他人非语言行为到自身面部反应的映射关系；进一步将CNN结构编码为图表示，使用图神经网络进行建模，实现对听者的真实五大人格的精准识别。该方法在NoXi数据集上的平均分类准确率达到91.5%，明显优于其他方法。
> >
> >[2025-10-26]-[徐赟博]-[HA-VLN 2.0: An Open Benchmark and Leaderboard for Human-Aware Navigation in Discrete and Continuous Environments with Dynamic Multi-Human Interactions]
> >
> ><img width="512" height="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/HAVLN.png" />
> >
> >论文简述：本文在原始的仿真环境habitat上对原始环境进行了编辑，提出了一个新的任务：human-aware的VLN。在室内环境中加入多个合成的人类运动实体，要求agent在环境中进行移动的时候除了注意对不可移动位置的避障外，还要注重对未来人类运动的避障。同时，对于一个人类实体，通过多视角的LLM的描述的聚合，形成了新的指令描述，并且提出了离散和连续两种数据集用于可迁移测试。
> >
> > [2025-10-26]-[何艺超]-[Beyond Emotion Recognition: A Multi-Turn Multimodal Emotion Understanding and Reasoning Benchmark]
> >
> ><img width="512" height="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/MTMEUR.png" />
> >
> >论文简述：MTMEUR构建了一个包含1,451个现实场景视频和5,101个渐进式问题的数据集，覆盖愤怒、快乐等七类情感，通过自动化生成与迭代进化机制（如问题复杂度提升和选项深度优化）来确保数据质量。同时，作者设计了一种多智能体协作框架来提升模型情感推理能力，具体来说，该框架包括四个核心智能体：背景智能体、角色智能体、事件智能体和决策智能体。背景智能体负责理解视频中的背景信息，角色智能体关注角色的动态变化，事件智能体分析视频中的事件细节，而决策智能体则综合前述信息，做出情感理解和推理的决策。实验上，作者使用他们的数据集测试了多个开源MLLM，发现现有模型在这项任务中仍然面临重大挑战，并且负面情绪的推理往往比对积极情绪的推理更具挑战性。
>
> >[2025-10-26]-[戴逦]-[ Versatile Diffusion: Text, Images and Variations All in One Diffusion Model]
> >
> ><img width="512" height="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/VD.PNG" />
> >
> >论文简述：论文提出一个统一的多模态扩散生成框架——Versatile Diffusion(VD),能够在同一个模型中同时处理text-to-image、image-to-text和image-variation等多种任务。论文的核心创新是提出了multi-flow，通过共享可切换的模块实现跨模态信息的协同建模，大幅提升了模型的多任务泛化能力。VD在一个统一架构中融合了VAE、CLIP等组件，既能捕捉跨模态语义关系，又能进行风格-语义的解耦、双上下文与多上下文融合生成。实验表明，VD 在图像与文本生成任务上均优于或相当于现有单任务模型（如Stable Diffusion、BLIP），并能实现多模态融合的创新应用，为构建通用生成式人工智能提供了重要方向。
> >
> > [2025-10-26]-[陈银]-[本周未精读]
> >
> >[2025-10-26]-[张雪松]-[MoMa-Kitchen: A 100K+ Benchmark for Affordance-Grounded Last-Mile Navigation in Mobile Manipulation](https://momakitchen.github.io/)
> >
> ><img width="512" height="512" alt="image" src="https://github.com/user-attachments/assets/4e15748f-bfca-4a2e-8564-4f0c86cf9031" />
> > 
> >论文简述：本文主要关注移动操纵（Mobile Manipulation）时的导航可供性问题(Affordance-Grounded),即agent在最后一步停止时候base和目标object的距离是否可达以及操作是否可行，比如中间是否有障碍物。为此，本文提出了含有100k+个厨房场景的样本的benchmark（MoMa-Kitchen），还开发了轻量级基准模型NavAff（和大多数vla模型一样，将RGBD等输入拼接一起，然后经过几层transformer输出融合预测结果），用于导航能力的具身化建模。我们的方法使模型能够学习基于操作能力的最终定位，适应不同类型的机械臂和平台高度，从而为具身智能中**导航与操作的更稳健、更通用的集成**铺平道路。
> > 
> >[2025-10-25]-[程浩]-[本周未精读] \
> >

> >[2025-10-19]-[陈乾]-[A NOVEL APPROACH FOR MULTIMODAL EMOTION RECOGNITION : MULTIMODAL SEMANTIC INFORMATION FUSION]
> >
> ><img width="512" height="512" alt="image" src="https://github.com/2003-0224/PapperImage/blob/main/(%7DOSH49BBNBFX)~%7DY~P0I_T.png" />
> >
> >论文简述：本文提出了一种新的多模态情感识别方法DeepMSI-MER，通过融合对比学习与视觉序列压缩，解决了传统情感识别方法在模态异质性、特征冗余和模态关联利用不足等问题。模型首先利用 BERT 和 Wav2Vec 提取文本与音频的高层语义特征，并与视频模态特征进行融合，构建跨模态语义信息。视觉序列压缩模块通过筛选关键帧并减少冗余数据，提高了视觉特征的表达效率。随后模型采用基于对比学习的特征融合策略，加强模态间的一致性与区分能力。在IEMOCAP与MELD两个公开数据集上的实验结果显示，DeepMSI-MER在识别准确率和鲁棒性方面显著优于多种先进基线模型，平均准确率分别达到 84.7% 和 69.4%，尤其在区分相似情绪（如愤怒与沮丧）时表现突出。消融实验进一步验证了三模态融合的有效性，其中视觉模态贡献最大。该研究表明，对比学习与视觉压缩相结合的多模态融合策略能有效提升情感识别的精度与泛化能力。
> >
> >[2025-10-19]-[于洋晨]-[本周无精度]
> 
> >[2025-10-19]-[张宇]-[Advancing Textual Prompt Learning with Anchored Attributes]
> >
> ><img width="512" height="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/ATPrompt.png" />
> >
> >论文简述：本周重新回顾了ATPrompt。作者发现传统的文本提示仅在类别维度上学习软提示，容易过拟合已见类别而难以泛化，于是将若干固定的属性 token（如颜色、形状等）并入软提示，使提示同时携带类别信息与属性信息，从而把提示空间从一维的“类别”扩展为多维的“属性—类别”混合空间。为自动确定最合适的属性集合，论文设计了一个流水线：先用大语言模型生成属性基，再构造所有可能组合形成候选池，随后采用类似 DARTS 的可微分搜索（对候选路径做 softmax 放松并交替优化提示参数与路径权重）来选择最优属性组合。方法包含浅层与深层两种变体以兼容不同深度的提示注入方式，且在深层变体中仅对类别相关的软提示做丢弃与重加以保持属性表示的连续性。而它的局限在于：单独依赖属性锚定难以完全取代基于正则化的先进方法，属性锚点的位置仍需人为选择。
> >
> >[2025-10-19]-[何艺超]-[Explainable Multimodal Emotion Recognition]
> >
> ><img width="512" height="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/EMER.png" />
> >
> >论文简述：是大模型应用在情感识别领域比较早的工作。传统的多模态情感识别工作通常假设基准数据集的标签准确，并专注于开发更有效的架构。本文借用大型语言模型（LLM）来消除单模态线索的歧义，生成更完整的多模态解释。通过预标注（GPT4V） + Two-Round Checks（人工） + 消除歧义（GPT3.5）得到了322条高质量的EMER数据条目，并初步设计了OV-MER的评价标准，在下游多个MLLM上进行了测试，说明了当前通用大模型在MER任务上的不足。
> >
> >   [2025-10-19]-[贾朋]-[Cafe-Talk: Generating 3D Talking Face Animation with Multimodal Coarse- and Fine-grained Control](https://arxiv.org/abs/2503.14517)
> > <img width="512" alt="image" src="https://raw.githubusercontent.com/2351548518/images/main/20250709/20251016213016982.png">
> >
> > 论文简述: 现有方法仅采用离散情感标签实现序列级全局表情控制，导致时空细粒度面部控制灵活性不足。作者提出基于DiT的3D人脸生成模型Cafe-Talk，通过同时整合粗、细粒度多模态控制条件解决该问题。针对多种条件耦合带来的性能挑战，采用两阶段训练策略：首先仅使用语音与粗粒度条件训练模型，然后通过设计的细粒度控制适配器逐步引入AU表征的细粒度指令，避免语音-唇形同步质量下降。
> >
> > 
> >[2025-10-19]-[戴逦]-本周未精读
> >
> >  [2025-10-19]-[徐赟博]-本周未精读
> >
>>[2025-10-19]-[李欣雨]-[Revealing Personality Traits: A New Benchmark Dataset for Explainable Personality Recognition on Dialogues]-EMNLP2024
>>
>><img width="512" height="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/CoPE.png" />
>>
>>论文简述：文章提出了新的可解释的人格识别，区别于传统人格识别研究无法解释模型的预测结果，实现“从行为到状态，再到特质”的人格的链式解释。文章将人格识别建模为一个三段式推理过程，提出了CoPE框架（Chain-of-Personality-Evidence）：1.从对话中提取具体行为/情感证据；2.推理出短期人格状态；3.综合多状态归纳出长期人格特质。构建了 2000 段中文对话的数据集PersonalityEvd，定义了两项子任务：EPR-S（状态识别），给定一段对话，预测目标角色在某人格维度上的状态标签，并提供证据；EPR-T（特质识别），给定某角色的全部对话，预测其人格特质标签，并提供跨对话的证据总结。文章使用了多种模型进行实验，实验结果表明大模型在“给出自然语言推理”上远远不及人类，但引入状态证据可以显著提升特质识别（+6%），验证了可解释路径必要性。
> 
>>[2025-10-18]-[陈银]-[MGM: Motion-Guided Masking for Spatiotemporal Representation Learning]-ICCV2023
>>
>><img width="512" height="image" alt="image" src="https://raw.githubusercontent.com/cyinen/imge/master/20251014161016.png" />
>>
>>论文简述：针对当前掩码自编码器（VideoMAE）大多沿用MAE随机掩码策略，但是对视频来说，时空信息都是很重要，简单的随机掩码不一定足够有效。本文提出借助压缩视频本身自带的运动信息来引导掩码。具体来说就是根据压缩视频的motion, 对每一帧中运动最剧烈的地方生成一个掩码块，并加入一些随机的抖动，使得模型去学习这些运动剧烈，信息丰富的区域。实验表明这种掩码方式要比tube mask要好，尤其是在运动剧烈的数据集上。
> 
> >[2025-10-18]-[程浩]-[本周未精读] \
> >[2025-10-17]-[张雪松]-[Long-VLA: Unleashing Long-Horizon Capability of Vision Language Action Model for Robot Manipulation][CoRL 2025](https://long-vla.github.io)
> >
> > <img width="512" height="image" alt="image" src="https://github.com/user-attachments/assets/c791f9f3-dff6-407f-bb88-ca799707e780" />
> >
> > 论文简述: 本文提出了一种端到端的Long-VLA模型致力于解决长程的VLA任务（移动+交互）。具体来说，为了解决之前长程VLA方法在移动和交互之间的动态耦合和误差传播，本文提出了一种阶段性的输入mask策略，自适应地分割把每个子任务分割为移动和交互两个阶段，在move阶段mask掉抓取器的view，交互阶段mask掉third-person camera views。这一能力的实现得益于对数据集进行“阶段性”解耦。本文进一步提出了L-CALVIN benchmark评估长程的操纵能力。实验分析了Long-VLA在模拟和真实环境下，这种策略的有效性。
> >
> >[2025-10-12]-[李欣雨]-[本周未精读]
> >
> >[2025-10-12]-[程浩]-[本周未精读]
> >
> >[2025-10-12]-[张宇]-[本周未精读]
>
> >[2025-10-12]-[于洋晨]-[本周未精读]
>
> > >[2025-10-12]-[张雪松]-[本周未精读]
>
> >[2025-10-12]-[何艺超]-[本周未精读]
> 
> >[2025-10-12]-[贾朋]-[MEDTalk: Multimodal-Controlled 3D Facial Animation with Dynamic Emotions by Disentangled Embedding](https://arxiv.org/pdf/2507.06071)
> > <img width="512" alt="image" src="https://raw.githubusercontent.com/2351548518/images/main/20250709/20251012222119245.png">
> >
> > 论文简述: 音频驱动的情感3D面部动画旨在生成同步的唇部运动和面部表情。然而，大多数现有方法专注于静态和预定义的情感标签，限制了多样性和自然性。作者提出了 MEDTalk，一个用于生成细粒度和动态情感头像的新型框架。首先通过交叉重建过程，将内容和情感嵌入空间从运动序列中解耦，实现对唇部运动和面部表情的独立控制。整合音频和语音文本预测逐帧的强度变化以动态调整静态情感特征。为了增强控制和个性化还结合了多模态输入（文本描述和参考表情图像）。
> 
> >[2025-10-12]-[戴逦]-[DialogueLLM: Context and emotion knowledge-tuned large language models for emotion recognition in conversations.]
> >
> ><img width="512" height="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/DialogueLLM.PNG" />
> >
> >论文简述：本文提出一种针对对话情绪识别（ERC）的专用大型语言模型DialogueLLM。该模型以LLaMA 2-7B为基础，通过LoRA引入情绪与上下文知识，将ERC从传统分类任务重构为条件文本生成任务。作者构建了一个包含三大基准数据集（MELD、IEMOCAP、EmoryNLP）共2.4万条语句的情绪知识指令数据集，并利用ERNIE Bot生成与语句时间对齐的视频文本描述，使模型能融合视觉与语言特征。DialogueLLM的输入由上下文语句、视频描述、目标语句及情绪指令组成，通过提示模板驱动模型生成情绪标签，模型在三大基准上显著优于15个最先进ERC模型及3个通用LLM。
>
> >[2025-10-12]-[陈乾]-[BEYOND SIMPLE FUSION: ADAPTIVE GATED FUSION FOR ROBUST MULTIMODAL SENTIMENT ANALYSIS](https://arxiv.org/pdf/2510.01677)
> >
> ><img width="512" height="512" alt="image" src="https://github.com/2003-0224/PapperImage/blob/main/AGFN.png" />
> >
> >论文简述：多模态情感分析（MSA）融合文本、语音和视觉信息以提升情感识别精度，但传统融合方法难以处理模态噪声与语义冲突。本文提出自适应门控融合网络（AGFN），通过IEG，MIG建模样本级权重，实现动态加权融合。消融实验验证两种门控机制在鲁棒性与细粒度情感识别中的关键作用。AGFN在CMU-MOSI与CMU-MOSEI数据集上取得SOTA性能，并通过特征可视化与预测空间相关性分析证明其具备更强的抗噪与泛化能力。
> >
> > [2025-10-12]-[徐赟博]-[Rethinking the Embodied Gap in Vision-and-Language Navigation: A Holistic Study of Physical and Visual Disparities]-[iccv2025]
> > 
> ><img width="512" height="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/VLNPE.png" />
> >
> >论文简述：本文提出了一种在新模拟器上的VLN数据集，主要是在观察图像层面进行的改进。通过人型机器人/轮式机器人/腿式机器人的真实数据采样，可以模拟出agent在导航过程中随着动作的改变，观察图像进行物理属性上的改变，比如相机高度和视角的偏移，光照的变化等多种物理元素。这种新数据集结构跨越了之前纯模拟的导航环境，可以进一步的缩短模拟器环境下的导航和实际部署的具身实体导航之间的差异。代码开源，考虑follow
> > 
>>[2025-10-12]-[陈银]-[DFormerv2: Geometry Self-Attention for RGBD Semantic Segmentation]-CVPR2025
>>
>><img width="512" height="353" alt="image" src="https://github.com/user-attachments/assets/ea8f8ba0-0d44-4ee6-bc05-4bf6e5e2e686" />
>>
>>论文简述: 传统的RGBD语义分割虽然将深度信息引入作为一种有效的辅助信息，但是对深度信息的处理主要依赖另一个独立的encoder进行编码，然后和RGB信息编码融合。而本文从数据角度出发，使用深度信息生成几何先验来引导注意力的计算，分配注意力权重，同时将原来的注意力沿着x, y轴分解，在不降低性能的同时明显减少了计算量。该方法在RGBD分割任务上取得了新的领先性能。
> >
> >[2025-09-28]-[陈乾]-[Sync-TVA: A Graph-Attention Framework for Multimodal Emotion Recognition with Cross-Modal Fusion]
> >
> ><img width="512" height="512" alt="image" src="https://github.com/2003-0224/PapperImage/blob/main/8c63abd665b17f4186b416c106481dea.png" />
> >
> >论文简述：本文提出了 Sync-TVA，一个用于多模态情感识别的图注意力框架，重点解决在多模态融合中跨模态交互有限及模态贡献不均的问题。模型主要包含两个部分：模态特定动态增强模块（modality-specific dynamic enhancement）：为每个模态（文本、音频、视觉）设计专门的模块，用以“净化”与强化该模态内部最情绪相关、最有用的特征。异构跨模态图结构 + 跨注意力融合机制：构建模态间语义关系图（heterogeneous cross-modal graph），然后通过跨注意力（cross-attention）机制对这些模态线索（cues）进行对齐与融合，以得到更鲁棒的情感推断。实验中使用了 MELD 和 IEMOCAP 两个常用多模态情感识别数据集。结果显示 Sync-TVA 在 准确率（Accuracy） 和 加权 F1 分数（Weighted F1） 等指标上，相比现有最新模型有持续的提升，尤其在类别不平衡的情形下优势明显。
>
> >[2025-09-28]-[于洋晨]-[本周一直在尝试做实验，未精度]
> 
>>[2025-09-28]-[戴逦]-[MemoCMT: multimodal emotion recognition using cross-modal transformer-based feature fusion]
> >
> ><img width="512" height="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/MemoCMT.png" />
> >
> >论文简述：提出 MemoCMT，一种基于跨模态 Transformer 的多模态情感识别架构，解决了单模态模型难以同时捕获局部与全局情感特征的问题。论文方法利用 HuBERT 作为语音特征编码器、BERT 作为文本特征编码器，并通过跨注意力机制实现语音与文本特征的对齐，随后结合多种聚合策略（CLS、MEAN、MAX、MIN）压缩融合表征，其中 MIN 聚合展现出最优效果。在 IEMOCAP、ESD 与 MELD 三个公开数据集上进行训练与测试，MemoCMT 在W-Acc与UW-Acc上均显著超越现有主流方法，表现出该模型在跨模态特征建模、鲁棒性方面的强大能力。
>
>>[2025-09-28]-[贾朋]-[UniTalker: Scaling up Audio-Driven 3D Facial  Animation through A Unified Model](https://arxiv.org/pdf/2408.00762)
> > <img width="512" alt="image" src="https://raw.githubusercontent.com/2351548518/images/main/20250709/20250928214210014.png">
> >
> > 论文简述: 音频驱动的3D面部动画。3D Talking Head虽已取得显著进展，但由于3D标注标准不一致，先前模型仅能基于特定标注数据进行训练，限制了训练规模。作者提出统一模型UniTalker，其多头部架构可有效利用多样化标注数据集。为提升训练稳定性并确保多头部输出一致性，采用三项训练策略：PCA降维、模型预热和身份嵌入。为扩增训练规模与多样性，作者构建A2F-Bench数据集，将训练数据从常规使用不足1小时的规模扩展至18.5小时。
>
>>[2025-09-28]-[何艺超]-[Listen, Watch, and Learn to Feel: Retrieval-Augmented Emotion Reasoning for Compound Emotion Generation]-[ACL 2025](https://aclanthology.org/2025.findings-acl.590/)
>>
>><img width="512" height="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/RAER.png" />
> >
>>论文简述：通过RAG增强MLLM情感推理，作者提出检索增强情绪推理（RAER）框架。具体来说，1.模型首先以 prompt + 多模态输入为条件，调用基础模型生成一个初始响应。（这个初始相应可能包含多个子步骤、多个情绪成分或者推理中间状态）2.将初始响应分解为一系列推理步骤，把模型的回答“拆开”成多个小片段。3.用上一步的推理结果（或到目前为止的上下文）作为查询，检索知识库K中与之相似的样本。 这些检索到的示例包含多模态输入 + 情绪描述或推理的结果，对当前情绪推理提供参考。4.如果当前步骤被判定为“模糊”，模型会调用一个 CoT 推理子模块，它以当前检索到的示例为输入，即在已有推理基础上，融合检索知识库中的参考示例，增强或修正当前判断，从而得到更稳健的结果。实验数据集包括MER2024、DFEW 和 IEMOCAP，结果表明，RAER可以增强MLLM的zero-shot下的情感推理结果，但是和专有模型分类准确率上比较仍有一定差距。
>
>>[2025-09-28]-[张宇]-[Compound Expression Recognition In-the-wild with AU-assisted Meta  Multi-task Learning]-[CVPR23](https://openaccess.thecvf.com/content/CVPR2023W/ABAW/papers/Li_Compound_Expression_Recognition_In-the-Wild_With_AU-Assisted_Meta_Multi-Task_Learning_CVPRW_2023_paper.pdf)
>>
>><img width="512" height="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/CFER-AU.png" />
> >
>>论文简述：这篇工作系统分析了真实场景下AU的激活分布，发现野外表情的AU组合和实验室里差别很大——常见原型AU频率下降、一些在实验室少见的AU在真实场景反而更常见。基于这些观察，作者用了一种带AU辅助的元学习多任务框架，把表情识别作为主任务、AU检测作为辅助任务，通过双向分布对齐损失让两种预测相互约束；同时用一个小型元网络在 meta-train → meta-test → base-learning 的循环中动态学习辅助损失与对齐损失的权重，从而降低固定权重带来的负迁移风险。可以这样通俗的理解这三个过程为一个过程：首先随机初始化元网络控制的参数Wau和Wam，然后用val上复合表情准确率来反向更新这两个参数，最后这两个参数越来越准确，多任务模型收敛越来越快（在更新这两个参数的时候，Net的参数也在更新）,可以说和AU-DFER一样都是基于统计学先验的知识注入的AU辅助FE的工作。
>>
>>[2025-09-28]-[李欣雨]-[Target-Dependent Multimodal Sentiment Analysis Via Employing Visual-to Emotional-Caption Translation Network Using Visual-Caption Pairs]
>>
>><img width="512" height="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/VECTN.png" />
> >
>>论文简述：这篇论文提出了一种创新的多模态情感分析框架——VECTN。VECTN框架由三大核心模块组成：人脸情绪描述模块通过人脸识别和属性分析模型提取年龄、性别、种族和情绪等信息，并基于模板生成自然语言描述；目标对齐与描述精炼模块对多张人脸利用对比学习机制（类似CLIP）计算图像与人脸描述之间的语义相似度，筛选出与目标实体最相关的面部情绪描述；融合模块将原始文本、目标实体、精炼后的人脸描述以及图像场景描述拼接成两组输入，分别送入两个RoBERTa-Large模型进行编码，并通过门控机制融合特征，有效降低噪声干扰，实现更精准的情感预测。实验在Twitter-2015和Twitter-2017两个公开数据集上进行。结果表明，VECTN在所有数据集上均显著优于现有最佳方法。
>>
>>[2025-09-28]-[徐赟博]-[View Invariant Learning for Vision-Language Navigation in Continuous Environments]
>>
>><img width="512" height="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/V2VLNCE.png" />
> >
>>论文简述：vln的setting和VLA有一个巨大的差距在于，VLN中接受固定相机高度的图像，而VLA的视觉输入会随着本体的运动发生抖动，比如相机高度和角度会发生变化，甚至会因为本体的抖动产生模糊的图像。本文讨论了这个问题，在同一个视角的全景输入中，动态的采样了来自不同相机高度的view图像，结果发现之前的工作对这种相机高度的变化非常不鲁棒。作者提出了一种用于学习视角不变性策略的方法。通过一种双阶段蒸馏的方法，用一个接受固定高度的教师模型和一个接受可变高度的学生模型，让学生模型在预测可导航点的logits逼近教师模型学习固定视角的logits；同样，教师/学生模型在接受不同高度的视觉输入和指令进行跨模态注意力后的embedding特征也进行蒸馏。实现了在作者所说的这种可变高度的新setting上的鲁棒性
>>
>> [2025-09-26]-[陈银]-[Temporal Group Constrained Transformer with Deformable Landmark Attention for Video Dimensional Emotion Recognition]-TAFFC
>>
>> <img width="532" height="512" alt="image" src="https://raw.githubusercontent.com/cyinen/imge/master/20250925213353.png" />
>>
>> 论文简述: 本文提出了一种针对视频维度情感识别（帧级别的AV预测）的Transformer模型，旨在解决空间特征提取和时序稳定性建模两个问题。首先通过改进 Deformable 注意力，将原来的均匀初始化的参考网格点改成利用landmark先验初始化的点，然后通过高斯采样，并自动学习一个偏移，精确聚焦与人脸表情相关的区域如眼睛，眉毛，嘴等；对于时间建模，提出多层级的组归纳方法，在组级别的宏观层面进行信息交互，然后利用这种稳定的信息来指导或者纠正某些单帧预测，缓解因为遮挡或者表情模糊导致预测结果抖动剧烈的”闪烁问题"。实验证明了这种方法的有效性
>>
>>
>> [2025-09-25]-[张雪松]-[VLA-ADAPTER: AN EFFECTIVE PARADIGM FOR TINY-SCALE VISION-LANGUAGE-ACTION MODEL](https://vla-adapter.github.io/)
>> <img width="532" height="512" alt="image" src="https://github.com/user-attachments/assets/88545bd9-5eaa-45bc-bda2-b217f5077061" />
>>
>> 论文简述: 本文首先调查如何有效地桥接(bridge)视觉语言表征到动作（想法可能类似blip2桥接图像到大语言模型，但方式不一样），探索了mid/last/all layer特征以及action query的作用。基于这一调查，本文提出了一种轻量的含有bridge注意力的Policy模块，本文方法的backbone基于0.5B的qwen2.5，在LIBERO和CALVIN两个benchmark上，实现了较高的VLA agent表现。开源性较好。
>>
>>[2025-09-21]-[徐赟博]-[ DAgger Diffusion Navigation: DAgger Boosted
 Diffusion Policy for Vision-Language Navigation]-[arxiv](https://arxiv.org/pdf/2508.09444)
>>
>><img width="512" height="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/DifNav.png" />
>>
>>论文简述：本文在VLN-CE开辟了新的训练范式，取代了两阶段训练（先训练一个waypoint predictor来逼近离散导航的predifined graph的作用，然后在冻结waypoint predictor的基础上进行端到端训练）。本文引入了新的训练方法，以diffusion policy的的方式生成新的action，而非之前的模仿学习方式，改善了之前训练方式中的累计误差问题，在联合训练的基础上大幅提升了导航性能。能够在抛弃预定义连通图基础实现不错的导航性能，是逼近general领域的很重要一步。目前在等待该代码完全开源，目前只开源了测试部分。
>>
>>[2025-09-21]-[张宇]-本周赶实验，未精读
>>
>>[2025-09-21]-[贾朋]-本周未精读
>>
>>[2025-09-21]-[于洋晨]-本周未精读
>>
>>[2025-09-21]-[何艺超]-本周未精读
>>
>>[2025-09-21]-[戴逦]-[Learning Transferable Visual Models From Natural Language Supervision]
>>
>>论文简述：本论文提出了一种基于自然语言监督的大规模视觉表征学习框架——CLIP。CLIP利用海量图文对作为弱监督信号，通过双编码器结构（图像编码器与文本编码器）分别将多模态输入映射到同一语义嵌入空间，并采用最大化匹配对相似度、最小化非匹配对相似度的方式来实现模型的训练。CLIP与传统依赖类别标签的监督学习不同，它将自然语言视为开放词汇的监督源，从而赋予模型强大的零样本迁移能力：在推理阶段，模型无需微调即可通过计算图像与候选文本描述的相似度完成分类、检索等任务。实验结果表明，CLIP在超过30个视觉基准任务上展现了与全监督方法相当甚至更优的性能，凸显了自然语言监督在构建可扩展、多任务视觉模型中的潜力。
>>
>>[2025-09-21]-[李欣雨]-[A multi-model attention based CNN-BiLSTM model for personality traits prediction based on user behavior on social media]-[Knowledge-Based Systems]
>>
>>论文简述：这篇论文提出了一种融合文本语义与社交网络特征的多模态人格预测模型。模型利用BERT进行动态词嵌入，结合CNN提取局部特征、BiLSTM建模上下文依赖，再通过注意力机制聚焦关键语义信息，最后融合网络结构特征（如网络密度、中介性等）进行Big Five人格分类。实验在myPersonality数据集上进行，数据集包含250位Facebook用户状态与标签数据，结果显示该模型在开放性和外向性上的预测准确率超过78%。
>>
>>[2025-09-21]-[陈乾]-[EmoBench-Reddit: A Hierarchical Benchmark for Evaluating the
Emotional Intelligence of Multimodal Large Language Models]
>>
>>论文简述：这篇论文提出了一个名为 EmoBench-Reddit 的全新分层式基准，用于系统评估多模态大语言模型（MLLMs）的情感智能。研究者指出，现有评测多偏重于客观任务（如问答、图像描述），难以检验模型对复杂、主观人类情感的理解能力。为填补这一空白，他们从 Reddit 精选构建了 350 个真实的图文样本，涵盖悲伤、幽默、讽刺、快乐四类情感，并设计了由感知到认知的分层任务框架：从基础的颜色识别、物体定位，到高阶的意图识别与深度推理，逐步模拟人类从“看到”到“理解”的过程。数据标注采用 AI 协助与人工验证结合，保证了高质量与一致性。通过对 GPT-5、Gemini-2.5-pro、GPT-4o 等九个代表性模型的全面测试，结果表明：顶级模型在基础感知与初级认知表现稳定，但在复杂情感推理上普遍失分，尤其是讽刺任务成为所有模型的“拦路虎”；相对简单的快乐与幽默情感更易被识别。该研究揭示了当前模型的“感知-认知鸿沟”，指出深层推理、上下文建模及文化语境理解是未来突破方向。EmoBench-Reddit 不仅为衡量 MLLM 的情感智能提供了有效工具，也为推动 AI 向更具同理心与人类对齐的情感理解能力发展奠定了基础
>>
>> [2025-09-21]-[张雪松]-[CAST: Counterfactual Labels Improve Instruction Following in Vision-Language-Action Models]
>> <img width="532" height="512" alt="image" src="https://github.com/user-attachments/assets/926a7bf1-23d1-4ac3-864f-163907914b13" />
>> 论文简述 本文提出 CAST (Counterfactual Augmentation with Synthetic Trajectories)，一种利用视觉语言模型自动生成反事实语言与动作标签的新型数据增强方法。该方法在无需额外数据采集的情况下，丰富了机器人轨迹数据的语义多样性与精细度，有效缓解语言忽视问题。基于CAST训练的 CounterfactualVLA 显著提升了机器人在复杂指令跟随任务中的表现，在真实环境导航实验中成功率提升27%，并超越多项现有先进方法，验证了反事实标注在指令跟随中的潜力。
>>
>> [2025-09-21]-[陈银]-[Grounding Emotion Recognition with Visual Prototypes: VEGA -- Revisiting CLIP in MERC]-MM2025
>>
>> 论文简述：这篇论文提出了一个名为 VEGA 的创新框架，用于解决多模态对话中的情感识别问题。其核心思想是颠覆性地利用 CLIP 的视觉编码器，从真实世界的面部表情图像中提取稳定的、具有心理学意义的视觉原型作为“锚点”，以此来引导和规范多模态特征的学习过程。通过设计的双分支架构、随机锚点采样和自蒸馏机制，VEGA 不仅在 IEMOCAP 和 MELD 数据集上实现了新的 SOTA 性能，更重要的是，它为情感计算领域提供了一个模块化的、受认知理论启发的、更接近人类情感感知机制的解决方案，为未来构建语义结构化和人类对齐的情感智能系统铺平了道路。
>>
>> <img width="512" height="512" alt="image" src="https://raw.githubusercontent.com/cyinen/imge/master/20250818140342.png" />
>
> >[2025-09-14]-[戴逦]-[MM-LLMs: Recent Advances in MultiModal Large Language Models]
> >
> >论文简述：本文是一篇多模态大模型综述论文，作者比较了2024年及之前的各种多模态大模型的性能，并提出一种大模型研究范式：将文本外的其他模态进行对应的编码器提取特征，接着将各种特征与文本特征进行对齐，再输入到各种大模型中得到输出，输出再经过一个投影层来对应到不同的任务中。
> >
> >  [2025-09-14]-[于洋晨]-本周未精读
> 
> >  [2025-09-14]-[张宇]-本周复习之前的论文和赶实验，未精读
> > 
> > [2025-09-14]-[何艺超]-[AER-LLM: Ambiguity-aware Emotion Recognition Leveraging Large Language Models]-[ICASSP 2025]
> >
> > 论文简述：本文主要探索LLM在模糊情感方面的潜力，作者认为当前的LLM往往仅关注单一的情绪标签，忽视了情绪的模糊性。于是作者设计了一个探索的框架，数据集方面使用MSP-Podcast, IEMOCAP 和 GoEmotions中的4种标签，招募多个标注员从4种标签中做出选择，从而得到每个样本的4种标签的GT概率分布。模型上，调用了Gemini-1.5-Flash的API，将数据集中的对话文本以及文本化后的语音特征输入给LLM，分别进行Zero-shot和Few-shot级别的提问，让LLM也输出一个概率分布，通过比较两种概率分布来测试LLM在面对模糊情感识别方面的潜力，并用熵来衡量情绪标注中的歧义程度。从实验结果上看，通过增大上下文范围，也即增多对话语境可以增强LLM的模糊情感识别，使得模型和人工GT的概率分布更接近，与此同时，few-shot相比于zero-shot可以增强大模型对于模糊情感任务的理解，也可以使得LLM生成更倾向于GT概率分布的答案。
> > 
> > [2025-09-14]-[徐赟博]-赶稿子，未精读
> > 
>> [2025-09-14]-[张雪松]-帮我徐赟博赶icassp论文,未精读
>> 
>>  [2025-09-14]-[李欣雨]-[Emotion-Assisted Multi-modal Personality Recognition using Adversarial Contrastive Learning]-[Knowledge-Based Systems]
>>
>> <img width="512" height="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/EAPRC.jpg" />
>>
>> 论文简述：文章提出了 EAPRC 模型，包含特征提取模块、对抗对比学习模块、模态融合与回归模块和情绪辅助策略模块。在特征提取模块主要提取图片、音频、文本特征。在对抗对比学习模块引入对抗扰动生成对抗样本，提升模型鲁棒性（首次将对抗对比学习引入人格识别）。使用 Transformer 编码器进行多模态特征融合通过自注意力机制捕捉模态间和时序依赖关系。最后使用 DeepFace 提取情绪特征，将情绪得分映射到五大人格维度，加权融合到最终预测结果中。该模型在 ChaLearn 数据集和ELEA 数据集上均优于现有方法，具有良好的泛化能力和实用前景。
>
> 
>> [2025-09-14]-[陈银]-[Text Prompt Region Decomposition for Effective Facial Expression Recognition]-[TAFFC]
>> 

>> <img width="512" height="512" alt="image" src="https://github.com/user-attachments/assets/f8a782a3-67c8-48e3-99c3-e717af8e3cf2" />
>> 
>> 论文简述：在 CLIP 中，视觉与语言的 embedding 具有较高的一致性。例如，在文本空间中，人脸整体的 embedding 与眼睛、嘴巴等局部区域的 embedding 更为接近，而与树木等无关对象的 embedding 差距更大。这表明视觉的 global embedding 已经隐含了关于人脸区域的丰富表征。基于这一观察，本文提出利用文本提示（如额头、眼睛、眉毛等区域的描述）来引导对人脸特征进行解耦，从 global visual embedding 中提取区域相关的特征，从而降低对显式 landmark 的依赖。实现了在FER任务上的SOTA。
>
> 
> >[2025-09-14]-[陈乾]-[Towards Multimodal Empathetic Response Generation: A Rich Text-Speech-Vision Avatar-based Benchmark]-[www 2025](https://AvaMERG.github.io/)
> >
> ><img width="512" height="512" alt="image" src="https://github.com/2003-0224/PapperImage/blob/main/481f7720a973a095bd328ec120a1e220.png" />
> >
> >论文简介：本文作者提出MERG任务，引入文本、语音和面部视觉信息，并构建第一个大规模MERG基准数据集 AvaMERG，此外，为了实现端到端 MERG，作者基于多模态大语言模型（MLLM）设计了Empatheia系统，采用HuBERT 和 CLIP ViT-L/14@336px作为语音和视频编码器，将 Vicuna作为LLM Beckbone，同时引入共情链推理机制，逐步引导 LLM 进行渐进式地进行思考。为了保证多模态在内容和风格上保证一致性，作者设计了两个模块：内容同步器(CS)和风格解耦器(SD)，最终实验表明Empatheia 系统显著优于基线方法，且与自动评估一致，验证了多模态信息对共情理解和生成的提升作用。
>
<<<<<<< HEAD
=======
> >[2025-09-07]-[贾朋]-本周未精读
> >
> >[2025-09-07]-[戴逦]-[DEEMO:De-identity Multimodal Emotion Recognition and Reasoning]
> >
> >论文简述：本文提出了一种去身份化多模态情感识别与推理框架(DEEMO)，通过整合去身份化视频、音频和文本线索，并强调非面部肢体语言作为隐私友好模态，实现情感状态的隐私保护建模。该方法在避免身份敏感信息泄露的同时增强了模型对多模态情感任务的鲁棒性和泛化能力。
>
> >[2025-09-07]-[李欣雨]-[MPRNet: A Temporal-Aware Cross-Modal Encoding Framework for Personality Recognition]-[IEEE Transactions on Affective Computing]
> >
> ><img width="512" height="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/MPRNet25.9.7.jpg" />
> >
> >论文简述：本文提出了一种用于人格识别的多模态时序编码感知框架MPRNet，并构建了一个新的人格数据集NEUPR。MPRNet是一个统一的多模态人格识别框架，有两个核心的模块：一是多模态编码模块，使用 LSTM 建模各模态（音频、面部、眼动、文本）的时序特征，同时引入 BERT 提取的语义嵌入用作分类人格特征的辅助指导，二是特征增强模块，使用通道注意力机制，动态调整不同特征通道的重要性。本文提出的新的人格数据集NEUPR，其优势在于每个会话中时间的丰富性，数据集的视频内容包括音频、面部表情、眼动数据和语音转录文本。同时，NEUPR 提供了更加自然的人格特征表达。因为参与者不仅将他们的 MBTI 结果作为个性标签提交，还记录下了自己在没有脚本或外部指示的情况下自由说话。这种无脚本和自我指导的形式能够捕捉真实的行为线索，与每个参与者自己对其性格特征的看法和呈现相一致。
> 
> >[2025-09-07]-[陈乾]-[Empathetic Response Generation Through Multi-modality]-[IEEE Transactions on Affective Computing]
> >
> ><img width="512" alt="image" src="https://github.com/2003-0224/PapperImage/blob/main/6c1e8ec58d6163cbb39046ba6fdd5369.png" />
> >
> >论文简述：本文关注现有共情回复生成（ERG）方法主要依赖文本，容易导致对用户情绪和情境理解不完整甚至偏差。为解决这一问题，作者提出 ERGM，并探索了三种关键方法：1：利用跨模态注意力机制融合文本、视觉和音频信息进行情绪识别，实现情感共鸣；2：引入由图像描述或人工标注生成的指导性文本，补充场景和用户体验信息；3：设计多源注意力机制，在生成过程中同时吸收对话历史与指导文本，从而提升情境理解。实验在 MELD、IEMOCAP 和 MEDIC 三个多模态数据集上显示，ERGM 在情绪识别准确率上较现有方法提升 约3.6%–7.8%，回复多样性也有一定的提升，而在人类评价中共情程度也显著提升。
>
>
> > [2025-08-31]-[于洋晨]-[Uncertain Multimodal Intention and Emotion Understanding in the Wild]-[CVPR2025](https://openaccess.thecvf.com/content/CVPR2025/papers/Yang_Uncertain_Multimodal_Intention_and_Emotion_Understanding_in_the_Wild_CVPR_2025_paper.pdf)
> > 
> > 论文简述：本文提出了MINE数据集，是不同于传统受控或固定模态、仅独立关注情感识别的数据集。它直接从真实社交媒体采集，原生呈现了多模态数据（文本、图像、视频、音频）的自然不确定性，也就是存在了自然的模态缺失。其次，它首次详尽标注了情感与意图的标签，以及它们之间广泛的联系的注释，突破了传统方法将二者割裂的局限。相对于其他与MELD数据集相似的数据集来说，该数据集提供了更加贴近于真实场景的测试基准。
>
> >[2025-09-07]-[何艺超]-[Do Multimodal LLMs See Sentiment?](http://arxiv.org/abs/2508.16873)
> >
>><img width="512" height="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/MLLMsent.png" />
> > 
> > 论文简述：提出MLLMsent框架，和以人为中心的情感识别任务不同，其直接考虑整个画面的情感，其本质是将大模型看成辅助的工具，验证大模型辅助工具+简单微调是否能超越传统方法，作者探索了三种方法: 1.图像直接情感分类；2.把图像通过MLLM转化成描述，然后使用Text LLM对描述进行情感分类；3.使用图像描述对语言模型进行微调，然后再进行描述的情感分类。实验表明，GPT-4o mini（Zeroshot对image进行描述） + ModernBERT(使用描述信息微调)后的结果超越传统的基于词典，CNN，Swin Transformer等基线模型，并且在未训练的数据集上泛化性也更好。相比较我们的任务，不是以人为中心的从直觉上看描述得会更准，因为其更偏向于整个画面得风格所带的情感，而以人为中心的难点就是，通过crop的人脸，难以进行和表情直接相关的自动化准确描述。
>
>>[2025-09-07]-[张宇]-[Face2Exp: Combating Data Biases for Facial Expression Recognition]-[CVPR22](https://openaccess.thecvf.com/content/CVPR2022/html/Zeng_Face2Exp_Combating_Data_Biases_for_Facial_Expression_Recognition_CVPR_2022_paper.html)
> >
>><img width="512" height="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/Face2Exp.png" />
> > 
> > 论文简述：利用 FR 的大规模无标签数据，来帮助 FER（但不是使用掩码自监督的学习范式），但同时通过元学习消除FER（专注于表情）和FE（专注于身份，光照以及姿态）的领域偏差，具体分为三个模块，base网络，adaptation网络，反馈机制。base：只在 FER 上训练 + 在 FR 上打标签。adaptation：在 FR 上训练 + 在 FR 和 FER 上测试，比较差异。反馈：adaptation 的测试差异回流到 base，驱动 base 改进伪标签，结果就是adaptation 学到的不是 FR 数据原本的偏差，而是被 base “矫正过”的表情知识，从而让 adaptation 下次训练的更好。但存在一个致命的局限->受限于FER数据集的质量，作为初始的标准的答案，adaptation网络直接受它的影响。
> 
>>
>>[2025-09-07]-[徐赟博]-[Frequency-enhanced Data Augmentation forVision-and-Language Navigation]-[nips2023]
>>
>><img width="512" height="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/FDA.png" />
> > 
> > 论文简述：本文是一个从图像频域角度进行数据增强的工作。作者首先给了一个假设：VLN中视觉图像中的高频信息部分对于导航来说很重要。因为高频信息对应的边缘、轮廓、物体。而低频信息对应的是背景。首先，他在傅里叶频域对图像进行了扰动，发现出现了性能下降而高频扰动下降更多，证明了他的观点。接着他在频域进行了数据增强，方式包括，对于一个view，使用同一个viewpoint的其他view，随机选择RGB一个通道，经过高通滤波和低通滤波，和原始图像的对应channel混合，进而得到增强特征。但FDA使用增强特征的方式是一个训练过程使用两个特征，奇数轮次使用原始特征训练，偶数轮次使用增强特征训练。实验比较完备，故事写的也还好。
>>
>> [2025-09-07]-[陈银]-改论文，本周未精度
> 
> > [2025-09-05]-[张雪松]-[VLMO: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts](https://github.com/microsoft/unilm/tree/master/vlmo)
> > <img width="512" alt="image" src="https://github.com/user-attachments/assets/1addacec-599a-4d1e-82e1-5b56256e6956">
> > 
> > 论文简述: 本文提出了一种使用混合模态专家的视觉语言预训练模型(VLMo)，具体地，通过共享的Transformer骨干结合不同模态专家，实现图像、文本及跨模态的灵活建模。该方法在保持参数高效的同时增强了模型对单模态和多模态任务的适应性。大规模的仅视觉，仅语言和视觉语言数据分别用来分阶段地训练仅视觉，仅语言和视觉语言专家，这种统一的预训练范式能够满足不同的任务对模型架构的需求。根据此篇文章，后续拟提出ixture-of-Action-Experts用于导航和操纵。
> 
>>>>>>> 82ae37b2ef76a87a7d1bde5cf740265e4b2a8922
> > [2025-08-31]-[贾朋]-[LAM: Large Avatar Model for One-shot Animatable Gaussian Head](https://aigc3d.github.io/projects/LAM/)
> > <img width="512" alt="image" src="https://raw.githubusercontent.com/2351548518/images/main/20250709/20250807155920575.png">
> >
> > 论文简述: 通过单张图像实现可动画化高斯头部重建。与现有方法需要大量视频序列训练或依赖辅助神经网络进行推理时的动画生成与渲染不同，该方法可直接生成具备动画与渲染能力的高斯头部。框架的核心是规范空间高斯属性生成器，该组件以FLAME规范空间关键点为查询向量，通过Transformer与多尺度图像特征交互，精确预测规范空间内的高斯属性。重建后的规范高斯虚拟角色可采用FLAME模型的线性混合蒙皮技术驱动动画，并在多平台实现实时渲染。
> 
>  [2025-08-31]-[张雪松]-MoManipVLA: Transferring Vision-language-action Models for General Mobile Manipulation]-[CVPR2025]
> >
> ><img width="512" height="512" alt="image" src="https://github.com/user-attachments/assets/9edb6c32-3cab-4a23-9e0b-7e838da80e25" />
> >
> > 论文简述：论文提出MoManipVLA 框架，将预训练的视觉-语言-动作（VLA）模型从固定基座操控扩展到移动操控任务。传统方法在跨任务和环境的泛化性不足，而MoManipVLA利用VLA生成高泛化能力的末端执行器关键点，再通过运动规划目标（可达性、平滑性、避碰性）和双层优化框架，联合规划移动底盘与机械臂的轨迹。实验表明，该方法在OVMM和真实环境中比现有方法提升4.2%成功率。（本文未开源，估计也不开源）
> 
> > > [2025-08-31]-[徐赟博]-[Vision-and-Language Navigation via Causal Learning]-[CVPR2024](https://arxiv.org/pdf/2404.10241)
> >
> ><img width="512" height="512" alt="image" src="https://github.com/ReadingPapers/Report/assets/166887834/08d576d0-2c08-4669-9bd3-4c3ea10c43be" />
> > 
> > 论文简述：对该论文的重新精读。因为对该论文以因果的名义提取了大量视觉/语言特征作为因果推理的所谓混淆因子用于训练过程中，设计了多种特征融合机制充当因果推理的干预过程，代码可用，重新精读作为改进工作的启发。视觉语言导航的过程被理解为观察环境和文本指令被分别编码成视觉特征和语言特征，进而综合推理得到了导航判断。但是在这一过程中，观察环境和文本指令中都包含了某种混淆因子会影响最终的判断。比如观察环境的房间类型和object特征，文本指令的语言模式风格。这也是之前VLN工作提取过的特征，作者将这些当作混淆因子，并通过多种融合机制（attention+门控）作为Do Intervention。
<<<<<<< HEAD
> > 
=======
> 
>>>>>>> 82ae37b2ef76a87a7d1bde5cf740265e4b2a8922
> > [2025-08-31]-[于洋晨]-[Emotion across Modalities and Cultures: Multilingual Multimodal Emotion-Cause Analysis with Memory-inspired Framework]-[ACM2025](https://sentic.net/emotion-across-modalities-and-cultures.pdf)
> >
> ><img width="512" height="512" alt="image" src="https://github.com/yyc-hfut/PaperReading/blob/main/M3F.png" />
> > 
> > 论文简述：本文构建了中文的MEC4数据集，用于多模态情感-诱因任务的分析，并且提出了一种基于大语言模型的记忆启发式框架M3F，通过独特的多模态记忆库以及Q-Former机制，有效的捕捉了非语言模态的长时序变化以及全局信息，显著提升了在多语言多模态对话重情感-诱因分析的性能。同时，在跨文化的分析中，文中提到了文化差异对情感表达在模态依赖性上的影响，在中文这种高语境文化中，情感表达更为含蓄，往往需要结合三个模态才能获取更加完整的上下文，模态之间相互补充。而在英文这种低语境文化中，情感表达更加直接和明确，主要依赖于文本和音频模态，视觉信息可能与音频表达产生冲突，甚至引入噪声。这一观点与先前的MMML等论文中的实验结果有相似之处，就是在英文的对话情感识别任务中，视频模态信息往往会引入噪声。
> 
> >[2025-08-31]-[张宇]-[Action Unit Enhance Dynamic Facial Expression Recognition]-[arxiv](https://arxiv.org/abs/2507.07678)
> >
> > <img width="512" height="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/AUDFER.png" />
> >
> > 论文简述:把AU↔ 表情的先验知识量化成一个权重矩阵，并把它写进训练的AU辅助损失里，作为可插拔中间件加到现有视频表情识别模型上（文中选取的三个现有的DFER模型:M3DFEL、MAE-DFER、Former-DFER），在DFEW、FERV39k、MAFW 上不加算力也能稳定涨点，尤其改善小样本类别（如 Disgust）。具体做法：用OpenFace对DFER数据集生成 AU伪标＋从四个数据集统计AU-expression强相关性→做成加权BCE的AU loss→与原表达式分类损失按比例融合，属于不是重新发明时序模型，而是注入先验/损失函数层面的方法（甚至可以看做为一种track），但是貌似也引入了两点不足->(1)AU 与表情都来自工具（OpenFace、M3DFEL）的伪标，存在噪声与循环依赖风险（用模型推的表情再去定义 AU→表情权重）(2)野外数据集标注au有很大的噪声，何尝不可以用实验室au数据呢。
>
> >[2025-08-31]-[何艺超]-[MME-Emotion: A Holistic Evaluation Benchmark for Emotional Intelligence in Multimodal Large Language Models](http://arxiv.org/abs/2508.09210)
> >
> > <img width="512" height="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/MME-Emotion.png" />
> >
> > 论文简述:全新的针对大模型的情感评估基准，将情感任务细分成8个子任务，分别是实验室中的情绪识别 （ER-Lab）、野外情绪识别 （ER-Wild）、噪声下的情绪识别（Noise-ER）、细粒度情绪识别 （FG-ER）、多标签情绪识别 （ML-ER）、情绪分析（SA）、细粒度情感分析（FG-SA）和意图识别 （IR）。但是基准是跨任务的，也就是说情感大模型无论涉及到哪个子任务，评估套件均是通用的。具体来说，首先获取受试MLLM对给定闭集情感问题的推理过程和答案，然后使用单模态步骤代理从模型的回答中自动提取关键推理步骤。使用评判模型对推理步骤进行二元打分得到推理分数（Rea-S），分类的准确率得到识别分数（Rec-S），将二者加权得到思维链分数（CoT-S）。总的来说，这个评估基准完善了情感大模型评估仅偏向结果而弱化过程的评估范式。
>
>>[2025-08-30-陈银-[Leveraging Eye Movement for Instructing Robust Video-based Facial Expression Recognition]-TAFFC2025
>>
>> <img width="512" height="512" alt="image" src="https://github.com/user-attachments/assets/747ba6ee-f4f4-4495-86de-c6732cffebc5" />
>>
>> 论文简述:本文提出了一种新的眼动引导视频表情识别方法 EM-VFER， 旨在提升视频表情识别在复杂场景（例如伪装表情、文化差异）下的鲁棒性。该方法首先使用眼动仪获取高质量眼动信号与视频，构建新数据集 EMER，并基于此对多模态Transformer (MERT)进行预训练；然后在常规视频表情数据集上进行微调，但采用设计的 渐进式眼动引导学习 (PEML) 来利用眼动先验，同时抑制低质量视觉眼动噪声。该方法有效提升了宏表情与微表情识别的精度，并在多个公开数据集上得到最优结果。
> 
> >[2025-07-06]-[何艺超]-本周未精读
> >[2025-07-06]-[张雪松]-本周未精读
> >
> >[2025-07-06]-[张宇]-[Understanding and Mitigating Annotation Bias  in Facial Expression Recognition](https://arxiv.org/abs/2108.08504)
> >
> > 论文简述: 数据集的bias会导致模型被带偏，假设这个bias不是人们统一的认知，所以这篇工作以此为出发点，作者通过使用 OpenFace 识别图像中的au，系统的考察了多种wild与lab表情数据集中的性别标注偏差，发现在实验室数据集中，当动作单元强度相同时，男性与女性被标注为happy或angry的概率基本一致，而在ExpW、RAF‑DB 及 AffectNet 等wild数据集中，即便 AU 已被控制，女性依然更容易被标注为happy、男性更容易被标注为angry，说明标注偏差确实存在并且会被模型学习。为了解决这一偏差，作者利用 AU‑校准表情识别框架：在常规的交叉熵损失之外，构建以 AU 相似度为基础的三元组损失，将具有相同 AU 组合的样本拉近、不同的拉远，从而强制模型更多地依据客观的 AU 信息而非性别特征进行表情判断。
> 
> >[2025-07-06]-[贾朋]-[GGTalker: Talking Head Systhesis with Generalizable Gaussian Priors and Identity-Specific Adaptation](https://arxiv.org/abs/2506.21513)
> > <img width="512" alt="image" src="https://raw.githubusercontent.com/2351548518/images/main/20250424/20250706222130651.png">
> >
> > 论文简述: 现有Talking Head方法能在固定视角与小范围音频变化下取得令人满意的效果，但在大幅头部旋转和分布外(OOD)音频条件下表现欠佳，作者认为核心问题在于缺乏足够的3D先验知识，这限制了合成数字人的外推能力。作者提出GGTalker，通过融合先验与身份适应来合成数字人脸。引入两阶段的"先验-适应"训练策略：首先学习高斯头部先验，再适配个体特征。通过训练音频-表情先验与表情-视觉先验，分别捕获唇部运动的规律和头部纹理的通用分布；在定制适应阶段，则精细建模个体说话风格与纹理细节。
> 
> >[2025-07-06]-[徐赟博]-[VLN-R1: Vision-Language Navigation via Reinforcement Fine-Tuning]
> >
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/VLNR1.png">
> > 
> > 论文简述：本文提出了一种在VLN领域引入类R1推理模型的工作。核心贡献主要有三点1）数据集上，在连续模拟器上通过agent的GT 轨迹和动作收集了第一视角的动作视频流；2）模型结构上，在token选择上，根据导航agent的特性，设计了一种长短期自适应采样，以较高的概率选取短期内的记忆sample作为短期导航状态，并且选择一段时间之前的sample保持长期状态；3）训练上，除了使用了GRPO的训练策略之外，还设计了一种根据时间/step自适应的奖励衰减机制。个人感觉主要贡献在于第一视角导航视频数据的收集，其他设计都可以找到蓝本。
> > 
>>[2025-07-6]-[陈银]-[Question-Aware Gaussian Experts for Audio-Visual Question Answering]-CVPR2025
>>
>><img width="512" alt="image" src="https://github.com/user-attachments/assets/1b39738f-33ee-4b89-b7d1-6d2abf81d614" />
>>
>>论文简述：
>>本文提出了一个新的AVQA的框架：首先通过问题感知融合，在编码阶段将问题的预警注入到音频和视频特征之中，使得特征的处理更有针对性；其次，创新性的提出采用“混合高斯专家”模块，利用多个高斯分布作为“软掩码”，对音视频进行自适应的加权，动态聚焦最关键的时间片段。实验证明，该方法能更有效地处理复杂的时序关系，并在多个AVAQ数据集上取得了SOTA效果。
>
> > [2025-06-29]-[贾朋]-[SyncTalk++- High-Fidelity and Efficient Synchronized Talking Heads Synthesis Using Gaussian Splatting](https://arxiv.org/abs/2506.14742)
> > <img width="512" alt="image" src="https://raw.githubusercontent.com/2351548518/images/main/20250424/20250629225113227.png">
> >
> > 论文简述: 作者提出SyncTalk++：其动态肖像渲染器采用3DGS确保身份特征一致性；FaceSync控制器通过创新性运用3D面部 blendshape 模型，在唇语同步的同时精准重建面部表情；为达成自然头部运动，Head-Sync稳定器优化头部姿态提升稳定性。该方法保持跨帧视觉细节的一致性及连续性，并将渲染速度显著提升至101帧/秒。
> 
> > [2025-06-29]-[张宇]-[Merging Multi-Task Models via Weight-Ensembling Mixture of Experts]
> > 
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/MMTM.png">
> >
> > 论文简述：不同于端到端训练的MTL学习范式，属于MTL另一种比较火的学习范式（模型合并），目标是把多个针对不同任务独立微调过的模型合并成一个统一的多任务模型。传统的模型融合方法像“task arithmetic”那样，只是线性地在参数空间插值，容易在多个任务之间引发参数冲突。本篇工作的做法是：除了Transformer的Attention模块用现有方法简单合并外，把MLP模块替换成了MoE结构，用于动态地整合共享知识和任务特有知识，它保留了预训练模型的MLP参数，并用任务向量（fine-tune后的参数增量）表示每个任务的特征，再通过一个轻量级路由器（实验证明两层的比单层的好）根据输入动态地组合这些专家向量，能根据不同输入自动激活最合适的任务知识，可以减弱了任务间的干扰问题。
> > 
> > [2025-06-29]-[何艺超]-本周未精读
> > 
> > [2025-06-29]-[徐赟博]-[Weakly-supervised VLM-guided Partial Contrastive Learning for Visual Language Navigation]
> > 
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/WPCL.png">
> >
> > 论文简述：本文同样是通过VLM提取场景中的图像描述进行对比学习的工作，精读了一下借鉴思路。VLN中使用的视觉编码器CLIP/ResNet对于视角的变化非常敏感，作者提出对于同一个viewpoint不同视角的图像进行对比学习提升agent的视角变化的敏感性。作者使用VLM主要是用于提取Object来为不同视角的图像标记为正负样本。使用LLava对每一个视角抽取object，然后通过判定每个视角中存在的共享的object的数量被判定为正样本还是负样本，然后统一进行对比学习。思路比较简单，但略有相似。
> > 
>> [2025-06-29]-[陈银]-[AffectGPT: A New Dataset, Model, and Benchmark for Emotion Understanding  with Multimodal Large Language Models]-[ICML2025](https://arxiv.org/pdf/2501.16566)
> > <img width="512" alt="image" src="https://raw.githubusercontent.com/cyinen/imge/master/20250629133148.png">
> >
>>论文简述：本文章提出了一个新的数据集标注策略，并标注了一个新的多模态情感描述数据集：Mer-Caption+。该数据集是在MER2024的数据集上，通过low-level筛选和模型众包的方式，筛选出了31K比较精细的数据。并在此基础上，训练了一个AffectGPT，在多个情感分析的任务上都取得了SOTA性能。
>>
>> 
> >[2025-06-22]-[戴逦]-本周未精读
>
> >[2025-06-22]-[于洋晨]-本周未精读
> 
> >[2025-06-22]-[张雪松]-调试SPOC代码，略读相关文章，本周未精读
> >
> >[2025-06-22]-[何艺超]-推进比赛，本周未精读
> >
> >[2025-06-22]-[张宇]-[DLM-VMTL:A Double Layer Mapper for heterogeneous data video Multi-task prompt learning]-[arxiv](https://arxiv.org/abs/2408.16195)
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/DLM.png">
> >
> > 论文简述: 该工作针对视频理解领域中异构数据集上的多任务学习难题，通过DLM-VMTL 的双层 Prompt 映射器：第一层用 Transformer 的自注意力拿辅助任务中间层的特征“抠”出一堆跨任务的 Prompt，第二层再用轻量级的 Adapter 把这些 Prompt 投射到主任务的特征空间里，不同于传统的HPS和SPS，也区别于需要为每个数据集训练独立路由器并以Top-K策略激活专家的方案，这种灵活的共享方式既高效又稳定，不会互相抢资源，也不怕在新任务上忘掉旧任务的能力
> 
> > [2025-06-22]-[徐赟博]-本周未精读
> >
> >   [2025-06-22]-[贾朋]-[GaussianAvatars- Photorealistic Head Avatars with Rigged 3D Gaussians](https://arxiv.org/abs/2312.02069)
> > <img width="512" alt="image" src="https://raw.githubusercontent.com/2351548518/images/main/20250424/20250621214458246.png">
> >
> > 论文简述: 3DGS Head,基于3DGS的动态三维表征技术，通过将其绑定到FLAME模型上实现。这种组合既能保证超写实渲染效果，又能依托FLAME模型实现精确动画控制，例如通过驱动序列进行表情迁移，或手动调节FLAME参数。作者采用三角面片局部坐标系参数化每个3DGS点，并通过显式位移偏移优化以获得更精确的几何表征。在新视角合成指标上大幅领先其他方法。
> 
> >[2025-06-22]-[陈银]-[《Dynamic Curriculum for Imbalanced Multimodal Learning》](https://arxiv.org/pdf/2503.06456)
> ><img width="512" alt="image" src="https://raw.githubusercontent.com/cyinen/imge/master/20250622125111.png">
> >
>>论文简述：本文提出了一种名为DynCIM的动态课程学习框架，用于解决多模态学习中样本和模态不平衡的问题。DynCIM通过样本级和模态级的动态课程学习评估样本难度和模态贡献，并引入模态门控机制动态调整模态权重，从而优化多模态融合效果。在多个基准数据集上的实验结果表明，DynCIM能够有效提升多模态学习的性能，优于现有的最先进方法。
> 
> > [2025-06-15]-[贾朋]-本周未精读
>
> > [2025-06-15]-[戴逦]-本周未精读
>
> > [2025-06-15]-[于洋晨]-本周未精读
> 
> >  [2025-06-15]-[徐赟博]-[Cross from Left to Right Brain: Adaptive Text Dreamer for Vision-and-Language Navigation](https://arxiv.org/pdf/2505.20897)-
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/ATD.png">
> >
> > 论文简述：本文提出了一种在VLN中直接微调大模型的方法。从原始的R2R数据集上收集了两种新语料，一种是对场景的描述，另一种是对导航动作做出判断的依据。然后在训练大模型的时候，分别以两个损失函数去让多模态大模型学习到关于导航场景的知识。定义了两种大模型的左右脑训练方式，左脑生成对场景的描述，右脑生成预测导航动作的思维链。在生成的过程中，大模型接受视觉和文本token作为输入，然后在左右脑两路分支并行训练。但插入了可学习的query在两个支路中间传递信息，因为最重要的部分是预测导航动作，所以设计的query是由右脑支路去query左脑场景描述支路的状态嵌入。贡献在于造了新概念，然后工作量很大程度体现在对原始R2R数据集造场景描述和导航思维链的新语料库
> >
> >  [2025-06-15]-[张雪松]-[P3Nav: A Unified Framework for Embodied Navigation Integrating Perception,Planning, and Prediction](https://arxiv.org/pdf/2503.18525)-
> > <img width="512" alt="image" src="https://github.com/user-attachments/assets/035295f5-4703-484e-bcda-c86a278f6187">
> >
> > 论文简述：本文聚焦（目标）导航和具身问答EQA两种任务协同，提出了一种P3Nav的导航框架，统一了agent的感知（perception），规划（planning）和预测（prediction）能力。其中，P3Nav采用一种自适应的历史采样器高效地处理3D的历史发现。最后使用LLM输出action head和答案（LLM是OpenFlamingo，一种开源的自回归大视觉语言模型，仅训练cross-attention部分）。在CHORES-S的目标导航任务上达到了sota水平。实验部分表明，本文的方法仅仅通过EQA任务增强P3Nav agent对环境的感知，并未在EQA数据集上进行实验。
> > 
> > [2025-06-15]-[何艺超]-推进比赛，本周未精读
> > 
> > [2025-06-15]-[张宇]-本周未精读
> > 
> >  [2025-06-15]-[陈银]-[Diagnosing and Re-learning for Balanced  Multimodal Learning](https://arxiv.org/pdf/2407.09705)-ECCV2024
> > <img width="512" alt="image" src="https://github.com/user-attachments/assets/88fe5c06-564d-4d85-9984-5ed5ba972e16" />
> >
> > 论文简述: 在多模态学习中的不平衡问题中现有方法往往忽略了模态本身的内在信息量和噪声水平，导致在处理信息量少且噪声多的模态时效果不佳。本文通过评估每个模态的单模态表示空间的可分离性来诊断其学习状态，并据此对编码器进行软重新初始化：对学习良好的模态进行较大幅度的重新初始化以减少模型对其依赖，同时避免其过拟合；对学习不足的模态进行轻微重新初始化以增强其学习能力并避免记住噪声。实验表明，该方法在多个数据集和多模态框架上均优于现有方法，能够有效提升多模态模型的整体性能和单模态表示质量，同时在信息量少的模态情况下表现出色
> 
> > [2025-06-08]-[戴逦]-本周未精读
>
> > [2025-06-08]-[何艺超]-推进比赛，本周未精读
>
> > [2025-06-08]-[张宇]-[An Efficient General-Purpose Modular Vision Model via Multi-Task Heterogeneous Training](https://arxiv.org/abs/2306.17165)
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/MTHL.png">
> >
> > 论文简述: 该工作在ImageNet、COCO和ADE20K等异构数据集上同时训练一个模型，实现分类、检测和分割三大任务。本质上的做法感觉没有太多新点，还是用多个专家模块替换Transformer中的MLP层，但是不同于DAMEX那篇工作，它是为每个数据集配备独立路由器，通过Top-K策略在前向时只激活最相关的专家，不同于hard level的专家数据集互相约束，而是引入基于动量缓冲区的互信息损失来鼓励专家专注于特定数据集。主吹的是在连续学习场景下，仅需为新任务添加少量新专家与路由器，无需更新旧专家即可避免灾难性遗忘。
>
> >[2025-06-08]-[于洋晨]-本周未精读
> 
> > [2025-06-08]-[贾朋]-[DEGSTalk: Decomposed Per-Embedding Gaussian Fields for Hair-Preserving Talking Face Synthesis](https://arxiv.org/pdf/2412.20148)
> > <img width="512" alt="image" src="https://raw.githubusercontent.com/2351548518/images/main/20250424/20250603155557058.png">
> >
> > 论文简述: 3DGS Talking Head,作者提出3DGS的分解预嵌入高斯场（DEGSTalk）说话人脸合成方法，用于生成更具真实感的长发说话人脸。采用预测的3DMM系数作为中间面部表征调整高斯基元，从而精确捕捉动态面部区域与微妙表情。通过对可变形高斯场进行微调以融合面部和头发区域，随后执行渲染，解决了在对话人脸合成中准确重建长发个体的挑战。
> 
> > [2025-06-08]-[徐赟博]-[EvolveNav： Self-Improving Embodied Reasoning for LLM-Based Vision-Language Navigation][Arxiv]
> > 
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/EvolveNav.png">
> > 
> > 论文简述：本文提出了一种使用思维链微调来促进大模型在导航表现的方法。方法分为两阶段训练，分别是第一阶段思维链监督微调和二阶段自反思后训练（post-training）。首先，在第一阶段中，构建COT模板，“我需要到达一个地方具有<物品>在<方向>”。其中物品和方向分别通过caption模型和映射得到。然后，LLM输出的包含思维链的output需要和构建的CoT模板对齐。第二阶段中，为了防止模型过拟合于固定的思维链格式，这里让模型去选择CoT的正误，除了一个GT的COT外，还会给出一个类似模板，但关键细节错误的COT。这个工作是比较新颖的在导航领域使用思维链微调的工作，后续可以借鉴 
> >
> > [2025-06-08]-[张雪松]-本周未精读，阅读和复现[SPOC](https://spoc-robot.github.io/)论文代码
> >
>> [2025-06-08]-[陈银]-[Breaking Performance Shackles of Full Fine-Tuning on Visual Recognition Tasks][CVPR 2025]
>>
>> 论文简述：在大多数的视觉任务中，PEFT的方法（Adapter, Lora等）的性能都难以超过full fine-tuning。本文设计了一种多感知视觉adapter (Mona), 利用conv替代liner, 并使用多个conv实现多层感知，首次在视觉分割，实例分割等复杂的视觉感知任务上超过了全量参数微调的方法。
>> 
>> <img width="512" alt="image" src="https://github.com/user-attachments/assets/71026772-790e-4176-b166-9fed01c21333" />
> 
> > [2025-05-30]-[陈银]-本周未精读
> 
> > [2025-05-25]-[于洋晨]-本周未精读
> 
> > [2025-05-25]-[戴逦]-本周未精读
>
> >   [2025-05-25]-[贾朋]-[DEEPTalk: Dynamic Emotion Embedding for Probabilistic Speech-Driven 3D Face Animation](https://arxiv.org/pdf/2408.06010)
> > <img width="512" alt="image" src="https://raw.githubusercontent.com/2351548518/images/main/20250424/20250525204158226.png">
> >
> > 论文简述: 3D Talking Head,作者提出一种直接从语音输入生成多样化、富含情感3D面部表情的方法。首先训练动态情感嵌入模型DEE，该模型采用概率对比学习构建语音与面部动作的联合情感嵌入空间，该概率框架能捕捉情感解析过程中的不确定性。为生成动态面部运动，设计时序分层VQ-VAE作为运动先验，克服了传统VAE和VQ-VAE的缺陷。基于这些强先验，开发出非自回归预测码本索引的Talking Head生成器DEEPTalk。
> 
> > [2025-05-25]-[何艺超]-本周未精读
> > 
> > [2025-05-25]-[张宇]-[DAMEX: Dataset-aware Mixture-of-Experts for visual understanding of mixture-of-datasets]-[NeurIPS23](https://neurips.cc/media/neurips-2023/Slides/71495.pdf)
>>
>> <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/DAMEX.png">
>> 
>>论文简述：本文在异构数据集上利用隐含的通用知识进行合理的共享，异构数据集在域，标注方式等有很大不同，因此梯度冲突比同构数据集更大，为了合理利用共享信息，本篇工作设计了数据集的专家替代原始dense fnn模块，专家的架构是原始的FNN直接复制过来的，针对某个数据样本的token可以根据router灵活的选择某个专家，为了让专家更具数据特色，通过专家和数据集的映射关系直接利用损失反映到梯度更新上，但是不同的数据集还是共用一个router，或许可以考虑不同数据集维护自己那份独立的router。
>>
> > [2025-05-25]-[张雪松]-[SPOC: Imitating Shortest Paths in Simulation Enables Effective Navigation and Manipulation in the Real World](https://spoc-robot.github.io/)
>>
>> <img width="512" alt="image" src="https://github.com/user-attachments/assets/22e9a693-dd9c-4963-8e50-58722c8b0818">
>> 
>>论文简述：本文提出了一种名为CHORES的benchmark，它涉及在AI2-THOR仿真环境下，的同时执行导航和操作的十个任务。本文还提出了一种基于transformer的SPOC方法，模仿最短路径的模仿学习方式训练agent，使用了DINOv2和SigLIP等技术，不包括LLM。输入仅仅包含RGB图像，不包括深度和GPS定位。目前本文的局限性在于训练代价大（384 GPU(A100) hours），基于合成的图像而没有应用到实际的场景
>>
>> [2025-05-25]-[陈银]-[EMOE: Modality-Specific Enhanced Dynamic Emotion Experts]
>>
>> <img width="512" alt="image" src="https://raw.githubusercontent.com/cyinen/imge/master/20250517112044.png">
> >论文简述：本文提出了一种新模型，用于解决多模态情感识别（MER）中的模态平衡困境和模态特异性消失问题。EMOE通过Mixture of Modality Experts动态调整样本的模态权重，并引入路由熵损失优化模态平衡；通过Unimodal Distillation保留单一模态预测能力，指导多模态特征学习。在CMU-MOSI、CMU-MOSEI和MIntRec数据集上的实验表明，EMOE在准确率和F1分数上优于或媲美现有方法，并成功扩展到多模态意图识别等任务
>
> > [2025-05-23]-[徐赟博]-[Dynam3D: Dynamic Layered 3D Tokens Empower VLM for Vision-and-Language Navigation]
> >
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/Dynam3D.png">
> > 
> > 论文简述：本文讨论了一种3D视觉表征用于视觉语言导航。作者提出了一种‘Patch-Instance-Zone’的分层3D表征。Patch是CLIP提取的2D patch特征通过多视角图像投射成3D Patch特征，Instance是2D的语义分割模型的结果，通过一个需要学习的预测器，预测2D和3D对应区域内Instance分割结果是否匹配。Zone指的是3D坐标里均匀分布的一个空间块，使用一个Zone Encoder聚合一个Zone区域内所有的Instance特征作为Zone Token。然后，作者把这种代表不同粒度的3D token（patch/instance/zone token）共同输入到一个3D VLM进行训练。在单目setting的数据集上进行了测试，多个数据集都有明显的性能提升。这篇工作作者应该是聚合了一些3D检测/分割的工作，比较陌生；但在单目任务的setting下使用多视角图像建模三维环境表征有点不理解这是在干啥。
> > 
> > [2025-05-18]-[于洋晨]-本周未精读
> 
> > [2025-05-18]-[戴逦]-本周未精读
>
> >   [2025-05-18]-[贾朋]-[EmoFace- Emotion-Content Disentangled Speech-Driven 3D Talking Face Animation](https://arxiv.org/pdf/2408.11518v2)
> > <img width="512" alt="image" src="https://raw.githubusercontent.com/2351548518/images/main/20250424/20250518191051318.png">
> >
> > 论文简述: 3D Talking Head,作者提出双流神经网络EmoFace，包含情感分支与内容分支，创新性采用网格注意力机制，通过新型时空图卷积网络SpiralConv3D分析并融合情感与内容特征，学习网格顶点间的时空特征依赖关系。首次在3D面部动画任务中引入具有中间监督的自增长训练策略，动态调整真实数据采用比例。在3D-RAVDESS数据集上，本方法的LVE与EVE分别比SelfTalk低20%与35%。
> 
> > [2025-05-18]-[何艺超]-[Emotion-Qwen: Training Hybrid Experts for Unified Emotion and General Vision-Language Understanding](https://github.com/ReadingPapers/Report/blob/main/Images/Emotion-Qwen.png)
> >
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/Emotion-Qwen.png">
> >
> >论文简述：Emotion-llama团队的第二个情感大模型，从让模型具备情感推理能力的同时，通用视觉任务的能力也要具备，仅支持视频和文本输入，使用通用数据集进行预训练，情感数据集指令微调的方法，模型架构上：给peojector换成了MoE以处理一般视频任务和情感任务，并通过DeepFace开源工具获取了情感高相关的视频帧，让这个情感高度相关的帧与视频特征融合。LLM用的是qwen2.5的7B模型，使用3张A800-80G进行训练。
> >
> > [2025-05-18]-[张雪松]-未找到合适论文，本周未精读
> > 
> > [2025-05-18]-[徐赟博]-本周未精读
> > 
>>[2025-05-18]-[张宇]-[Mod-Squad: Designing Mixtures of Experts As Modular Multi-Task Learners]-[cvpr23](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Mod-Squad_Designing_Mixtures_of_Experts_As_Modular_Multi-Task_Learners_CVPR_2023_paper.pdf)
> 
>> <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/Mod.png">
>>
>> 论文简述：属于从模型方面缓解mtl中的参数竞争和梯度冲突，更合理的共享。做法是将ViT结构中的attn和FFN模块划分为多个专家，也就是子空间的专家，划分隐藏维度，并规定哪些专家是共享专家，哪些是任务特定的专家，并利用专家和任务的互信息，使特定于任务的专家更加的专一，保证平衡激活频率的同时，又不会导致dense expert的出现，多任务模型训练完成后，通过“剪支”丢弃与当前任务不相关其他所有专家，使其成单任务模型，保证推理速度的同时不会降低其性能。
>
>>[2025-05-17]-[陈银]-[R1-Omni Explainable Omni-Multimodal EmotionRecognition with Reinforcement Learning]
> 
>> <img width="512" alt="image" src="https://github.com/user-attachments/assets/a521efb2-04a8-4f14-a8ce-4e8241322df2">
>>
>> 论文简述：阿里通义千问实验室的一篇论文，首次将强化学习 RLVR 方法结合 GRPO 用于多模态的情绪识别，强化了模型的推理能力，展现出了与有监督训练的模型相当的性能，特别是在在分布外数据集评估中展现出卓越的鲁棒性。之前是CoT 火了一段时间，现在RLVR+GRPO在视觉多模态推理上也开始火了起来，应该会是一个主流的方向，值得关注。
>
> > [2025-05-11]-[张雪松]-[SG-Nav: Online 3D Scene Graph Prompting for LLM-based Zero-shot Object Navigation][NeurIPS2024]
> > 
> > <img width="512" alt="image" src="https://github.com/user-attachments/assets/73adb57b-a252-4f27-80ba-8fdc70057206">
> >
> > 论文简述: 本文是UniGoal的前身(CVPR2025)，开源性较好。该论文提出了一种新颖的3D场景图提示框架，用于在无需下游训练的情况下提升大语言模型（LLM）在零样本目标导航任务中的表现。作者通过构建紧凑、语义丰富的3D场景图并将其转化为语言提示，引导LLM完成导航决策。同时，论文设计了结构化搜索策略以生成高质量提示，并利用多模态感知信息对目标进行显著性建模。实验结果在Gibson和MP3D两个基准上均显著优于现有方法。

> > [2025-05-11]-程浩-[Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation]-[Deepseek Technical Report](https://arxiv.org/abs/2410.13848) \
> >![image](https://github.com/user-attachments/assets/bec6208c-96fc-418b-b219-72304c5041d4) \
> > 论文简述：在现有的统一MLLM中，如Chameleon，采用llava架构，并对视觉理解和生成任务采用了相同的encoder，但作者认为这两种任务实际上不一样：理解需要抽象的高级语义表征，而生成则需要具体详细的信息。所以在统一模型的基础上，作者解耦了多模态理解和生成框架的vision encoder，Gen.encoder采用VQ形式，Und.encoder采用SigLIP（可理解为advance CLIP）。

> > [2025-05-11]-[张宇]-本周未精读
> > 
> > [2025-05-11]-[于洋晨]-[EMOE:Modality-Specific Enhanced Dynamic Emotion Experts][暂无出处]
> > 
> > <img width="512" alt="image" src="https://github.com/yyc-hfut/PaperReading/blob/main/EMOE.png">
> >
> > 论文简述: 本文提出EMOE结构，用于多模态情感识别以及多模态意图识别任务。如图所示，该结构首先使用模态混合专家系统，根据不同样本的特征动态调整模态的重要性。同时，本文认为传统模型忽略了单模态数据的预测能力，所以又提出使用贡献更大的单模态特征，蒸馏多模态特征，使得在融合特征中保留单模态的预测能力。最终，通过消融实验，验证了两个模块的有效性。但是，最终在CMU-MOSI以及CMU-MOSEI数据集上的效果不是特别理想，起码比之前看似简单的MMML模型的ACC7指标低了0.5个点（相较于MMML_context，低了2.5个点），其他指标也是差不多。不过，该工作提出的，保留单模态的预测能力以及样本级模态差异的问题，确实值得思考。
> 
> >   [2025-05-11]-[贾朋]-[Generalizable and Animatable Gaussian Head Avatar](https://github.com/xg-chu/GAGAvatar)
> > <img width="512" alt="image" src="https://raw.githubusercontent.com/2351548518/images/main/20250424/20250430211122196.png">
> >
> > 论文简述: 作者提出一种可泛化可驱动的Gaussian头部虚拟形象（GAGAvatar），用于单张照片的动画化头部重建。核心创新在于提出的双提升方法，该方法能生成高保真度的3D Gaussian，完整捕捉身份特征与面部细节，结合全局图像特征与3D形变模型来构建控制表情的3D Gaussian结构。训练完成后，模型无需针对性优化即可重建未见过的身份信息，并以实时速率完成驱动渲染。
> 
> > [2025-05-11]-[戴逦]-[Masked Autoencoders Are Scalable Vision Learners]-[CVPR 2022]
> > 
> > 论文简述：本文提出了一种自监督学习方法——MAE。该方法可以重建被遮挡的图片，并达到不错的效果。方法有两个核心设计，首先，作者设计了一种非对称的编码器-解码器架构，编码器仅处理可见的patches，这种设计使其训练速度大幅加快，解码器利用编码器学习到的潜在表示来重建不可见的patches。其次，作者通过实验发现，随机遮挡图片的比例设置为75%时，会得到比较好的重建效果。
>
> > [2025-05-11]-[何艺超]-本周未精读
> > 
> > [2025-05-11]-[陈银]-[R1-Omni: Explainable Omni-Multimodal Emotion
Recognition with Reinforcement Learning] 
> > 
> > <img width="512" alt="image" src="https://github.com/user-attachments/assets/b64225a2-b3f7-43ed-a6b3-027714b9761f" />
> > 
> > 论文简述: 本文首次将可验证奖励的强化学习（Reinforcement Learning with Verifiable Reward, RLVR）应用于全多模态大型语言模型，以提升情感识别任务的性能。该模型在推理能力、情感识别准确性和泛化能力方面都有显著提升，尤其是在处理分布外（OOD）数据集时表现出色。显示了强化学习在数据泛化能力方面的巨大潜力
> >
> > [2025-05-10]-[徐赟博]-[Landmark-RxR: Solving Vision-and-Language
 Navigation with Fine-Grained Alignment Supervision]-[Nips2021](https://proceedings.neurips.cc/paper/2021/file/0602940f23884f782058efac46f64b0f-Paper.pdf)
> > 
> ><img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/landmark.png">
> > 
> > 论文简述:本文在RXR数据集的基础上补充了细粒度的landmark，借鉴了此前fine-grined R2R的工作，将导航数据集拆解成子指令。这篇文章我更关注的是他如何使用细粒度的landmark去进行强化学习中的奖励的建模。首先，作者定义了一个轨迹关键点的含义，在导航轨迹过程中起关键作用的位置节点，这些节点可以反应导航过程的关键位置。而landmark天然体现了这一特性，作者利用导航过程中的landmark塑造reward。Soft Reward和Hard Reward两种。硬奖励是节点离关键点距离小于阈值就+1，反之-1；软奖励依然是根据和关键点的距离塑造的，计算公式是-exp{距离}。具有可借鉴的价值。
> >
> > [2025-05-07]-[张雪松]-五一劳动节放假，未精读
> 
> > [2025-04-27]-[于洋晨]-整理代码实验，大致搜索一些文章，无精读
>
> > [2025-04-27]-程浩-[Qwen2.5-Omni Technical Report]-[arxiv](https://arxiv.org/abs/2503.20215)
> > 
> >  <img width="512" alt="image" src="https://github.com/user-attachments/assets/e044400c-ac99-4a17-9624-f07686ae695f">
> >
> > 论文简述：利用音频encoder和视频encoder将多种模态编码到大模型的表征空间，并且利用Talker进行语言输出，构建了一个统一的多模态模型。其中TMRoPE与我之前思路一致。
> 
> > [2025-04-27]-[聂建涛]-本周未精读
> 
> > [2025-04-27]-[何艺超]-[HumanOmni: A Large Vision-Speech Language Model for Human-Centric Video Understanding](http://arxiv.org/abs/2501.15111)
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/HumanOmni.png">
> >
> > 论文简述:和R1 Omni以及Omni emotion来自于一个团队，这篇文章涉及的角度更广，把情感识别放在了以人为中心的视频理解角度考虑，构建了超大规模的数据集，使用qwen 2.5作为backbone ，和前面几个典型大模型工作的不同是设计了不同任务的projector然后把bert提取的文本用作指导三个不同视觉任务projector的合并权重生成。
> 
> > [2025-04-27]-[贾朋]-[MonoGaussianAvatar: Monocular Gaussian Point-based Head Avatar](https://arxiv.org/abs/2312.04558)
> > <img width="512" alt="image" src="https://raw.githubusercontent.com/2351548518/images/main/20250424/20250426211819060.png">
> >
> > 论文简述: 当前头部虚拟形象技术已运用3DMM、点云及神经隐式表示等最新研究成果，然而3DMM方法受限于固定拓扑结构，点云方法因海量点数量导致沉重训练负担，而后者则在形变灵活性与渲染效率方面存在局限。针对这些问题，作者提出MonoGaussianAvatar，一种融合3D高斯点表示与高斯形变场的新方法，可从单目肖像视频学习显式头部虚拟形象。该模型结合面部表情与姿态，能够从单目视频中重建精细的几何结构与外观，引入了高斯点插入与删除策略，并采用高斯变形场以保留配饰（如眼镜）在新姿态下的结构完整性。
> 
> >[2025-04-27]-[张雪松]-[InstructNav: Zero-shot System for Generic Instruction Navigation in Unexplored Environment]-[CoRL-2024](https://github.com/LYX0501/InstructNav?tab=readme-ov-file)
> >
> ><img width="512" alt="image" src="https://github.com/LYX0501/InstructNav/blob/main/InstructNav.png" />
> > 
> > 论文简述：本文的主要贡献在于: (1) 构建基于LLM的动态导航链，统一不同形式的导航指令（目标导航，视觉语言导航，命令驱动的导航）进行导航。（2）之前的方法构建单个（基于文本指令和视觉环境的语义相似度）value map进行导航，本文进一步整合了action，Intuition和轨迹等value map进行导航。（3）实验结果在HM3D object goal navigation，R2R-CE 和 DDN demand-driven navigation三个任务上取得了较好的结果。
> >
> > [2025-04-27]-[戴逦]-[Deep Residual Learning for Image Recognition]-[CVPR 2016]
> >
> >论文简述：对于随着网络深度加深会出现梯度消失的问题，作者提出使用残差连接的方式来解决这个问题。具体来说，假设任务所需要学习的函数为H(x)，现有的浅层网络的输出为x，那么在浅层网络上再加入的一些层应该去学习H(x)-x，新加入的网络层不需要重新学习，而是去学习已经学习到的东西与真实的东西之间的残差，最后整个网络的输出等价于浅层神经网络和新加入的神经网络学习到的残差的输出之和。
>
> > [2025-04-27]-[徐赟博]-本周没有找到比较感兴趣的论文，泛读了一些比较General的论文，没有精读
> > 
> >[2025-04-27]-[陈银]-[AVadCLIP: Audio-Visual Collaboration for Robust Video Anomaly Detection]-AAAI2025
> >
> ><img width="512" alt="image" src="https://raw.githubusercontent.com/cyinen/imge/master/20250427101243.png" />
> > 
> > 论文简述：本文提出了一种新颖的弱监督视频异常检测（WSVAD）框架 AVadCLIP，通过视听协作增强了监控视频中异常检测的鲁棒性和准确性。框架基于对比语言-图像预训练（CLIP）的跨模态能力，融合视觉和音频信息，解决了传统仅依赖视觉方法在复杂环境（如遮挡、低光照、视觉噪声）中误报率高的问题。AVadCLIP在多个基准数据集上表现出色，即使在推理阶段缺乏音频数据时仍能保持鲁棒性
>>
> > [2025-04-24]-[张宇]-[Mitigating Task Interference in Multi-Task Learning via Explicit Task Routing  with Non-Learnable Primitives]-[CVPR23](https://openaccess.thecvf.com/content/CVPR2023/papers/Ding_Mitigating_Task_Interference_in_Multi-Task_Learning_via_Explicit_Task_Routing_CVPR_2023_paper.pdf)
> >
> ><img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/ETR-NLP.png" />
> > 
> > 论文简述：针对MTL中不同任务之间梯度冲突导致性能下降的问题，也就是所谓的任务干扰，在目前研究如何缓解梯度冲突导致任务干扰的三个主流背景下（loss，参数分区，模型），以模型为入手点，提出了（1）显式任务路由（ETR）（2）非可学习原语（NLP）相结合的模块。其中NLP也就是一些不可学习的模块进行组合（例如Relu，pool，Conv (binary)）（没有可学习参数自然缓解了一些梯度冲突），而ETR就是通过在每层网络中并行提取多种任务无关的非可学习特征，再利用可调比例 γ 将可学习参数显式分为共享与任务专属两部分（γ=0单任务，γ=1硬共享），最终cat在一起，既能保留充分的信息共享，又能为各任务提供专属容量，最终在 CelebA、Cityscapes 和 NYU-v2 等进行实验，发现单个NLP只对单任务有帮助，MTL还是需要多个NLP组合，对于ETR，γ=0.9（都快属于hard sharing了）最好，而且这两个模块互补，并与现有硬共享、损失平衡、软共享及隐式划分等方法进行比较，性能有较大的提升。
> >
> > [2025-04-20]-程浩-[本周未精度]
>
> >  [2025-04-20]-[戴逦]-[Attention is All you Need]-[NIPS 2017]
> >
> >  论文简述：作者提出Transformer，一种完全基于注意力机制，彻底摒弃了循环和卷积结构的神经网络结构。它仅仅使用注意力机制来捕捉输入和输出间的全局依赖，显著提高了并行化计算的能力。Transformer采用传统的编码器-解码器结构，都由6个子层组成。编码器的子层由多头自注意力层和前馈网络层构成，解码器的子层由带有掩码机制的多头注意力层（即只学习t时刻的输入与t时刻之前时序的输入的关系，不学习与t之后时刻的输入的关系）和多头注意力层和前馈网络层构成。作者在最后还预测该结构可以应用在其他模态如图像、音频和视频模态上并取得效果。
>
>  >[2025-04-20]-[聂建涛]-本周未精读
> 
> >   [2025-04-20]-[贾朋]-[GaussianTalker: Real-Time High-Fidelity Talking Head Synthesis with Audio-Driven 3D Gaussian Splatting]- [ACM MM2024](https://arxiv.org/abs/2404.16012)
> >
> > <img width="512" alt="image" src="https://raw.githubusercontent.com/2351548518/images/main/20250120/20250420212614224.png">
> >
> > 论文简述: 作者提出GaussianTalker，通过构建头部的规范3DGS表示，并使其变形与音频同步。创新在于将3D高斯属性编码为共享隐式特征表示，在此与音频特征融合以操纵每个高斯属性，利用了空间感知特征并强化相邻点之间的交互作用。特征嵌入随后输入空间-音频注意力模块，该模块可逐帧预测每个高斯属性的偏移量。相较于以往用于操纵大量高斯及其复杂参数的方法，该方案更为稳定。
> 
> > [2025-04-20]-[张宇]-[Transforming Vision Transformer: Towards Efficient Multi-Task Asynchronous Learning]-[NeurIPS24](https://proceedings.neurips.cc/paper_files/paper/2024/hash/93fab021315170101c92e8330a56fbdb-Abstract-Conference.html)
> >
> ><img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/MoLOAR.png" />
> > 
> > 论文简述：本篇工作属于MoE+LoAR+MTO的工作。MTL中一直存在两个主要的研究目标：（1）以高效的MTL（2）多任务优化策略（MTO），但他们基本上要么是解决了复杂不紧凑的网络但忽视了任务之间同步优化会导致不同任务难度不同收敛不一致破坏任务的固有特性，要么是意识到了异步优化但无法做到参数集成或网络可调参数居多导致过拟合，从而推理时间和性能都会下降。考虑了不同任务难度不同不能强制同步优化会导致竞争干扰，以及共享的backbone无法为每个任务提取代表的特征。具体怎么做的：（1）模型层面：重新排列预训练后模型的FNN的权重矩阵并使用K-means聚类方法将其分解为权重类似的多个矩阵，定义为低级专家，这些低级专家是低秩的信息含量不满，所以可以应用LoAR的微调策略进行训练（2）优化策略方面：使用质量保留优化策略将已经学习好的任务的权重进行保留并不会对其破坏，做到任务间异步优化，并采用Router Fading策略避免早期模型对某些专家过度依赖，最后参数集成的方法在返回到预训练的ViT,减少额外参数量并加快推理速度。实验从性能，参数量，以及推理速度都很好，但没有对比单任务模型。目前代码还没开源。
> >
> >[2025-04-20]-[于洋晨]-[本周无精读]
>
> >[2025-04-20]-[何艺超]-[To Err Like Human: Affective Bias-Inspired Measures for Visual Emotion Recognition Evaluation]-Nips2024
> >
> ><img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/to_err_like_human.png" />
> > 
> > 论文简述：传统的分类任务通常是通过准确率来衡量模型的好坏，不同类别在分类的评价标准中占据相同的重要性；然而针对情感任务，作者辩证地提出也要衡量分类错误的类别，因为在情感分类的任务中，由于情绪之间的心理相似性，将某种情绪分为一个类别可能比另一个结果更为严重，例如作者提出，将“兴奋”错误地分类为“愤怒”显然比“敬畏”更为严重。于是作者利用情感轮来定义情感距离，宏观上把情感氛围积极和消极，在具体的类别中，在损失函数上定义全局的衡量标准ECC，以及分类错误的类别EMC，并通过半监督伪标签的子任务，证明了其方法更加适用于伪标签的生成，也即说明了其损失函数的有效性。
> >
>[2025-04-19]-[陈银]-[Audio-Visual Adaptive Fusion Network for Question Answering Based on Contrastive Learning]-AAAI2025
> >
> ><img width="512" alt="image" src="https://raw.githubusercontent.com/cyinen/imge/master/20250419150539.png" />
> > 
> > 论文简述：本文提出了一种基于对比学习的音频-视觉自适应融合网络（AVAF-Net），用于解决音频-视觉问答（AVQA）任务中的时间-空间维度对齐和跨模态信息融合问题。通过时间对齐对比学习（TACL）和空间对齐对比学习（SACL），AVAF-Net有效对齐了视觉和音频信息，并利用问题导向的自适应融合（QOAF）模块动态调整融合权重，提高答案预测的准确性。在MUSIC-AVQA数据集上的实验表明，AVAF-Net在平均准确率上超越了所有基线模型，取得了显著的性能提升。
>> 
>> >[2025-04-17]-[张雪松]-[VL-Nav: Real-time Vision-Language Navigation with Spatial Reasoning]-[paper](https://sairlab.org/vlnav/)
> >
> ><img width="512" alt="image" src="https://github.com/user-attachments/assets/6b608222-006f-423e-a0b1-cb257d596ee6" />
> > 
> > 论文简述：本文主要对VLFM进行了改进，具体包括（1）使用yolo-world代替blip-2计算图象与指令（object）之间的语义相似性（value），好处是更加轻量化且能够细粒度地计算像素级别的语义值。（2）提出了Instance-Based Target Points (IBTP)，这种基于实例的方法模仿了人类在搜索对象时的行为：一瞥到可能与目标匹配的东西，人们就会自然地靠近以确认。（3）提出了两种好奇心的计算策略（基于距离和探索过的区域数量）。实验结果仅展现了部分object类别上的结果，显著超过了vlfm，但是没有在整个数据集（验证集）进行公平比较。
>> 
> >[2025-04-16]-[徐赟博]-[ST-Booster: An Iterative SpatioTemporal Perception Booster for
 Vision-and-Language Navigation in Continuous Environments]-[paper](https://arxiv.org/pdf/2504.09843)
> >
> ><img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/VLN-ST-Booster.png" />
> > 
> > 论文简述：本文提出了一种引入侧重于Local 感知的Grid Memeory Map辅助连续条件下的导航。如今的CE导航任务普遍采用的框架是使用全局的拓扑图进行导航。然而这种方式虽然对导航过程中的长程关系具有很好的建模能力，但是缺乏细粒度的感知能力。GridMemoryMap能够通过动态的构造Grid存储特定位置里的视觉特征，进而提供一种和位置以及历史相关细粒度的视觉感知。作者把GridMM作为导航的另一支路，并设计了Node2Cell和Cell2Node两个模块实现拓扑图中的节点和GridMM中的cell映射到同一个嵌入空间进行对齐。并且，又把导航过程中的拓扑图特征和GridMM特征用于预测Waypoint中。
> >
> >[2025-04-14]-[聂建涛]-补上周：由于赶论文，无精读
> > [2025-04-14]-[戴逦]-[Tokens-to-Token ViT: Training Vision Transformers From Scratch on ImageNet]-[https://openaccess.thecvf.com/content/ICCV2021/html/Yuan_Tokens-to-Token_ViT_Training_Vision_Transformers_From_Scratch_on_ImageNet_ICCV_2021_paper.html?ref=https://githubhelp.com]
> >
> > 论文简述：作者提出T2T-ViT,一种在中等数据集上训练效果接近CNN模型的ViT模型。该模型的创新点在于逐步的Tokenization和采用deep-narrow结构的骨干网络。逐步Tokenization的每一次处理过程包含重构和软切分(带有重叠的patch划分)两个步骤,通过反复执行上述“重构+软切分”的过程,可以逐步压缩图像token的长度和在token中融合局部结构信息。作者探索了多种CNN中的结构设计,如DenseNet中的密集连接结构、wide-ResNet中的"深窄 vs 浅密”结构、ResNeXt中的多头变种结构等,通过实验发现,采用deep-narrow结构效果最好。
> >
> >  [2025-04-14]-[张雪松]-[VLFM: Vision-Language Frontier Maps for Zero-Shot Semantic Navigation][2024 ICRA BSET PAPER](https://github.com/bdaiinstitute/vlfm)
> > 
> > <img width="512" alt="image" src="https://github.com/user-attachments/assets/e30d272b-b71b-4a04-a179-e6c6c2ef68b0">
> > 
> > 论文简述: 本文提出了一种零样本导航方法——视觉-语言边界地图（VLFM），该方法受人类推理过程的启发，旨在引导机器人从未见过的语义目标导航到新环境中。VLFM 利用深度观测构建占用地图以识别边界，并利用 RGB 观测和预训练的视觉-语言模型生成基于语言的价值地图。然后，VLFM 使用该地图来确定最有希望探索的边界，以找到给定目标对象类别的实例。在 Habitat 模拟器中的 Gibson、Habitat-Matterport 3D (HM3D) 和 Matterport 3D (MP3D) 数据集的逼真环境中评估了 VLFM。值得注意的是，VLFM 在所有三个数据集上的目标导向导航任务中均取得了最先进的性能（以路径长度加权的成功率衡量）。此外，实验证明了 VLFM 的零样本特性使其能够轻松部署到现实世界中的机器人上，例如波士顿动力公司的 Spot 移动操作平台。并展示了其在现实世界中无需任何环境先验知识的情况下，高效导航到办公室建筑内目标对象的能力。VLFM 的成就凸显了视觉-语言模型在推进语义导航领域方面的巨大潜力。
> 
> > [2025-04-14]-[于洋晨]-[A Dual Contrastive Learning Framework for Enhanced Multimodal Conversational Emotion Recognition][2025 Coling](https://aclanthology.org/2025.coling-main.272/)
> > 
> > <img width="512" alt="image" src="https://github.com/yyc-hfut/PaperReading/blob/main/DCLF.jpg">
> >
> > 论文简述: 本文提出了一种双对比学习框架（DCLF）。通过构建对话历史与当前目标utterance的正负样本（负样本为同一情感标签下的随机语句， 消除情感标签的影响）从而使得模型能够更加敏感地捕捉上下文语义。其次，本文提到，如果每个模态地正确预测全部保留，那最终地预测效果能达到W-f1的得分为81.7（大模型sota约为71），所以本文构建了一种模态贡献对比学习方法，以捕捉不同模态在识别倾向上地相关性和差异性。总结来说，其实很多思路都一样，对于文本模态来说，上下文极其重要，而音视频却不同，如何更加准确地理解文本上下文是提升模型性能的一种思路。然后就是，模态贡献的差异性，诸多任务都凸显了分离和理解各个模态贡献的重要性。
> 
> > [2025-04-13]-[徐赟博]-[Unseen from Seen: Rewriting Observation-Instruction Using Foundation Models for Augmenting Vision-Language Navigation]
> >
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/VLN-RAM.png" />
> >
> > 论文简述：本文提供了一种基于指令重写对VLN数据进行增强的方法。原始的VLN数据是人工注释的，只是对导航轨迹的解释。缺乏细节。本文提出对于导航场景，使用VLM提取场景描述，再使用LLM为原始的VLN指令数据添加新的Object和Layout信息形成全新的对导航轨迹的文本场景描述。接着，结合原始的场景描述和增强的场景描述，使用LLM对指令进行改写。最后，引入了新的混合训练策略，混合原始数据和增强数据，减少生成数据可能存在的噪声或者幻觉对结果的影响。
> > 
> >   [2025-04-13]-[贾朋]-[EmoTalkingGaussian: Continuous Emotion-conditioned Talking Head Synthesis](https://arxiv.org/abs/2502.00654)
> > <img width="512" alt="image" src="https://raw.githubusercontent.com/2351548518/images/main/20250120/20250408162826785.png">
> >
> > 论文简述: 基于3DGS的说话头合成方法通常仅在缺乏面部表情多样性的短视频上进行训练，生成的说话头难以表现丰富的情感变化，作者提出EmoTalkingGaussian，该模型能够基于连续情感数值（即效价和唤醒度）调控面部表情，同时保持唇部运动与输入音频的同步性。首先采用TalkingGaussian框架，基于音频和动作单元合成三维高斯泼溅式说话头部；随后使用EmoStyle生成唇形同步的情感面部图像，通过多样化情感面部数据训练EmoTalkingGaussian的情感操控模块。
> 
> >  [2025-04-13]-[张宇]-[Multi-scale Promoted Self-adjusting Correlation Learning for Facial Action Unit Detection]-[TAFFC24](https://arxiv.org/pdf/2308.07770)
> >
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/sacl.png" />
> >
> > 论文简述: 现有方法在au相关性建模方面存在固定位置先验的依赖，此工作也是想依赖au之间的位置先验，但是又不能使其固定不变缺乏泛化性，所以基于此出发点，提出了一种名为自调整au相关学习的方法。具体做法：利用固定位置先验作为基准通过网络学习自适应更新au相关图结构，并通过整合不同阶段的特征叠加来丰富感受野，提升au检测性能。该方法在参数量和计算量仅为最优方法的28.7%和12.0%的情况下，在BP4D和DISFA数据集上的性能优于现有方法（但是性能还是太低分别只有65.6 65.5的f1分数）。而且此方法太过依赖面部的关键点检测定位au的区域，如果关键点检测不准确，会拉低对后面的AU检测和相关性学习。
> 
> > [2025-04-13]-[陈银]-[Cacophony: An Improved Contrastive Audio-Text Model] TASLP
> >
> ><img width="512" alt="image" src="https://github.com/user-attachments/assets/e85c1106-dc11-468a-a092-24288a697746" />
>>
> > 论文简述: 音频-文本模型在规模和性能上落后于图像-文本模型（如CLIP）。现有音频-文本数据集规模有限（最大约10万对），且标签质量参差不齐（干净标签、噪声标签、弱标签/无标签）。本文提出通过扩大数据集规模和优化训练流程，提升对比式音频-文本模型的性能。本文构建了一个包含13000小时音频的音频-文本数据集（390万对），通过两阶段的训练策略，包括音频编码器的自监督预训练和对别-描述的训练，增强模型的表征能力。
>
> > [2025-04-13]-[何艺超]-[R1-Omni: Explainable Omni-Multimodal Emotion Recognition with Reinforcement Learning](https://arxiv.org/abs/2503.05379)
> >
> > 论文简述: 3月10号第二版刚挂在arxiv的工作，这篇工作首次将具有可验证奖励的强化学习（RLVR）应用于情绪识别的多模态大型语言模型，RLVR的思路来源于deepseek，在情感的子任务上可以让即使是没有详细数据的训练数据【如DFEW，MAFW只有category的标签】，也可以让模型根据这些训练数据获得推理的能力，但是从实验结果来看，分类的准确率和目前主流方法的准确率（如视觉指令微调的大模型，还有传统的分类方法）比差距还是很大。但是和EMER进行SFT的方法比要好一些，这里的原因还是EMER数据集的322条数据太少了，即使是指令微调了，也不会调的很彻底，总的来说是给了一个大模型做情感识别的新的方式方法：即通过没有描述的数据让大模型具备推理到情感标签的能力。
> 
> > [2025-04-08]-程浩-补上周：由于赶论文deadline，无精读
>
> > [2025-04-07]-[贾朋]-[InsTaG: Learning Personalized 3D Talking Head from Few-Second Video](https://arxiv.org/abs/2502.20387)
> > <img width="512" alt="image" src="https://raw.githubusercontent.com/2351548518/images/main/20250120/20250407135714879.png">
> >
> > 论文简述: 作者提出InsTaG，基于轻量级3D高斯泼溅人物特定合成器与通用运动先验，在保持高度个性化和高效的同时，实现了高质量快速适配。大多数NeRF类方法采用针对特定人物的训练，给定包含目标说话者的视频片段，整个针对特定人物的模型会被训练以记忆目标说话头部，对未见过的身份不具备泛化能力，需要大量高质量视频帧和长时间训练来适应每个新身份。作者通过预训练身份无关的3D动作场作为先验知识并将其与新身份对齐以加速适配，提出了基于高斯泼溅的即时说话头像合成框架InsTaG。
> 
> >[2025-04-06]-[于洋晨]-[因实验与假期，本周无精读]
> 
> > [2025-04-06]-[聂建涛]-[An Efficient Attribute-Preserving Framework for Face Swapping]-[TMM2024](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10400952)
> >
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/An%20Efficient%20Attribute-Preserving%20Framework%20for%20Face%20Swapping.png">
> >
> >论文简述：本文提出了一种高效的属性保持框架AP-Swap，用于面部交换，能够在保持身份一致的同时，有效保留目标面部的关键属性如头部姿势、表情和视线方向。方法包含两个创新模块，专门用于保留关键的面部属性。首先通过全局残余属性保留编码器（GRAPE）自适应地从目标人脸中提取全局完整的属性特征；其次，除了源面部图像和目标面部图像的常规网络流之外，引入了一个考虑到目标面部关键点的网络流。这个额外的网络流支持我们的面部关键点引导特征纠缠模块（LFEM），该模块通过执行基于地标的属性保留（LBAP）操作，有效地保留了细粒度的面部属性。
> >
> > [2025-04-06]-[徐赟博]-[Lookahead Exploration with Neural Radiance Representation for Continuous Vision-Language Navigation]-[CVPR2024](https://arxiv.org/pdf/2404.01943)
> >
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/HNR.png">
> >
> >论文简述：本文提出了一种基于Nerf合成未来视图的方法进行连续环境下的视觉语言导航的方法。首先，从多视角图像中抽取Feature Cloud 类似于稀疏点云。然后在连续导航的统一设定下，在每个导航轮次通过特征云再通过Nerf的可学习MLP建立对整个导航场景的三维隐式表示，合成关于未来视角的图像。包括多种特征，分别是未来视图的Nerf特征，和未来视图的渲染真实图像。分别将未来视图的Nerf特征和未来视图的GT CLIP特征做交叉熵损失和未来视图的渲染图像和未来视图的GT 图像进行交叉熵损失来建立对3D场景信息的保存。在推理过程中，对可选择的可导航点的未来图像进行预测和打分，选择最优打分的导航点作为下一步。但是该工作代码实现和论文内容不太一致，而且基于Nerf的方法需要太长的时间代价，4卡3090训练一个多星期，不太值得Follow。
> >
> > [2025-04-6]-[张宇]-[Facial Action Unit Detection by Adaptively Constraining Self-Attention and Causally Deconfounding Sample][IJCV 2024](https://arxiv.org/abs/2212.10071)
> >
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/ACD.png">
> > 
> > 论文简述:现有AU检测方法大多直接使用自注意力机制空间建模依赖，但容易受到伪相关性的干扰。所以本文从注意力约束和因果先验去混淆两个角度入手，提升 AU 检测性能。首先空间约束自注意（ACS），在原有自注意力结构上加入两类约束（先验和语义）：先验约束通过 AU 区域信息指导注意力分布，语义约束则根据样本标签相似性自适应调整注意力图，提升 AU 相关区域的权重，然后再用空间因果去混淆（SCD），引入扰动机制和因果效应建模，显式估计某一区域对 AU 预测的实际因果影响， 实验证明在 AU 复杂、标签不均衡的场景下更具优势。但是该方法仅限于空间维度，未对 AU 的时序因果演化进行建模，也未考虑模态间的因果关系，或许可以进一步引入时间因果图的建模思路，将面部区域在连续帧中的动态变化看作潜在干预因素，探究某些区域的早期活动是否导致后续 AU 的激活。
> >
> > [2025-04-6]-[何艺超]-[Large Language Models Are Reasoning Teachers][ACL 2023](https://arxiv.org/abs/2212.10071)
> >
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/Fine-tune-CoT.png">
> > 
> > 论文简述: 基于提示的CoT方法依赖于非常大的模型，基本都是几十B甚至几百B。作者提出Fine-tune-CoT，使用绝对大的大模型作为推理教师，获取特定任务下的CoT样本来微调0.3-10B模型。具体有三个步骤：Step 1. Reasoning generation【推理生成】，从具备CoT的模型中获取样本的推理链条。Step 2. Curation：筛选生成的样本并将其重新格式化为提示完成对。Step 3. Fine-tune：正常地基于下一个词预测任务。与此同时，方法可以选择性地生成多个推理链条，并使用随机采样从教师模型中生成多个推理解，以增加学生模型的训练数据。局限性：都是基于文本，多模态尚未探索，不过也很好转化。
> >
> > [2025-04-5]-[张雪松]-[UniGoal: Towards Universal Zero-shot Goal-oriented Navigation](https://arxiv.org/abs/2503.10630) arxiv2025.3.18
> >
> > <img width="512" alt="image" src="https://github.com/user-attachments/assets/3c612ef4-f138-4ba7-bd76-a293a2bd02f1">
> > 
> > 论文简述: 本文提出了UniGoal，一个面向通用**零样本目标导航ZSON**的框架，旨在提升模型在未知导航任务上的泛化能力。该方法采用多阶段预训练策略，先利用大规模图文数据学习通用感知与语言理解能力，再通过任务相关数据引导模型学习导航语义。同时引入多模态表示对齐机制，将语言、视觉和目标表示统一到共享语义空间，实现任务间的知识迁移和出色的导航表现。
> 
> > [2025-04-5]-[陈银]-[MMA-DFER: MultiModal Adaptation of unimodal models for Dynamic Facial Expression Recognition in-the-wild] CVPRW 2024
> >
> > <img width="512" alt="image" src="https://raw.githubusercontent.com/cyinen/imge/master/20250319183145.png">
> >
> > 论文简述: 不同与自监督预训练或者将FER模型扩展到DFER, 本文提出了一种新的方法，将预训练的MAE-Face和AudioMAE模型用到多模态的DFER上面。通过引入prompts适配各自的模态，并通过Fusion Bottleneck实现夸模态的融合，并在最后引入时序Transformer进行时序建模。本文提出的方法在DFER和MAFW上都实现了很高的性能，但是视觉模型仍依赖在规模人脸数据集上预训练。
>
> > [2025-03-31]-[于洋晨]-[Multimodal Emotion Recognition with Target Speaker-Based Facial Embeddings][2025 ICASSP](https://ieeexplore.ieee.org/abstract/document/10888205)
> > <img width="512" alt="image" src="https://github.com/yyc-hfut/PaperReading/blob/main/TSB.jpg">
> >
> > 论文简述: 继FacialMMT（2023 ACL）之后，今年很多基于Speaker's Face去做视频模态的特征增强任务。但目前来说，整体效果一般，单模态的视频效果仍然较差，其原因可能是在对话场景中，真实情感更多的表露在语言中，而表情中表露的真实情感更少，或者说简单的facial expression recognize任务很难察觉表观下的真实情感。例如，苦笑，尴尬的笑，冷笑，表观上看起来，都是笑容，但是其真实情感可能还是要结合具体语境进行判断，很难依靠面部表情识别其真实的，在多模态场景下的情感。
> 
> >   [2025-03-30]-[贾朋]-[TalkingGaussian: Structure-Persistent 3D Talking Head Synthesis via Gaussian Splatting](https://arxiv.org/abs/2404.15264)
> > <img width="512" alt="image" src="https://raw.githubusercontent.com/2351548518/images/main/20250120/20250330162410057.png">
> >
> > 论文简述: 作者提出TalkingGaussian，基于点的高斯泼溅技术，通过对持久高斯图元施加平滑连续的形变来表征面部运动，无需像既有方法那样学习复杂的外观变化。在此形变范式下，作者发现面部-口腔运动的不一致性会影响细微发音动作的学习。为此，作者将模型设计为分别处理面部和口腔区域的双分支结构，从而通过简化学习任务来重建更精确的口腔运动与结构。
> 
> > [2025-3-30] -[何艺超] - [MoE-LLaVA: Mixture of Experts for Large Vision-Language Models] - [Arxiv](http://arxiv.org/abs/2401.15947)
> > <img width='512' alt='image' src="https://github.com/ReadingPapers/Report/blob/main/Images/MoE-LLaVA.png"/>
> >
> > 论文简述：作者设计MoE-Tuning，对应的模型称作MoE-LLaVA 。该架构通过稀疏激活的专家机制，在保持计算成本不变的情况下可以扩展模型参数量。效果来说，MoE-LLaVA 只有大约 3B 的稀疏激活参数，但是可以在各种视觉理解数据集上表现出与 LLaVA-1.5-7B 相当的性能。MoE-LLaVA的核心是采用三阶段训练策略，第一阶段只训练视觉projector来进行多模态的对齐；第二阶段使用更复杂的指令，包括图像逻辑推理和文本识别等任务，这些任务使得模型具有更强的多模态理解【本质是指令微调】；第三阶段把第二阶段的ffn权重迁移至MoE的ffn中，模型上MoE ffn和原始ffn交替，以得到参数量削减的稀疏模型。
>
> > > [2025-3-30] -[张雪松] - [NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models]
> > <img width='512' alt='image' src="https://github.com/user-attachments/assets/a538981c-9345-4045-925b-4a702552a497"/>
> > 
> > 论文简述：本文提出NavGPT-2，一种结合大型视觉语言模型（VLM）与导航策略网络的方法，旨在提升视觉语言导航（VLN）任务的性能。通过冻结预训练的LLM，利用Q-former将多视角图像编码为固定长度的视觉标记，并注入方向信息生成导航提示。NavGPT-2采用两阶段训练：第一阶段微调Q-former生成导航推理，第二阶段结合拓扑图策略网络进行动作预测。实验表明，该方法在数据效率和性能上优于现有VLN专家模型，同时保留了LLM的解释能力，能够生成人类可理解的导航推理。
> 
> > [2025-3-30] -[徐赟博] - [VLFM: Vision-Language Frontier Mapsfor Zero-Shot Semantic Navigation] - [ICRA2024 最佳论文](https://arxiv.org/pdf/2312.03275)
> > <img width='512' alt='image' src="https://github.com/ReadingPapers/Report/blob/main/Images/VLFM.png"/>
> >
> > 论文简述：本文提供了一种在目标导航任务中的零样本方法。作者认为在人类在未知环境中进行导航所依靠的是环境中的语义知识来推断环境的布局。而大模型在训练过程中富含的先验语义知识可以用来促进Zero Shot导航能力。该方法提出基于空间语义相似度构建Value Map， 类似于SLAM，但核心是每个位置的视觉观察和目标物体（该任务最终的目的是找到对应的一个物体而非指令）的余弦相似度得分，同时，在生成语义得分的同时，另外又增加了置信度通道，由观察位置和视角光轴中心的距离决定，用来决定语义分数通道的更新方式。因为智能体在模拟环境中移动的时候观察全景图像存在视角重叠的问题，靠近光轴中心的被完全更新，边缘的以更小的权重更新。论文其他部分和连续导航存在差距，没完全精读。
>  >
>>[2025-03-29]-张宇-[PE-CLIP: A Parameter-Efficient Fine-Tuning of Vision Language Models for Dynamic Facial Expression Recognition]-[arxiv](https://arxiv.org/abs/2503.16945)
><img width='512' alt='image' src="https://github.com/ReadingPapers/Report/blob/main/Images/PE-CLIP.png"/>
>
>论文简述：目前用clip做dfer任务的通常需全参数微调，会破坏原有参数导致严重记忆遗忘问题。本篇工作也就是针对这一问题，设计了一种参数高效微调框架，通过时序动态adapter（TDA）和共享adapter（ShA）增强CLIP的时序建模能力与跨模态对齐。TDA通过GRU的动态缩放机制自动学习时序变化（也就是通过常规的adapter（但其中的激活函数用的是GELU）在clip的内部将多帧融合，不需要再经过时序模型），ShA以轻量瓶颈结构同步优化视觉与文本特征（这个adapter通过架构图也能看出是共享参数的）。最终通过面部动作单元（AU）的语义描述作为监督信号实现分类。最终实验表明，PE-CLIP在DFEW和FERV39K数据集上只有9M可训练参数，而且表现也不错，DFEW上的WAR达到了75.35%（目前在FER任务上对clip进行高效参数微调是一个不错的想法）。
>
>>[2025-03-29]-陈银-[CoPL:Parameter-Efficient Collaborative Prompt Learning for Audio-Visual Tasks] MM 2024
><img width='512' alt='image' src="https://raw.githubusercontent.com/cyinen/imge/master/20250328095606.png"/>
>
>论文简述： 将预训练的uni-modal模型迁移到多模态的任务能够减少训练的cost, 但是现有的方法主要关注的是多模态的融合，忽视了忽视模态特定的微调环节。为了解决这个，本文提出了高效的协作式提示学习（CoPL）来微调 uni-modal和多模态特征，包括模态特定的prompt与任务耦合的prompts, 并通过prompt bank实现基于实例的特征提取。最后使用MSE做模态对齐。在下游的几个音视频任务上以更少的参数实现了更好的性能。
>
>>[2025-03-22]-陈银-[Align before Adapt: Leveraging Entity-to-Region Alignments for Generalizable Video Action Recognition] CVPR 2024
><img width='512' alt='image' src="https://raw.githubusercontent.com/cyinen/imge/master/20250312181304.png"/>
>
>论文简述：本文提出了 Align Befor Adapt， 在Adapt 视频表征学习之前，先利用每一帧与实体-区域对齐。通过对区域感知图像嵌入与离线构建的文本语料库进行匹配，实现了实体对齐。利用已对齐的实体，将其文本嵌入作为查询输入到基于Transformer的视频适配器中，这有助于将视频中最重要的实体语义提取为向量表示。<font color="#366092">该范式在适配过程中重用了视觉语言预训练（VLP）的视觉-语言对齐机制，并尝试通过底层实体来解释动作。这种方法通过弥合复杂活动语义的差距来促进对动作的理解，特别是在面对不熟悉或未见过的类别时。</font>
>
>>[2025-03-22]-张雪松-[ETPNav: Evolving Topological Planning for
Vision-Language Navigation in Continuous
Environments](https://ieeexplore.ieee.org/abstract/document/10495141/)（TPAMI2024）
><img width="512" alt="image" src="https://github.com/user-attachments/assets/3d838dcf-6172-4b37-9737-f535762dc538" />
>
> 论文简述：连续和离散环境的VLN任务最主要的区别在于。连续环境中的agent无法依赖prior的graph map的结构进行high-level的导航，只能进行low-level的导航。为此，[之前的工作](https://github.com/YicongHong/Discrete-Continuous-VLN)已经设计了一个Candidate Waypoints Predictor来预测下一个可导航点，使得agent在连续的环境中来执行highlevel的导航动作。本文基于此，构建一个graphmap来预测下一个最可能的可导航点，然后使用一个参数化的控制器执行低水平的action从而实现导航任务。除此之外，（1）之前的Waypoints Predictor都是基于RGB-D的，而本文发现仅用depth效果更好。（2）本文对避障进行了简单的处理。最后，本文大幅提升了agent的导航表现，为连续环境下的VLN任务提供了一个strong baseline。

>>[2025-02-23]-张雪松-[NaVILA: Legged Robot Vision-Language-Action-Model for Navigation](https://navila-bot.github.io/)
><img width="512" alt="image" src="https://github.com/user-attachments/assets/37d1283a-2e83-4693-a6d2-b6ccafce7cf6" />
> 
> 论文简述：本文（应该是首次）将视觉语言导航任务成功落地到基于腿式机器人（宇树四足Go2和人形H1机器人）的室内室外导航场景。本文的方法分为两阶段，第一阶段是预训练视觉语言模型VILA（包含视觉encoder，一个projector，和一个LLM），其中预训练资源包括仿真环境中的VLN数据集，处理后的youtube中的视频，具身问答数据集等，有监督的微调阶段不冻结LLM的参数。第二阶段是使用单阶段的基于强化学习的动作学习策略，将第一阶段的语言输出等信息转换为四足的DoFs的低水平动作。实验结果，以单个view作为输入超过了gridmm在仿真环境的表现，在自己造的真实场景下也达到了将近50%的成功率。首次投ICLR2025被拒 https://openreview.net/forum?id=gkDRrvqeWF
> 
> >[2025-02-09]-[何艺超]-本周未精读
> >
>>[2025-02-09]-陈银-[HumanOmni: A Large Vision-Speech Language Model for Human-Centric Video Understanding]
><img width="512" alt="image" src="https://github.com/user-attachments/assets/af0134ac-3211-45a4-b14b-1fc320bd2eb7" />
>
> 论文简述：本文提出了一种以人为中心的视觉语音大模型，并构建了一个包含240万个人中心视频片段和详细caption的数据库，以及超过1400万个指令对。在“情感识别、面部表情描述和动作理解”方面，模型表现优异的性能。数据集的构建流程如下: 场景检测和分割->过滤低分辨率 clip->使用 Qwen2-VL 去掉相似的 clip->最终使用自动标注标注出人脸和身体的 bounding box->然后用两个多模态 VLM 模型进行caption，并使用大预言模型进行汇总共同点,去除幻觉，按照特定模板生成 Instrucitons。除此之外，还手动对其中的 50K 同时包含视觉和语音的video clips 进行了标注，用于后续的微调。模型的训练分为三阶段，视觉对齐，语音对齐，以及多模态交互。最终在DFEW， MAFW 上的 WAR分别达到了 82.46%，68.40%.
> 
> > [2025-01-26]-陈银-[A Multi-Task Learning Framework for Emotion  Recognition Using 2D Continuous Space]-TAFFC
>
>><img width="512" alt="image" src="https://github.com/user-attachments/assets/c41e9e42-aeb2-445d-8893-85fb7f112603" />
> 
> 论文简述：本文提出了一种基于深度信念网络（DBN）框架的多任务学习方法，用于情绪识别。该方法将情绪类别识别作为主要任务，将激活度和情感价的识别作为次要任务，通过两种策略——基于类别水平的分类和基于连续水平的回归——将次要任务整合到情绪识别系统中。实验在IEMOCAP和SEMAINE两个数据库上进行，结果表明，与仅使用基线特征的方法相比，该多任务学习框架在未加权准确率上取得了显著提升，证明了利用激活度和情感价信息在情绪识别中的有效性。
> > [2025-01-26]-程浩-本周未精读
> 
> > [2025-01-26]-[张雪松]-本周未精读
>
> > [2025-01-26]-[聂建涛]-本周未精读
>
> > [2025-01-26]-[于洋晨]-本周未精读
> 
> > [2025-01-26]-[贾朋]-本周未精读
> > 
> > [2025-01-26]-[徐赟博]-本周未精读
> > 
> > [2025-01-26]-[何艺超]-[Omni-Emotion: Extending Video MLLM with Detailed Face and Audio Modeling for Multimodal Emotion Analysis](https://arxiv.org/pdf/2501.09502v1)
> > 
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/Omni-Emotion%20MLLM.png">
> >
> > 论文简述：Emotion-LLaMA的后续工作，和Emotion-LLaMA非常相像，细节上更加精细，并且在MER-OV，EMER，recognition三个任务上都做了实验，十分全面，首先训练过程从二阶段调整成三阶段，即分步把音频对齐，视频对齐和细粒度微调，而不是视频音频一块对齐。在数据集的处理阶段，相较于emotion-llama完全借助大模型和AU知识生成caption，这里没有使用AU知识，而是借鉴了EmoLLM使用到的FaceXFormer针对年龄，性别，以及人脸特征提取，虽然instruction也是借助大模型提取，但是增加了gpt-3.5和人工的审查机制（emotion-llama没有审查），最终得到24137和3500的细粒度，粗粒度数据。然后模型设计这块，没有什么创新，单纯讨论了一下project在针对全画幅特征和人脸特征的融合机制，分别是：逐帧拼接，交叉注意力计算，以及全视频最终特征拼接。
> 
> >  [2025-1-26]-[张宇]-[CRoF: CLIP-based Robust Few-shot Learning on Noisy Labels]-[arxiv](https://arxiv.org/abs/2412.12793)
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/corf.png">
> >
> > 论文简述: 本篇工作是改进CLIP去适应噪声下的小样本学习，主要的出发点对准的是噪声这个特性，考虑到clip会把相似的类别误判的主要原因是：1）标签噪声导致2）clip的硬匹配造成，所以解决这一现状让clip在多噪声的环境下还能有较高得泛化性只能从第二点出发，设置加权平滑的软匹配，即top-k加权策略（目标如果被占cos得分的第一，赋值为1，如果在top-k中，那么通过加权把它的得分赋值到排名的最高，如果超出了范围，那么得分就是0），这也是本文主要的创新点之一，其他两点在我看来没有那么新颖感觉像是拼凑上去的（1.通过gpt-o1来进行prompt的生成，一个类别生成五组2.微调clip的两端，使用adapter+残差连接）这三点后面的消融中也都感觉很独立，所以在我看来，多噪声环境下的小样本学习主要还是靠clip的软匹配来获益的，总的来说这三点可以看成是一种策略，即插即用的训练策略，比较轻盈高效。
> 
> > [2025-1-19]-程浩-赶论文，未精~~度~~读
> > 
> > [2025-1-19]-[何艺超]-加紧多模态自监督的数据集微调数据传输等+家中琐事，本周没有来得及精读
> > 
> > [2025-1-19]-[张宇]-[本周没有精读]
> 
> > [2025-01-19]-[贾朋]-[Emotional Conversation- Empowering Talking Faces with Cohesive Expression, Gaze and Pose Generation](https://arxiv.org/abs/2406.07895)
> > <img width="512" alt="image" src="https://raw.githubusercontent.com/2351548518/images/main/20241219/20250113171534297.png">
> >
> > 论文简述: 提出了一个两阶段音频驱动的说话人脸生成框架，该框架采用 3D 面部标志作为中间变量，通过自监督学习实现了表情、注视和姿势的协调。作者将此任务分解为两个步骤，即语音到landmark的合成和landmaek到人脸的生成。语音到landmark合成：给定参考图像，提取归一化的landmarks、gaze和 head pose，并预测由输入语音和情感标签驱动的每帧运动。landmark到面部生成：每帧的facial landmarks被映射到潜在关键点，然后将其输入到预训练face-vid2vid模型中以生成最终的面部。
> 
> >[2025-1-18]-[徐赟博]-[AeroVerse: UAV-Agent Benchmark Suite for Simulating, Pre-training, Finetuning, and Evaluating Aerospace Embodied World Models]
> >
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/AeroVerse.png">
> >
> >论文简述：本文是构建无人机领域的新尝试。仿真器提供进行连续导航的条件。在两个典型城市（京沪）和学校以及居民区两个场景下搜集了大量的RGB图像和对应的深度图像，多个方向的环境caption。下游涵盖了场景描述，空间推理，导航决策等五个任务。提供了多个不同特点的数据集-以无人机为中心，以环境描述为中心，以空间推理为中心。论文看上去为无人机导航提供了很多可以直接用的插件。
> > 
> > [2025-1-17]-[聂建涛]-[Guided Interpretable Facial Expression Recognition via Spatial Action Unit Cues]-[paper](https://arxiv.org/pdf/2402.00281)
> >
> >  <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/Guided%20Interpretable%20Facial%20Expression%20Recognition%20via%20Spatial%20Action%20Unit%20Cues.png">
> >
> >论文简述：本文提出了一种新的学习策略，通过将空间动作单元（AU）线索显式地融入分类器训练，从而训练出深度可解释的模型。作者采用输入图像、表情标签和面部标记，结合AU代码本构建AU热图，以此作为空间线索，约束分类器的空间层特征与AU热图相关联，使用复合损失函数进行训练，以实现同时正确分类图像并生成与AU图相关的层内可视化注意力。
>
> >[2025-1-17]-[张雪松]-[NAVILA: LEGGED ROBOT VISION-LANGUAGEACTION MODEL FOR NAVIGATION](https://navila-bot.github.io/)
>> <img width="512" alt="image" src="https://github.com/user-attachments/assets/2c909803-ef4c-4f8b-8530-667796075ed7" />
>>
>>论文简述：本文主要关注腿式机器人的视觉语言导航。和之前的VLN任务相比，本任务的主要挑战在于，将自然语言指令迁移(translate)到机器人的腿部动作并不简单。暂时略读，一些demo比较炫酷，而且是面向实际应用场景的，代码似乎可用，准备未来fellow！
> 
>>[2025-1-17]-[陈银]-[Lifting Scheme-Based Implicit Disentanglement of Emotion-Related Facial Dynamics in the Wild]
>><img width="512" alt="image" src="https://github.com/user-attachments/assets/16272c26-1dd5-4807-a938-02079914fdc1" />
>>
>>论文简述：在自然场景的视频中，情感相关的表情往往被时间上和空间上的非情感相关表情和全局背景信息所稀释，这使得情感识别变得困难。本文提出了一种新的隐式面部动态解缠框架，通过小波提升方案（lifting scheme），在不使用显式操作或外部引导的情况下，隐式地将情感相关动态信息从全局背景信息中分离出来，减轻了情感无关帧的负面影响。
>
> 
> >[2025-1-13]-[徐赟博]-[ENVEDIT: Environment Editing for Vision-and-Language Navigation]
> ><img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/Envedit.png">
> >
> >论文简述：本文提出了一种对VLN的数据增强技术，在保留关键语义信息的基础上，改变房间的风格、外观、对象类别来减少训练集和测试集之间的分布迁移问题。通过条件可控生成的技术进行环境的编辑，其中注入生成模型的控制条件是语义分割的mask，这样可以保留到环境中的语义信息（和一定程度的结构信息）。在生成数据和原始数据的可视化对比中，作者发现具有高度的对应关系。在导航能力的定量对比中，这种数据增强技术可以同时在Seen和Unseen场景下提升明显
> >
> >[2025-01-13]-[于洋晨]-上周多次期末考试以及课程报告，未精度
>
> > [2025-01-12]-[贾朋]-[SyncTalk- The Devil is in the Synchronization for Talking Head Synthesis](https://ziqiaopeng.github.io/synctalk/)
> > <img width="512" alt="image" src="https://raw.githubusercontent.com/2351548518/images/main/20241219/20250109155213773.png">
> >
> > 论文简述: 传统的GAN很难保持一致的面部身份，而NeRF方法虽然可以解决这个问题，但通常会产生不匹配的嘴唇运动、面部表情不足和不稳定的头部姿势。作者提出了 SyncTalk这种基于 NeRF 的方法有效地保持了主体身份，增强了头部说话合成的同步性和真实感。SyncTalk 采用Face-Sync Controller将嘴唇运动与语音保持一致，并使用 3DMM 模型来捕捉准确的面部表情。大量的实验和用户研究表明，SyncTalk 在同步性和真实性方面优于最先进的方法。
> 
> > [2025-01-12]-[张雪松]-[Robustness Analysis of Video-Language Models Against Visual and Language Perturbations](https://proceedings.neurips.cc/paper_files/paper/2022/hash/de6ff07cbd222c10d694c2b2f732aceb-Abstract-Datasets_and_Benchmarks.html)
> > <img width="512" alt="image" src="https://github.com/user-attachments/assets/65b0509c-6b61-409a-a64e-63d0ff171650">
> >
> > 论文简述: 本文首次对视频-语言模型在真实世界扰动下的鲁棒性进行了广泛的研究。作者提出了两个大规模基准数据集（MSRVTT-P和YouCook2-P），并利用90种视觉和35种文本扰动进行测试，从而揭示出一些有趣的初步发现：1）当仅文本被扰动时，模型比仅视频被扰动时更为鲁棒；2）经过预训练的模型比从头训练的模型更鲁棒；3）模型更多地关注场景和物体，而不是动作和运动。
> 
> > [2024-1-12]-[张宇]-[Waffling around for Performance: Visual Classification with  Random Words and Broad Concepts]-[ICCV23](https://openaccess.thecvf.com/content/ICCV2023/html/Roth_Waffling_Around_for_Performance_Visual_Classification_with_Random_Words_and_ICCV_2023_paper.html)
> >
> >  <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/wallfclip.png">
> >
> >论文简述：LLM+CLIP的工作，但是探究的点很新颖，是探究性工作，一般LLM+CLIP的工作是使用类名单词作为强约束，多组符合常理的描述句子跟在类名单词后面作为细粒度的区分，但是这样的话很有可能是多组文本平均带来的性能提升，因为可能VLM不会真正理解那些一个类别多个描述的人为认定的细粒度描述语句（后面作者在实验中也证实了，如果它是从语言描述的细粒度程度上收益，那么按理说多组文本取max替代一般工作的mean方式也会表现不错，但是实验显示了换成max性能会大幅下降），这就更证实了作者的要探究的观点（胡乱的随机词作为类名之后的细粒度描述也能表现很好），所以也属于改进zero-shot的工作，prompt形式是“A photo of a {concept}: a {c}, which (is/has/etc) {随机单词或字符}.”所以还是有类名单词作为强约束的（因为后面消融也显示了只有随机字符作为监督性能不佳），为什么随机单词也能有区分呢？->可以把它看做是不同形式的噪声集成，可以理解成数据加噪，性能超越了使用专门设计的细粒度语言描述的工作。
>
> > [2024-1-12]-程浩- $\texttt{赶实验赶论文，未精读}$ \
> > [2025-1-12]-[聂建涛]-[Detecting Facial Action Units From Global-Local Fine-Grained Expressions]-[paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160018)
> >
> >  <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/Detecting%20Facial%20Action%20Units%20From%20Global-Local%20Fine-Grained%20Expressions.png">
> >
> >论文简述：论文提出了一种新的面部动作单位（AU）检测方法GLEE-Net，通过全局-局部面部表情嵌入技术，缓解了因AU注释需要专业知识导致的身份过拟合问题，同时利用了未被标记的的面部表情数据集来辅助AU检测。GLEE-Net框架包含三个分支，分别提取与身份无关的表情特征，其中全局分支消除身份影响建模整体面部表情，局部分支关注特定面部区域。该方法先在面部表情数据集上预训练得到身份无关的表情嵌入，然后在小量的AU数据集上进行微调。此外，引入了3D全局分支通过3D人脸重建提取表情系数以加强2D AU描述，最终使用基于Transformer的多标签分类器融合所有表示进行AU检测。
>
> > [2025-01-11]-何艺超-[Contrastive Instruction Tuning]-[ACL 2024 Findings](http://arxiv.org/abs/2402.11138)
> > <img width="512" alt="image" src=https://github.com/ReadingPapers/Report/blob/main/Images/COIN.png>
> >
> > 论文简述：作者针对指令微调在具体任务中过于依赖指令而在用户指令输入的单词错误，拼写错误，语法错误，语义错误中的表现不佳的问题，提出一种基于对比学习的指令微调，正样本对是原始指令和原始输入输出&原始指令改写指令和原始输入输出，然后负样本对是原始指令和原始输入输出&与任务无关指令和原始输入输出，或者是原始指令和原始输入输出&原始指令和与任务无关输入输出，同时将对比学习损失和LLM生成需要用的交叉熵损失结合，通过超参控制二者结合的比例，这种方法可以增强模型针对同一样本对于不同指令的语义理解，从而提高下游准确性。
> > 
> > [2025-01-11]-陈银-[Label-Guided Dynamic Spatial-Temporal Fusion for  Video-Based Facial Expression Recognition](https://ieeexplore.ieee.org/abstract/document/10552397)-[TMM]
> >
> > <img width="512" alt="image" src=https://raw.githubusercontent.com/cyinen/imge/master/20250108133119.png>
>>
>>论文简评：考虑到标签分布提供了图像的分类保真度，本文使用表情的分布来引导时空的融合。把 video的标签作为 label, 为每一个帧分配视频的 label, 并设计辅助损失函数来监督。利用学习到的标签分布计算一个动态权重，并用把权重用于时空融合。==有一种把监督信号加到中间，打破信息瓶颈的味道。==实验验证了为每一帧分配不同的权重是很重要的，能够让模型把注意力关注到更重要的帧上面，从而利用更有用的信息。**这个方向还有待深入研究**。
> 
> > [2025-1-5]-程浩-放假+实验+写论文=未精读\
> >
> > [2025-1-5]-[何艺超]-准备最后两门考试，未精读
> >
> > [2025-1-05]-[张雪松]-[ AeroVerse: UAV-Agent Benchmark Suite for Simulating, Pre-training, Finetuning, and Evaluating Aerospace Embodied World Models]-[paper](https://arxiv.org/pdf/2408.15511)
> >
> > <img width="512" alt="image" src="https://github.com/user-attachments/assets/10fec248-3237-46da-aeb7-2588ba9525cd">
> >
> > 论文简述：本文提出AeroVerse，作为无人机智能体领域的全新基准套件，包含AeroSimulator仿真平台、首个大规模真实图文预训练数据集AerialAgent-Ego10k与虚拟图文姿态对齐数据集CyberAgent-Ego500k，以及支持五类下游任务（场景感知、空间推理、导航探索、任务规划和动作决策）的微调数据集SkyAgent系列。此外，通过GPT-4开发的SkyAgent-Eval，实现更全面和客观的任务评估，有效揭示了现有2D/3D视觉语言模型在无人机任务中的潜力与局限性.（暂未开源，简单了解不算严格意义上的精读）
> >
> > [2025-1-05]-[徐赟博]-[SAME: Learning Generic Language-Guided Visual Navigation with State-Adaptive Mixture of Experts]
> >
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/SAME.png">
> >
> > 论文简述：本文提出了一种使用MOE的架构统一不同视觉语言导航任务的学习。作者首先使用了单独学习的方式，对不同语言引导程度的导航任务进行学习，发现这种方式会导致不同的任务之间产生负迁移的现象。于是，作者在Cross-Attention层引入MoE来促进多任务学习。但是，作者发现，直接进行moe的效果并不好，分析原因可能发生在不同导航任务的语言引导程度上（Zero/fine/coarse grined），于是作者在MoE层，预测路由概率时加入了与任务以及多模态相关的信息。促进了七种导航任务的多任务学习。实验做得不太完善，进行对比的很多工作都是没有在某个数据集上训练过的，但是也是在VLN中使用MoE的先河。
> >
> > [2025-01-05]-[贾朋]-[LES-Talker: Fine-Grained Emotion Editing for Talking Head Generation in Linear Emotion Space](https://peterfanfan.github.io/LES-Talker/)
> > <img width="512" alt="image" src="https://raw.githubusercontent.com/2351548518/images/main/20241219/20250105215301644.png">
> >
> > 论文简述: 虽然现有的Talking head模型在粗粒度情感编辑方面取得了进展，但仍然缺乏具有高可解释性的细粒度情感编辑模型。作者提出了 LES-Talker，一种具有高可解释性的Talking head生成模型，以实现不同情感类型、情感水平和AU的细粒度情感编辑，提出了基于AU的线性情感空间（LES）定义，将情感变换描述为向量变换。设计了跨维度注意力网络（CDAN）来挖掘 LES 表示和 3D 模型表示之间的相关性，使 LES 表示能够指导 3D 模型的可控变形。
> 
> > [2025-1-5]-[张宇]-准备最后两门考试，未精度
>
> > [2025-1-5]-[聂建涛]-[FaceMixup: Enhancing Facial Expression Recognition through Mixed Face Regularization]-[paper](https://arxiv.org/pdf/2405.20259)
> >
> >  <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/FaceMixup.png">
> >
> >论文简述：论文提出了一种新的面部数据增强方法，名为FaceMixup，旨在通过混合面部组件正则化来提高面部表情识别的性能。其主要思路是通过对不同类别的面部图像进行裁剪和替换操作来创建新的混合面部图像，并在训练过程中考虑这些类别信息。FaceMixup方法通过混合面部组件进行正则化，生成额外的训练样本，以此丰富数据集并提高深度学习模型的泛化能力。
>
> > [2025-01-05]-[陈银]-这周在改论文，未精度


为简化排版，2024年小组精读论文已经保存在[这里](https://github.com/ReadingPapers/Report/blob/main/Archived/2024%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E5%BD%92%E6%A1%A3.md)







