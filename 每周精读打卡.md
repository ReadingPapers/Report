######### 模版 #############

> week1 同一周在同一个level的bar上
> 
> [日期]-[first]-[论文标题]-[来源]（附带链接，链接可选，时间倒序）\
> >
> > 论文框架图（有助于一眼就能想起论文内容），宽度设置为512
> >
> > 论文简述 （一两句话总结精华，切勿过长）
> 
> [日期]-[second]-[论文标题]-[来源]（附带链接，链接可选，时间倒序）\
> >
> > 论文框架图（有助于一眼就能想起论文内容）
> >
> > 论文简述 （一两句话总结精华，切勿过长）
> 

> week2
> 
######### 模版 #############

## 必须是【全文】精读的论文，如实记录

> ==> 时间倒序，本次更新插入位置 <==
> 
> >[2025-07-06]-[张宇]-[Understanding and Mitigating Annotation Bias  in Facial Expression Recognition](https://arxiv.org/abs/2108.08504)
> >
> > 论文简述: 数据集的bias会导致模型被带偏，假设这个bias不是人们统一的认知，所以这篇工作以此为出发点，作者通过使用 OpenFace 识别图像中的au，系统的考察了多种wild与lab表情数据集中的性别标注偏差，发现在实验室数据集中，当动作单元强度相同时，男性与女性被标注为happy或angry的概率基本一致，而在ExpW、RAF‑DB 及 AffectNet 等wild数据集中，即便 AU 已被控制，女性依然更容易被标注为happy、男性更容易被标注为angry，说明标注偏差确实存在并且会被模型学习。为了解决这一偏差，作者利用 AU‑校准表情识别框架：在常规的交叉熵损失之外，构建以 AU 相似度为基础的三元组损失，将具有相同 AU 组合的样本拉近、不同的拉远，从而强制模型更多地依据客观的 AU 信息而非性别特征进行表情判断。
> 
> >[2025-07-06]-[贾朋]-[GGTalker: Talking Head Systhesis with Generalizable Gaussian Priors and Identity-Specific Adaptation](https://arxiv.org/abs/2506.21513)
> > <img width="512" alt="image" src="https://raw.githubusercontent.com/2351548518/images/main/20250424/20250706222130651.png">
> >
> > 论文简述: 现有Talking Head方法能在固定视角与小范围音频变化下取得令人满意的效果，但在大幅头部旋转和分布外(OOD)音频条件下表现欠佳，作者认为核心问题在于缺乏足够的3D先验知识，这限制了合成数字人的外推能力。作者提出GGTalker，通过融合先验与身份适应来合成数字人脸。引入两阶段的"先验-适应"训练策略：首先学习高斯头部先验，再适配个体特征。通过训练音频-表情先验与表情-视觉先验，分别捕获唇部运动的规律和头部纹理的通用分布；在定制适应阶段，则精细建模个体说话风格与纹理细节。
> 
> >[2025-07-06]-[徐赟博]-[VLN-R1: Vision-Language Navigation via Reinforcement Fine-Tuning]
> >
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/VLNR1.png">
> > 
> > 论文简述：本文提出了一种在VLN领域引入类R1推理模型的工作。核心贡献主要有三点1）数据集上，在连续模拟器上通过agent的GT 轨迹和动作收集了第一视角的动作视频流；2）模型结构上，在token选择上，根据导航agent的特性，设计了一种长短期自适应采样，以较高的概率选取短期内的记忆sample作为短期导航状态，并且选择一段时间之前的sample保持长期状态；3）训练上，除了使用了GRPO的训练策略之外，还设计了一种根据时间/step自适应的奖励衰减机制。个人感觉主要贡献在于第一视角导航视频数据的收集，其他设计都可以找到蓝本。
> > 
>>[2025-06-29]-[陈银]-[**Question-Aware Gaussian Experts for Audio-Visual Question Answering]-CVPR2025
>>
>><img width="512" alt="image" src="https://github.com/user-attachments/assets/1b39738f-33ee-4b89-b7d1-6d2abf81d614" />
>>
>>论文简述：
>>本文提出了一个新的AVQA的框架：首先通过问题感知融合，在编码阶段将问题的预警注入到音频和视频特征之中，使得特征的处理更有针对性；其次，创新性的提出采用“混合高斯专家”模块，利用多个高斯分布作为“软掩码”，对音视频进行自适应的加权，动态聚焦最关键的时间片段。实验证明，该方法能更有效地处理复杂的时序关系，并在多个AVAQ数据集上取得了SOTA效果。
>
> > [2025-06-29]-[贾朋]-[SyncTalk++- High-Fidelity and Efficient Synchronized Talking Heads Synthesis Using Gaussian Splatting](https://arxiv.org/abs/2506.14742)
> > <img width="512" alt="image" src="https://raw.githubusercontent.com/2351548518/images/main/20250424/20250629225113227.png">
> >
> > 论文简述: 作者提出SyncTalk++：其动态肖像渲染器采用3DGS确保身份特征一致性；FaceSync控制器通过创新性运用3D面部 blendshape 模型，在唇语同步的同时精准重建面部表情；为达成自然头部运动，Head-Sync稳定器优化头部姿态提升稳定性。该方法保持跨帧视觉细节的一致性及连续性，并将渲染速度显著提升至101帧/秒。
> 
> > [2025-06-29]-[张宇]-[Merging Multi-Task Models via Weight-Ensembling Mixture of Experts]
> > 
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/MMTM.png">
> >
> > 论文简述：不同于端到端训练的MTL学习范式，属于MTL另一种比较火的学习范式（模型合并），目标是把多个针对不同任务独立微调过的模型合并成一个统一的多任务模型。传统的模型融合方法像“task arithmetic”那样，只是线性地在参数空间插值，容易在多个任务之间引发参数冲突。本篇工作的做法是：除了Transformer的Attention模块用现有方法简单合并外，把MLP模块替换成了MoE结构，用于动态地整合共享知识和任务特有知识，它保留了预训练模型的MLP参数，并用任务向量（fine-tune后的参数增量）表示每个任务的特征，再通过一个轻量级路由器（实验证明两层的比单层的好）根据输入动态地组合这些专家向量，能根据不同输入自动激活最合适的任务知识，可以减弱了任务间的干扰问题。
> > 
> > [2025-06-29]-[何艺超]-本周未精读
> > 
> > [2025-06-29]-[徐赟博]-[Weakly-supervised VLM-guided Partial Contrastive Learning for Visual Language Navigation]
> > 
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/WPCL.png">
> >
> > 论文简述：本文同样是通过VLM提取场景中的图像描述进行对比学习的工作，精读了一下借鉴思路。VLN中使用的视觉编码器CLIP/ResNet对于视角的变化非常敏感，作者提出对于同一个viewpoint不同视角的图像进行对比学习提升agent的视角变化的敏感性。作者使用VLM主要是用于提取Object来为不同视角的图像标记为正负样本。使用LLava对每一个视角抽取object，然后通过判定每个视角中存在的共享的object的数量被判定为正样本还是负样本，然后统一进行对比学习。思路比较简单，但略有相似。
> > 
>> [2025-06-29]-[陈银]-[AffectGPT: A New Dataset, Model, and Benchmark for Emotion Understanding  with Multimodal Large Language Models]-[ICML2025](https://arxiv.org/pdf/2501.16566)
> > <img width="512" alt="image" src="https://raw.githubusercontent.com/cyinen/imge/master/20250629133148.png">
> >
>>论文简述：本文章提出了一个新的数据集标注策略，并标注了一个新的多模态情感描述数据集：Mer-Caption+。该数据集是在MER2024的数据集上，通过low-level筛选和模型众包的方式，筛选出了31K比较精细的数据。并在此基础上，训练了一个AffectGPT，在多个情感分析的任务上都取得了SOTA性能。
>>
>> 
> >[2025-06-22]-[戴逦]-本周未精读
>
> >[2025-06-22]-[于洋晨]-本周未精读
> 
> >[2025-06-22]-[张雪松]-调试SPOC代码，略读相关文章，本周未精读
> >
> >[2025-06-22]-[何艺超]-推进比赛，本周未精读
> >
> >[2025-06-22]-[张宇]-[DLM-VMTL:A Double Layer Mapper for heterogeneous data video Multi-task prompt learning]-[arxiv](https://arxiv.org/abs/2408.16195)
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/DLM.png">
> >
> > 论文简述: 该工作针对视频理解领域中异构数据集上的多任务学习难题，通过DLM-VMTL 的双层 Prompt 映射器：第一层用 Transformer 的自注意力拿辅助任务中间层的特征“抠”出一堆跨任务的 Prompt，第二层再用轻量级的 Adapter 把这些 Prompt 投射到主任务的特征空间里，不同于传统的HPS和SPS，也区别于需要为每个数据集训练独立路由器并以Top-K策略激活专家的方案，这种灵活的共享方式既高效又稳定，不会互相抢资源，也不怕在新任务上忘掉旧任务的能力
> 
> > [2025-06-22]-[徐赟博]-本周未精读
> >
> >   [2025-06-22]-[贾朋]-[GaussianAvatars- Photorealistic Head Avatars with Rigged 3D Gaussians](https://arxiv.org/abs/2312.02069)
> > <img width="512" alt="image" src="https://raw.githubusercontent.com/2351548518/images/main/20250424/20250621214458246.png">
> >
> > 论文简述: 3DGS Head,基于3DGS的动态三维表征技术，通过将其绑定到FLAME模型上实现。这种组合既能保证超写实渲染效果，又能依托FLAME模型实现精确动画控制，例如通过驱动序列进行表情迁移，或手动调节FLAME参数。作者采用三角面片局部坐标系参数化每个3DGS点，并通过显式位移偏移优化以获得更精确的几何表征。在新视角合成指标上大幅领先其他方法。
> 
> >[2025-06-22]-[陈银]-[《Dynamic Curriculum for Imbalanced Multimodal Learning》](https://arxiv.org/pdf/2503.06456)
> ><img width="512" alt="image" src="https://raw.githubusercontent.com/cyinen/imge/master/20250622125111.png">
> >
>>论文简述：本文提出了一种名为DynCIM的动态课程学习框架，用于解决多模态学习中样本和模态不平衡的问题。DynCIM通过样本级和模态级的动态课程学习评估样本难度和模态贡献，并引入模态门控机制动态调整模态权重，从而优化多模态融合效果。在多个基准数据集上的实验结果表明，DynCIM能够有效提升多模态学习的性能，优于现有的最先进方法。
> 
> > [2025-06-15]-[贾朋]-本周未精读
>
> > [2025-06-15]-[戴逦]-本周未精读
>
> > [2025-06-15]-[于洋晨]-本周未精读
> 
> >  [2025-06-15]-[徐赟博]-[Cross from Left to Right Brain: Adaptive Text Dreamer for Vision-and-Language Navigation](https://arxiv.org/pdf/2505.20897)-
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/ATD.png">
> >
> > 论文简述：本文提出了一种在VLN中直接微调大模型的方法。从原始的R2R数据集上收集了两种新语料，一种是对场景的描述，另一种是对导航动作做出判断的依据。然后在训练大模型的时候，分别以两个损失函数去让多模态大模型学习到关于导航场景的知识。定义了两种大模型的左右脑训练方式，左脑生成对场景的描述，右脑生成预测导航动作的思维链。在生成的过程中，大模型接受视觉和文本token作为输入，然后在左右脑两路分支并行训练。但插入了可学习的query在两个支路中间传递信息，因为最重要的部分是预测导航动作，所以设计的query是由右脑支路去query左脑场景描述支路的状态嵌入。贡献在于造了新概念，然后工作量很大程度体现在对原始R2R数据集造场景描述和导航思维链的新语料库
> >
> >  [2025-06-15]-[张雪松]-[P3Nav: A Unified Framework for Embodied Navigation Integrating Perception,Planning, and Prediction](https://arxiv.org/pdf/2503.18525)-
> > <img width="512" alt="image" src="https://github.com/user-attachments/assets/035295f5-4703-484e-bcda-c86a278f6187">
> >
> > 论文简述：本文聚焦（目标）导航和具身问答EQA两种任务协同，提出了一种P3Nav的导航框架，统一了agent的感知（perception），规划（planning）和预测（prediction）能力。其中，P3Nav采用一种自适应的历史采样器高效地处理3D的历史发现。最后使用LLM输出action head和答案（LLM是OpenFlamingo，一种开源的自回归大视觉语言模型，仅训练cross-attention部分）。在CHORES-S的目标导航任务上达到了sota水平。实验部分表明，本文的方法仅仅通过EQA任务增强P3Nav agent对环境的感知，并未在EQA数据集上进行实验。
> > 
> > [2025-06-15]-[何艺超]-推进比赛，本周未精读
> > 
> > [2025-06-15]-[张宇]-本周未精读
> > 
> >  [2025-06-15]-[陈银]-[Diagnosing and Re-learning for Balanced  Multimodal Learning](https://arxiv.org/pdf/2407.09705)-ECCV2024
> > <img width="512" alt="image" src="https://github.com/user-attachments/assets/88fe5c06-564d-4d85-9984-5ed5ba972e16" />
> >
> > 论文简述: 在多模态学习中的不平衡问题中现有方法往往忽略了模态本身的内在信息量和噪声水平，导致在处理信息量少且噪声多的模态时效果不佳。本文通过评估每个模态的单模态表示空间的可分离性来诊断其学习状态，并据此对编码器进行软重新初始化：对学习良好的模态进行较大幅度的重新初始化以减少模型对其依赖，同时避免其过拟合；对学习不足的模态进行轻微重新初始化以增强其学习能力并避免记住噪声。实验表明，该方法在多个数据集和多模态框架上均优于现有方法，能够有效提升多模态模型的整体性能和单模态表示质量，同时在信息量少的模态情况下表现出色
> 
> > [2025-06-08]-[戴逦]-本周未精读
>
> > [2025-06-08]-[何艺超]-推进比赛，本周未精读
>
> > [2025-06-08]-[张宇]-[An Efficient General-Purpose Modular Vision Model via Multi-Task Heterogeneous Training](https://arxiv.org/abs/2306.17165)
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/MTHL.png">
> >
> > 论文简述: 该工作在ImageNet、COCO和ADE20K等异构数据集上同时训练一个模型，实现分类、检测和分割三大任务。本质上的做法感觉没有太多新点，还是用多个专家模块替换Transformer中的MLP层，但是不同于DAMEX那篇工作，它是为每个数据集配备独立路由器，通过Top-K策略在前向时只激活最相关的专家，不同于hard level的专家数据集互相约束，而是引入基于动量缓冲区的互信息损失来鼓励专家专注于特定数据集。主吹的是在连续学习场景下，仅需为新任务添加少量新专家与路由器，无需更新旧专家即可避免灾难性遗忘。
>
> >[2025-06-08]-[于洋晨]-本周未精读
> 
> > [2025-06-08]-[贾朋]-[DEGSTalk: Decomposed Per-Embedding Gaussian Fields for Hair-Preserving Talking Face Synthesis](https://arxiv.org/pdf/2412.20148)
> > <img width="512" alt="image" src="https://raw.githubusercontent.com/2351548518/images/main/20250424/20250603155557058.png">
> >
> > 论文简述: 3DGS Talking Head,作者提出3DGS的分解预嵌入高斯场（DEGSTalk）说话人脸合成方法，用于生成更具真实感的长发说话人脸。采用预测的3DMM系数作为中间面部表征调整高斯基元，从而精确捕捉动态面部区域与微妙表情。通过对可变形高斯场进行微调以融合面部和头发区域，随后执行渲染，解决了在对话人脸合成中准确重建长发个体的挑战。
> 
> > [2025-06-08]-[徐赟博]-[EvolveNav： Self-Improving Embodied Reasoning for LLM-Based Vision-Language Navigation][Arxiv]
> > 
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/EvolveNav.png">
> > 
> > 论文简述：本文提出了一种使用思维链微调来促进大模型在导航表现的方法。方法分为两阶段训练，分别是第一阶段思维链监督微调和二阶段自反思后训练（post-training）。首先，在第一阶段中，构建COT模板，“我需要到达一个地方具有<物品>在<方向>”。其中物品和方向分别通过caption模型和映射得到。然后，LLM输出的包含思维链的output需要和构建的CoT模板对齐。第二阶段中，为了防止模型过拟合于固定的思维链格式，这里让模型去选择CoT的正误，除了一个GT的COT外，还会给出一个类似模板，但关键细节错误的COT。这个工作是比较新颖的在导航领域使用思维链微调的工作，后续可以借鉴 
> >
> > [2025-06-08]-[张雪松]-本周未精读，阅读和复现[SPOC](https://spoc-robot.github.io/)论文代码
> >
>> [2025-06-08]-[陈银]-[Breaking Performance Shackles of Full Fine-Tuning on Visual Recognition Tasks][CVPR 2025]
>>
>> 论文简述：在大多数的视觉任务中，PEFT的方法（Adapter, Lora等）的性能都难以超过full fine-tuning。本文设计了一种多感知视觉adapter (Mona), 利用conv替代liner, 并使用多个conv实现多层感知，首次在视觉分割，实例分割等复杂的视觉感知任务上超过了全量参数微调的方法。
>> 
>> <img width="512" alt="image" src="https://github.com/user-attachments/assets/71026772-790e-4176-b166-9fed01c21333" />
> 
> > [2025-05-30]-[陈银]-本周未精读
> 
> > [2025-05-25]-[于洋晨]-本周未精读
> 
> > [2025-05-25]-[戴逦]-本周未精读
>
> >   [2025-05-25]-[贾朋]-[DEEPTalk: Dynamic Emotion Embedding for Probabilistic Speech-Driven 3D Face Animation](https://arxiv.org/pdf/2408.06010)
> > <img width="512" alt="image" src="https://raw.githubusercontent.com/2351548518/images/main/20250424/20250525204158226.png">
> >
> > 论文简述: 3D Talking Head,作者提出一种直接从语音输入生成多样化、富含情感3D面部表情的方法。首先训练动态情感嵌入模型DEE，该模型采用概率对比学习构建语音与面部动作的联合情感嵌入空间，该概率框架能捕捉情感解析过程中的不确定性。为生成动态面部运动，设计时序分层VQ-VAE作为运动先验，克服了传统VAE和VQ-VAE的缺陷。基于这些强先验，开发出非自回归预测码本索引的Talking Head生成器DEEPTalk。
> 
> > [2025-05-25]-[何艺超]-本周未精读
> > 
> > [2025-05-25]-[张宇]-[DAMEX: Dataset-aware Mixture-of-Experts for visual understanding of mixture-of-datasets]-[NeurIPS23](https://neurips.cc/media/neurips-2023/Slides/71495.pdf)
>>
>> <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/DAMEX.png">
>> 
>>论文简述：本文在异构数据集上利用隐含的通用知识进行合理的共享，异构数据集在域，标注方式等有很大不同，因此梯度冲突比同构数据集更大，为了合理利用共享信息，本篇工作设计了数据集的专家替代原始dense fnn模块，专家的架构是原始的FNN直接复制过来的，针对某个数据样本的token可以根据router灵活的选择某个专家，为了让专家更具数据特色，通过专家和数据集的映射关系直接利用损失反映到梯度更新上，但是不同的数据集还是共用一个router，或许可以考虑不同数据集维护自己那份独立的router。
>>
> > [2025-05-25]-[张雪松]-[SPOC: Imitating Shortest Paths in Simulation Enables Effective Navigation and Manipulation in the Real World](https://spoc-robot.github.io/)
>>
>> <img width="512" alt="image" src="https://github.com/user-attachments/assets/22e9a693-dd9c-4963-8e50-58722c8b0818">
>> 
>>论文简述：本文提出了一种名为CHORES的benchmark，它涉及在AI2-THOR仿真环境下，的同时执行导航和操作的十个任务。本文还提出了一种基于transformer的SPOC方法，模仿最短路径的模仿学习方式训练agent，使用了DINOv2和SigLIP等技术，不包括LLM。输入仅仅包含RGB图像，不包括深度和GPS定位。目前本文的局限性在于训练代价大（384 GPU(A100) hours），基于合成的图像而没有应用到实际的场景
>>
>> [2025-05-25]-[陈银]-[EMOE: Modality-Specific Enhanced Dynamic Emotion Experts]
>>
>> <img width="512" alt="image" src="https://raw.githubusercontent.com/cyinen/imge/master/20250517112044.png">
> >论文简述：本文提出了一种新模型，用于解决多模态情感识别（MER）中的模态平衡困境和模态特异性消失问题。EMOE通过Mixture of Modality Experts动态调整样本的模态权重，并引入路由熵损失优化模态平衡；通过Unimodal Distillation保留单一模态预测能力，指导多模态特征学习。在CMU-MOSI、CMU-MOSEI和MIntRec数据集上的实验表明，EMOE在准确率和F1分数上优于或媲美现有方法，并成功扩展到多模态意图识别等任务
>
> > [2025-05-23]-[徐赟博]-[Dynam3D: Dynamic Layered 3D Tokens Empower VLM for Vision-and-Language Navigation]
> >
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/Dynam3D.png">
> > 
> > 论文简述：本文讨论了一种3D视觉表征用于视觉语言导航。作者提出了一种‘Patch-Instance-Zone’的分层3D表征。Patch是CLIP提取的2D patch特征通过多视角图像投射成3D Patch特征，Instance是2D的语义分割模型的结果，通过一个需要学习的预测器，预测2D和3D对应区域内Instance分割结果是否匹配。Zone指的是3D坐标里均匀分布的一个空间块，使用一个Zone Encoder聚合一个Zone区域内所有的Instance特征作为Zone Token。然后，作者把这种代表不同粒度的3D token（patch/instance/zone token）共同输入到一个3D VLM进行训练。在单目setting的数据集上进行了测试，多个数据集都有明显的性能提升。这篇工作作者应该是聚合了一些3D检测/分割的工作，比较陌生；但在单目任务的setting下使用多视角图像建模三维环境表征有点不理解这是在干啥。
> > 
> > [2025-05-18]-[于洋晨]-本周未精读
> 
> > [2025-05-18]-[戴逦]-本周未精读
>
> >   [2025-05-18]-[贾朋]-[EmoFace- Emotion-Content Disentangled Speech-Driven 3D Talking Face Animation](https://arxiv.org/pdf/2408.11518v2)
> > <img width="512" alt="image" src="https://raw.githubusercontent.com/2351548518/images/main/20250424/20250518191051318.png">
> >
> > 论文简述: 3D Talking Head,作者提出双流神经网络EmoFace，包含情感分支与内容分支，创新性采用网格注意力机制，通过新型时空图卷积网络SpiralConv3D分析并融合情感与内容特征，学习网格顶点间的时空特征依赖关系。首次在3D面部动画任务中引入具有中间监督的自增长训练策略，动态调整真实数据采用比例。在3D-RAVDESS数据集上，本方法的LVE与EVE分别比SelfTalk低20%与35%。
> 
> > [2025-05-18]-[何艺超]-[Emotion-Qwen: Training Hybrid Experts for Unified Emotion and General Vision-Language Understanding](https://github.com/ReadingPapers/Report/blob/main/Images/Emotion-Qwen.png)
> >
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/Emotion-Qwen.png">
> >
> >论文简述：Emotion-llama团队的第二个情感大模型，从让模型具备情感推理能力的同时，通用视觉任务的能力也要具备，仅支持视频和文本输入，使用通用数据集进行预训练，情感数据集指令微调的方法，模型架构上：给peojector换成了MoE以处理一般视频任务和情感任务，并通过DeepFace开源工具获取了情感高相关的视频帧，让这个情感高度相关的帧与视频特征融合。LLM用的是qwen2.5的7B模型，使用3张A800-80G进行训练。
> >
> > [2025-05-18]-[张雪松]-未找到合适论文，本周未精读
> > 
> > [2025-05-18]-[徐赟博]-本周未精读
> > 
>>[2025-05-18]-[张宇]-[Mod-Squad: Designing Mixtures of Experts As Modular Multi-Task Learners]-[cvpr23](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Mod-Squad_Designing_Mixtures_of_Experts_As_Modular_Multi-Task_Learners_CVPR_2023_paper.pdf)
> 
>> <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/Mod.png">
>>
>> 论文简述：属于从模型方面缓解mtl中的参数竞争和梯度冲突，更合理的共享。做法是将ViT结构中的attn和FFN模块划分为多个专家，也就是子空间的专家，划分隐藏维度，并规定哪些专家是共享专家，哪些是任务特定的专家，并利用专家和任务的互信息，使特定于任务的专家更加的专一，保证平衡激活频率的同时，又不会导致dense expert的出现，多任务模型训练完成后，通过“剪支”丢弃与当前任务不相关其他所有专家，使其成单任务模型，保证推理速度的同时不会降低其性能。
>
>>[2025-05-17]-[陈银]-[R1-Omni Explainable Omni-Multimodal EmotionRecognition with Reinforcement Learning]
> 
>> <img width="512" alt="image" src="https://github.com/user-attachments/assets/a521efb2-04a8-4f14-a8ce-4e8241322df2">
>>
>> 论文简述：阿里通义千问实验室的一篇论文，首次将强化学习 RLVR 方法结合 GRPO 用于多模态的情绪识别，强化了模型的推理能力，展现出了与有监督训练的模型相当的性能，特别是在在分布外数据集评估中展现出卓越的鲁棒性。之前是CoT 火了一段时间，现在RLVR+GRPO在视觉多模态推理上也开始火了起来，应该会是一个主流的方向，值得关注。
>
> > [2025-05-11]-[张雪松]-[SG-Nav: Online 3D Scene Graph Prompting for LLM-based Zero-shot Object Navigation][NeurIPS2024]
> > 
> > <img width="512" alt="image" src="https://github.com/user-attachments/assets/73adb57b-a252-4f27-80ba-8fdc70057206">
> >
> > 论文简述: 本文是UniGoal的前身(CVPR2025)，开源性较好。该论文提出了一种新颖的3D场景图提示框架，用于在无需下游训练的情况下提升大语言模型（LLM）在零样本目标导航任务中的表现。作者通过构建紧凑、语义丰富的3D场景图并将其转化为语言提示，引导LLM完成导航决策。同时，论文设计了结构化搜索策略以生成高质量提示，并利用多模态感知信息对目标进行显著性建模。实验结果在Gibson和MP3D两个基准上均显著优于现有方法。

> > [2025-05-11]-程浩-[Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation]-[Deepseek Technical Report](https://arxiv.org/abs/2410.13848) \
> >![image](https://github.com/user-attachments/assets/bec6208c-96fc-418b-b219-72304c5041d4) \
> > 论文简述：在现有的统一MLLM中，如Chameleon，采用llava架构，并对视觉理解和生成任务采用了相同的encoder，但作者认为这两种任务实际上不一样：理解需要抽象的高级语义表征，而生成则需要具体详细的信息。所以在统一模型的基础上，作者解耦了多模态理解和生成框架的vision encoder，Gen.encoder采用VQ形式，Und.encoder采用SigLIP（可理解为advance CLIP）。

> > [2025-05-11]-[张宇]-本周未精读
> > 
> > [2025-05-11]-[于洋晨]-[EMOE:Modality-Specific Enhanced Dynamic Emotion Experts][暂无出处]
> > 
> > <img width="512" alt="image" src="https://github.com/yyc-hfut/PaperReading/blob/main/EMOE.png">
> >
> > 论文简述: 本文提出EMOE结构，用于多模态情感识别以及多模态意图识别任务。如图所示，该结构首先使用模态混合专家系统，根据不同样本的特征动态调整模态的重要性。同时，本文认为传统模型忽略了单模态数据的预测能力，所以又提出使用贡献更大的单模态特征，蒸馏多模态特征，使得在融合特征中保留单模态的预测能力。最终，通过消融实验，验证了两个模块的有效性。但是，最终在CMU-MOSI以及CMU-MOSEI数据集上的效果不是特别理想，起码比之前看似简单的MMML模型的ACC7指标低了0.5个点（相较于MMML_context，低了2.5个点），其他指标也是差不多。不过，该工作提出的，保留单模态的预测能力以及样本级模态差异的问题，确实值得思考。
> 
> >   [2025-05-11]-[贾朋]-[Generalizable and Animatable Gaussian Head Avatar](https://github.com/xg-chu/GAGAvatar)
> > <img width="512" alt="image" src="https://raw.githubusercontent.com/2351548518/images/main/20250424/20250430211122196.png">
> >
> > 论文简述: 作者提出一种可泛化可驱动的Gaussian头部虚拟形象（GAGAvatar），用于单张照片的动画化头部重建。核心创新在于提出的双提升方法，该方法能生成高保真度的3D Gaussian，完整捕捉身份特征与面部细节，结合全局图像特征与3D形变模型来构建控制表情的3D Gaussian结构。训练完成后，模型无需针对性优化即可重建未见过的身份信息，并以实时速率完成驱动渲染。
> 
> > [2025-05-11]-[戴逦]-[Masked Autoencoders Are Scalable Vision Learners]-[CVPR 2022]
> > 
> > 论文简述：本文提出了一种自监督学习方法——MAE。该方法可以重建被遮挡的图片，并达到不错的效果。方法有两个核心设计，首先，作者设计了一种非对称的编码器-解码器架构，编码器仅处理可见的patches，这种设计使其训练速度大幅加快，解码器利用编码器学习到的潜在表示来重建不可见的patches。其次，作者通过实验发现，随机遮挡图片的比例设置为75%时，会得到比较好的重建效果。
>
> > [2025-05-11]-[何艺超]-本周未精读
> > 
> > [2025-05-11]-[陈银]-[R1-Omni: Explainable Omni-Multimodal Emotion
Recognition with Reinforcement Learning] 
> > 
> > <img width="512" alt="image" src="https://github.com/user-attachments/assets/b64225a2-b3f7-43ed-a6b3-027714b9761f" />
> > 
> > 论文简述: 本文首次将可验证奖励的强化学习（Reinforcement Learning with Verifiable Reward, RLVR）应用于全多模态大型语言模型，以提升情感识别任务的性能。该模型在推理能力、情感识别准确性和泛化能力方面都有显著提升，尤其是在处理分布外（OOD）数据集时表现出色。显示了强化学习在数据泛化能力方面的巨大潜力
> >
> > [2025-05-10]-[徐赟博]-[Landmark-RxR: Solving Vision-and-Language
 Navigation with Fine-Grained Alignment Supervision]-[Nips2021](https://proceedings.neurips.cc/paper/2021/file/0602940f23884f782058efac46f64b0f-Paper.pdf)
> > 
> ><img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/landmark.png">
> > 
> > 论文简述:本文在RXR数据集的基础上补充了细粒度的landmark，借鉴了此前fine-grined R2R的工作，将导航数据集拆解成子指令。这篇文章我更关注的是他如何使用细粒度的landmark去进行强化学习中的奖励的建模。首先，作者定义了一个轨迹关键点的含义，在导航轨迹过程中起关键作用的位置节点，这些节点可以反应导航过程的关键位置。而landmark天然体现了这一特性，作者利用导航过程中的landmark塑造reward。Soft Reward和Hard Reward两种。硬奖励是节点离关键点距离小于阈值就+1，反之-1；软奖励依然是根据和关键点的距离塑造的，计算公式是-exp{距离}。具有可借鉴的价值。
> >
> > [2025-05-07]-[张雪松]-五一劳动节放假，未精读
> 
> > [2025-04-27]-[于洋晨]-整理代码实验，大致搜索一些文章，无精读
>
> > [2025-04-27]-程浩-[Qwen2.5-Omni Technical Report]-[arxiv](https://arxiv.org/abs/2503.20215)
> > 
> >  <img width="512" alt="image" src="https://github.com/user-attachments/assets/e044400c-ac99-4a17-9624-f07686ae695f">
> >
> > 论文简述：利用音频encoder和视频encoder将多种模态编码到大模型的表征空间，并且利用Talker进行语言输出，构建了一个统一的多模态模型。其中TMRoPE与我之前思路一致。
> 
> > [2025-04-27]-[聂建涛]-本周未精读
> 
> > [2025-04-27]-[何艺超]-[HumanOmni: A Large Vision-Speech Language Model for Human-Centric Video Understanding](http://arxiv.org/abs/2501.15111)
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/HumanOmni.png">
> >
> > 论文简述:和R1 Omni以及Omni emotion来自于一个团队，这篇文章涉及的角度更广，把情感识别放在了以人为中心的视频理解角度考虑，构建了超大规模的数据集，使用qwen 2.5作为backbone ，和前面几个典型大模型工作的不同是设计了不同任务的projector然后把bert提取的文本用作指导三个不同视觉任务projector的合并权重生成。
> 
> > [2025-04-27]-[贾朋]-[MonoGaussianAvatar: Monocular Gaussian Point-based Head Avatar](https://arxiv.org/abs/2312.04558)
> > <img width="512" alt="image" src="https://raw.githubusercontent.com/2351548518/images/main/20250424/20250426211819060.png">
> >
> > 论文简述: 当前头部虚拟形象技术已运用3DMM、点云及神经隐式表示等最新研究成果，然而3DMM方法受限于固定拓扑结构，点云方法因海量点数量导致沉重训练负担，而后者则在形变灵活性与渲染效率方面存在局限。针对这些问题，作者提出MonoGaussianAvatar，一种融合3D高斯点表示与高斯形变场的新方法，可从单目肖像视频学习显式头部虚拟形象。该模型结合面部表情与姿态，能够从单目视频中重建精细的几何结构与外观，引入了高斯点插入与删除策略，并采用高斯变形场以保留配饰（如眼镜）在新姿态下的结构完整性。
> 
> >[2025-04-27]-[张雪松]-[InstructNav: Zero-shot System for Generic Instruction Navigation in Unexplored Environment]-[CoRL-2024](https://github.com/LYX0501/InstructNav?tab=readme-ov-file)
> >
> ><img width="512" alt="image" src="https://github.com/LYX0501/InstructNav/blob/main/InstructNav.png" />
> > 
> > 论文简述：本文的主要贡献在于: (1) 构建基于LLM的动态导航链，统一不同形式的导航指令（目标导航，视觉语言导航，命令驱动的导航）进行导航。（2）之前的方法构建单个（基于文本指令和视觉环境的语义相似度）value map进行导航，本文进一步整合了action，Intuition和轨迹等value map进行导航。（3）实验结果在HM3D object goal navigation，R2R-CE 和 DDN demand-driven navigation三个任务上取得了较好的结果。
> >
> > [2025-04-27]-[戴逦]-[Deep Residual Learning for Image Recognition]-[CVPR 2016]
> >
> >论文简述：对于随着网络深度加深会出现梯度消失的问题，作者提出使用残差连接的方式来解决这个问题。具体来说，假设任务所需要学习的函数为H(x)，现有的浅层网络的输出为x，那么在浅层网络上再加入的一些层应该去学习H(x)-x，新加入的网络层不需要重新学习，而是去学习已经学习到的东西与真实的东西之间的残差，最后整个网络的输出等价于浅层神经网络和新加入的神经网络学习到的残差的输出之和。
>
> > [2025-04-27]-[徐赟博]-本周没有找到比较感兴趣的论文，泛读了一些比较General的论文，没有精读
> > 
> >[2025-04-27]-[陈银]-[AVadCLIP: Audio-Visual Collaboration for Robust Video Anomaly Detection]-AAAI2025
> >
> ><img width="512" alt="image" src="https://raw.githubusercontent.com/cyinen/imge/master/20250427101243.png" />
> > 
> > 论文简述：本文提出了一种新颖的弱监督视频异常检测（WSVAD）框架 AVadCLIP，通过视听协作增强了监控视频中异常检测的鲁棒性和准确性。框架基于对比语言-图像预训练（CLIP）的跨模态能力，融合视觉和音频信息，解决了传统仅依赖视觉方法在复杂环境（如遮挡、低光照、视觉噪声）中误报率高的问题。AVadCLIP在多个基准数据集上表现出色，即使在推理阶段缺乏音频数据时仍能保持鲁棒性
>>
> > [2025-04-24]-[张宇]-[Mitigating Task Interference in Multi-Task Learning via Explicit Task Routing  with Non-Learnable Primitives]-[CVPR23](https://openaccess.thecvf.com/content/CVPR2023/papers/Ding_Mitigating_Task_Interference_in_Multi-Task_Learning_via_Explicit_Task_Routing_CVPR_2023_paper.pdf)
> >
> ><img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/ETR-NLP.png" />
> > 
> > 论文简述：针对MTL中不同任务之间梯度冲突导致性能下降的问题，也就是所谓的任务干扰，在目前研究如何缓解梯度冲突导致任务干扰的三个主流背景下（loss，参数分区，模型），以模型为入手点，提出了（1）显式任务路由（ETR）（2）非可学习原语（NLP）相结合的模块。其中NLP也就是一些不可学习的模块进行组合（例如Relu，pool，Conv (binary)）（没有可学习参数自然缓解了一些梯度冲突），而ETR就是通过在每层网络中并行提取多种任务无关的非可学习特征，再利用可调比例 γ 将可学习参数显式分为共享与任务专属两部分（γ=0单任务，γ=1硬共享），最终cat在一起，既能保留充分的信息共享，又能为各任务提供专属容量，最终在 CelebA、Cityscapes 和 NYU-v2 等进行实验，发现单个NLP只对单任务有帮助，MTL还是需要多个NLP组合，对于ETR，γ=0.9（都快属于hard sharing了）最好，而且这两个模块互补，并与现有硬共享、损失平衡、软共享及隐式划分等方法进行比较，性能有较大的提升。
> >
> > [2025-04-20]-程浩-[本周未精度]
>
> >  [2025-04-20]-[戴逦]-[Attention is All you Need]-[NIPS 2017]
> >
> >  论文简述：作者提出Transformer，一种完全基于注意力机制，彻底摒弃了循环和卷积结构的神经网络结构。它仅仅使用注意力机制来捕捉输入和输出间的全局依赖，显著提高了并行化计算的能力。Transformer采用传统的编码器-解码器结构，都由6个子层组成。编码器的子层由多头自注意力层和前馈网络层构成，解码器的子层由带有掩码机制的多头注意力层（即只学习t时刻的输入与t时刻之前时序的输入的关系，不学习与t之后时刻的输入的关系）和多头注意力层和前馈网络层构成。作者在最后还预测该结构可以应用在其他模态如图像、音频和视频模态上并取得效果。
>
>  >[2025-04-20]-[聂建涛]-本周未精读
> 
> >   [2025-04-20]-[贾朋]-[GaussianTalker: Real-Time High-Fidelity Talking Head Synthesis with Audio-Driven 3D Gaussian Splatting]- [ACM MM2024](https://arxiv.org/abs/2404.16012)
> >
> > <img width="512" alt="image" src="https://raw.githubusercontent.com/2351548518/images/main/20250120/20250420212614224.png">
> >
> > 论文简述: 作者提出GaussianTalker，通过构建头部的规范3DGS表示，并使其变形与音频同步。创新在于将3D高斯属性编码为共享隐式特征表示，在此与音频特征融合以操纵每个高斯属性，利用了空间感知特征并强化相邻点之间的交互作用。特征嵌入随后输入空间-音频注意力模块，该模块可逐帧预测每个高斯属性的偏移量。相较于以往用于操纵大量高斯及其复杂参数的方法，该方案更为稳定。
> 
> > [2025-04-20]-[张宇]-[Transforming Vision Transformer: Towards Efficient Multi-Task Asynchronous Learning]-[NeurIPS24](https://proceedings.neurips.cc/paper_files/paper/2024/hash/93fab021315170101c92e8330a56fbdb-Abstract-Conference.html)
> >
> ><img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/MoLOAR.png" />
> > 
> > 论文简述：本篇工作属于MoE+LoAR+MTO的工作。MTL中一直存在两个主要的研究目标：（1）以高效的MTL（2）多任务优化策略（MTO），但他们基本上要么是解决了复杂不紧凑的网络但忽视了任务之间同步优化会导致不同任务难度不同收敛不一致破坏任务的固有特性，要么是意识到了异步优化但无法做到参数集成或网络可调参数居多导致过拟合，从而推理时间和性能都会下降。考虑了不同任务难度不同不能强制同步优化会导致竞争干扰，以及共享的backbone无法为每个任务提取代表的特征。具体怎么做的：（1）模型层面：重新排列预训练后模型的FNN的权重矩阵并使用K-means聚类方法将其分解为权重类似的多个矩阵，定义为低级专家，这些低级专家是低秩的信息含量不满，所以可以应用LoAR的微调策略进行训练（2）优化策略方面：使用质量保留优化策略将已经学习好的任务的权重进行保留并不会对其破坏，做到任务间异步优化，并采用Router Fading策略避免早期模型对某些专家过度依赖，最后参数集成的方法在返回到预训练的ViT,减少额外参数量并加快推理速度。实验从性能，参数量，以及推理速度都很好，但没有对比单任务模型。目前代码还没开源。
> >
> >[2025-04-20]-[于洋晨]-[本周无精读]
>
> >[2025-04-20]-[何艺超]-[To Err Like Human: Affective Bias-Inspired Measures for Visual Emotion Recognition Evaluation]-Nips2024
> >
> ><img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/to_err_like_human.png" />
> > 
> > 论文简述：传统的分类任务通常是通过准确率来衡量模型的好坏，不同类别在分类的评价标准中占据相同的重要性；然而针对情感任务，作者辩证地提出也要衡量分类错误的类别，因为在情感分类的任务中，由于情绪之间的心理相似性，将某种情绪分为一个类别可能比另一个结果更为严重，例如作者提出，将“兴奋”错误地分类为“愤怒”显然比“敬畏”更为严重。于是作者利用情感轮来定义情感距离，宏观上把情感氛围积极和消极，在具体的类别中，在损失函数上定义全局的衡量标准ECC，以及分类错误的类别EMC，并通过半监督伪标签的子任务，证明了其方法更加适用于伪标签的生成，也即说明了其损失函数的有效性。
> >
>[2025-04-19]-[陈银]-[Audio-Visual Adaptive Fusion Network for Question Answering Based on Contrastive Learning]-AAAI2025
> >
> ><img width="512" alt="image" src="https://raw.githubusercontent.com/cyinen/imge/master/20250419150539.png" />
> > 
> > 论文简述：本文提出了一种基于对比学习的音频-视觉自适应融合网络（AVAF-Net），用于解决音频-视觉问答（AVQA）任务中的时间-空间维度对齐和跨模态信息融合问题。通过时间对齐对比学习（TACL）和空间对齐对比学习（SACL），AVAF-Net有效对齐了视觉和音频信息，并利用问题导向的自适应融合（QOAF）模块动态调整融合权重，提高答案预测的准确性。在MUSIC-AVQA数据集上的实验表明，AVAF-Net在平均准确率上超越了所有基线模型，取得了显著的性能提升。
>> 
>> >[2025-04-17]-[张雪松]-[VL-Nav: Real-time Vision-Language Navigation with Spatial Reasoning]-[paper](https://sairlab.org/vlnav/)
> >
> ><img width="512" alt="image" src="https://github.com/user-attachments/assets/6b608222-006f-423e-a0b1-cb257d596ee6" />
> > 
> > 论文简述：本文主要对VLFM进行了改进，具体包括（1）使用yolo-world代替blip-2计算图象与指令（object）之间的语义相似性（value），好处是更加轻量化且能够细粒度地计算像素级别的语义值。（2）提出了Instance-Based Target Points (IBTP)，这种基于实例的方法模仿了人类在搜索对象时的行为：一瞥到可能与目标匹配的东西，人们就会自然地靠近以确认。（3）提出了两种好奇心的计算策略（基于距离和探索过的区域数量）。实验结果仅展现了部分object类别上的结果，显著超过了vlfm，但是没有在整个数据集（验证集）进行公平比较。
>> 
> >[2025-04-16]-[徐赟博]-[ST-Booster: An Iterative SpatioTemporal Perception Booster for
 Vision-and-Language Navigation in Continuous Environments]-[paper](https://arxiv.org/pdf/2504.09843)
> >
> ><img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/VLN-ST-Booster.png" />
> > 
> > 论文简述：本文提出了一种引入侧重于Local 感知的Grid Memeory Map辅助连续条件下的导航。如今的CE导航任务普遍采用的框架是使用全局的拓扑图进行导航。然而这种方式虽然对导航过程中的长程关系具有很好的建模能力，但是缺乏细粒度的感知能力。GridMemoryMap能够通过动态的构造Grid存储特定位置里的视觉特征，进而提供一种和位置以及历史相关细粒度的视觉感知。作者把GridMM作为导航的另一支路，并设计了Node2Cell和Cell2Node两个模块实现拓扑图中的节点和GridMM中的cell映射到同一个嵌入空间进行对齐。并且，又把导航过程中的拓扑图特征和GridMM特征用于预测Waypoint中。
> >
> >[2025-04-14]-[聂建涛]-补上周：由于赶论文，无精读
> > [2025-04-14]-[戴逦]-[Tokens-to-Token ViT: Training Vision Transformers From Scratch on ImageNet]-[https://openaccess.thecvf.com/content/ICCV2021/html/Yuan_Tokens-to-Token_ViT_Training_Vision_Transformers_From_Scratch_on_ImageNet_ICCV_2021_paper.html?ref=https://githubhelp.com]
> >
> > 论文简述：作者提出T2T-ViT,一种在中等数据集上训练效果接近CNN模型的ViT模型。该模型的创新点在于逐步的Tokenization和采用deep-narrow结构的骨干网络。逐步Tokenization的每一次处理过程包含重构和软切分(带有重叠的patch划分)两个步骤,通过反复执行上述“重构+软切分”的过程,可以逐步压缩图像token的长度和在token中融合局部结构信息。作者探索了多种CNN中的结构设计,如DenseNet中的密集连接结构、wide-ResNet中的"深窄 vs 浅密”结构、ResNeXt中的多头变种结构等,通过实验发现,采用deep-narrow结构效果最好。
> >
> >  [2025-04-14]-[张雪松]-[VLFM: Vision-Language Frontier Maps for Zero-Shot Semantic Navigation][2024 ICRA BSET PAPER](https://github.com/bdaiinstitute/vlfm)
> > 
> > <img width="512" alt="image" src="https://github.com/user-attachments/assets/e30d272b-b71b-4a04-a179-e6c6c2ef68b0">
> > 
> > 论文简述: 本文提出了一种零样本导航方法——视觉-语言边界地图（VLFM），该方法受人类推理过程的启发，旨在引导机器人从未见过的语义目标导航到新环境中。VLFM 利用深度观测构建占用地图以识别边界，并利用 RGB 观测和预训练的视觉-语言模型生成基于语言的价值地图。然后，VLFM 使用该地图来确定最有希望探索的边界，以找到给定目标对象类别的实例。在 Habitat 模拟器中的 Gibson、Habitat-Matterport 3D (HM3D) 和 Matterport 3D (MP3D) 数据集的逼真环境中评估了 VLFM。值得注意的是，VLFM 在所有三个数据集上的目标导向导航任务中均取得了最先进的性能（以路径长度加权的成功率衡量）。此外，实验证明了 VLFM 的零样本特性使其能够轻松部署到现实世界中的机器人上，例如波士顿动力公司的 Spot 移动操作平台。并展示了其在现实世界中无需任何环境先验知识的情况下，高效导航到办公室建筑内目标对象的能力。VLFM 的成就凸显了视觉-语言模型在推进语义导航领域方面的巨大潜力。
> 
> > [2025-04-14]-[于洋晨]-[A Dual Contrastive Learning Framework for Enhanced Multimodal Conversational Emotion Recognition][2025 Coling](https://aclanthology.org/2025.coling-main.272/)
> > 
> > <img width="512" alt="image" src="https://github.com/yyc-hfut/PaperReading/blob/main/DCLF.jpg">
> >
> > 论文简述: 本文提出了一种双对比学习框架（DCLF）。通过构建对话历史与当前目标utterance的正负样本（负样本为同一情感标签下的随机语句， 消除情感标签的影响）从而使得模型能够更加敏感地捕捉上下文语义。其次，本文提到，如果每个模态地正确预测全部保留，那最终地预测效果能达到W-f1的得分为81.7（大模型sota约为71），所以本文构建了一种模态贡献对比学习方法，以捕捉不同模态在识别倾向上地相关性和差异性。总结来说，其实很多思路都一样，对于文本模态来说，上下文极其重要，而音视频却不同，如何更加准确地理解文本上下文是提升模型性能的一种思路。然后就是，模态贡献的差异性，诸多任务都凸显了分离和理解各个模态贡献的重要性。
> 
> > [2025-04-13]-[徐赟博]-[Unseen from Seen: Rewriting Observation-Instruction Using Foundation Models for Augmenting Vision-Language Navigation]
> >
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/VLN-RAM.png" />
> >
> > 论文简述：本文提供了一种基于指令重写对VLN数据进行增强的方法。原始的VLN数据是人工注释的，只是对导航轨迹的解释。缺乏细节。本文提出对于导航场景，使用VLM提取场景描述，再使用LLM为原始的VLN指令数据添加新的Object和Layout信息形成全新的对导航轨迹的文本场景描述。接着，结合原始的场景描述和增强的场景描述，使用LLM对指令进行改写。最后，引入了新的混合训练策略，混合原始数据和增强数据，减少生成数据可能存在的噪声或者幻觉对结果的影响。
> > 
> >   [2025-04-13]-[贾朋]-[EmoTalkingGaussian: Continuous Emotion-conditioned Talking Head Synthesis](https://arxiv.org/abs/2502.00654)
> > <img width="512" alt="image" src="https://raw.githubusercontent.com/2351548518/images/main/20250120/20250408162826785.png">
> >
> > 论文简述: 基于3DGS的说话头合成方法通常仅在缺乏面部表情多样性的短视频上进行训练，生成的说话头难以表现丰富的情感变化，作者提出EmoTalkingGaussian，该模型能够基于连续情感数值（即效价和唤醒度）调控面部表情，同时保持唇部运动与输入音频的同步性。首先采用TalkingGaussian框架，基于音频和动作单元合成三维高斯泼溅式说话头部；随后使用EmoStyle生成唇形同步的情感面部图像，通过多样化情感面部数据训练EmoTalkingGaussian的情感操控模块。
> 
> >  [2025-04-13]-[张宇]-[Multi-scale Promoted Self-adjusting Correlation Learning for Facial Action Unit Detection]-[TAFFC24](https://arxiv.org/pdf/2308.07770)
> >
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/sacl.png" />
> >
> > 论文简述: 现有方法在au相关性建模方面存在固定位置先验的依赖，此工作也是想依赖au之间的位置先验，但是又不能使其固定不变缺乏泛化性，所以基于此出发点，提出了一种名为自调整au相关学习的方法。具体做法：利用固定位置先验作为基准通过网络学习自适应更新au相关图结构，并通过整合不同阶段的特征叠加来丰富感受野，提升au检测性能。该方法在参数量和计算量仅为最优方法的28.7%和12.0%的情况下，在BP4D和DISFA数据集上的性能优于现有方法（但是性能还是太低分别只有65.6 65.5的f1分数）。而且此方法太过依赖面部的关键点检测定位au的区域，如果关键点检测不准确，会拉低对后面的AU检测和相关性学习。
> 
> > [2025-04-13]-[陈银]-[Cacophony: An Improved Contrastive Audio-Text Model] TASLP
> >
> ><img width="512" alt="image" src="https://github.com/user-attachments/assets/e85c1106-dc11-468a-a092-24288a697746" />
>>
> > 论文简述: 音频-文本模型在规模和性能上落后于图像-文本模型（如CLIP）。现有音频-文本数据集规模有限（最大约10万对），且标签质量参差不齐（干净标签、噪声标签、弱标签/无标签）。本文提出通过扩大数据集规模和优化训练流程，提升对比式音频-文本模型的性能。本文构建了一个包含13000小时音频的音频-文本数据集（390万对），通过两阶段的训练策略，包括音频编码器的自监督预训练和对别-描述的训练，增强模型的表征能力。
>
> > [2025-04-13]-[何艺超]-[R1-Omni: Explainable Omni-Multimodal Emotion Recognition with Reinforcement Learning](https://arxiv.org/abs/2503.05379)
> >
> > 论文简述: 3月10号第二版刚挂在arxiv的工作，这篇工作首次将具有可验证奖励的强化学习（RLVR）应用于情绪识别的多模态大型语言模型，RLVR的思路来源于deepseek，在情感的子任务上可以让即使是没有详细数据的训练数据【如DFEW，MAFW只有category的标签】，也可以让模型根据这些训练数据获得推理的能力，但是从实验结果来看，分类的准确率和目前主流方法的准确率（如视觉指令微调的大模型，还有传统的分类方法）比差距还是很大。但是和EMER进行SFT的方法比要好一些，这里的原因还是EMER数据集的322条数据太少了，即使是指令微调了，也不会调的很彻底，总的来说是给了一个大模型做情感识别的新的方式方法：即通过没有描述的数据让大模型具备推理到情感标签的能力。
> 
> > [2025-04-08]-程浩-补上周：由于赶论文deadline，无精读
>
> > [2025-04-07]-[贾朋]-[InsTaG: Learning Personalized 3D Talking Head from Few-Second Video](https://arxiv.org/abs/2502.20387)
> > <img width="512" alt="image" src="https://raw.githubusercontent.com/2351548518/images/main/20250120/20250407135714879.png">
> >
> > 论文简述: 作者提出InsTaG，基于轻量级3D高斯泼溅人物特定合成器与通用运动先验，在保持高度个性化和高效的同时，实现了高质量快速适配。大多数NeRF类方法采用针对特定人物的训练，给定包含目标说话者的视频片段，整个针对特定人物的模型会被训练以记忆目标说话头部，对未见过的身份不具备泛化能力，需要大量高质量视频帧和长时间训练来适应每个新身份。作者通过预训练身份无关的3D动作场作为先验知识并将其与新身份对齐以加速适配，提出了基于高斯泼溅的即时说话头像合成框架InsTaG。
> 
> >[2025-04-06]-[于洋晨]-[因实验与假期，本周无精读]
> 
> > [2025-04-06]-[聂建涛]-[An Efficient Attribute-Preserving Framework for Face Swapping]-[TMM2024](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10400952)
> >
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/An%20Efficient%20Attribute-Preserving%20Framework%20for%20Face%20Swapping.png">
> >
> >论文简述：本文提出了一种高效的属性保持框架AP-Swap，用于面部交换，能够在保持身份一致的同时，有效保留目标面部的关键属性如头部姿势、表情和视线方向。方法包含两个创新模块，专门用于保留关键的面部属性。首先通过全局残余属性保留编码器（GRAPE）自适应地从目标人脸中提取全局完整的属性特征；其次，除了源面部图像和目标面部图像的常规网络流之外，引入了一个考虑到目标面部关键点的网络流。这个额外的网络流支持我们的面部关键点引导特征纠缠模块（LFEM），该模块通过执行基于地标的属性保留（LBAP）操作，有效地保留了细粒度的面部属性。
> >
> > [2025-04-06]-[徐赟博]-[Lookahead Exploration with Neural Radiance Representation for Continuous Vision-Language Navigation]-[CVPR2024](https://arxiv.org/pdf/2404.01943)
> >
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/HNR.png">
> >
> >论文简述：本文提出了一种基于Nerf合成未来视图的方法进行连续环境下的视觉语言导航的方法。首先，从多视角图像中抽取Feature Cloud 类似于稀疏点云。然后在连续导航的统一设定下，在每个导航轮次通过特征云再通过Nerf的可学习MLP建立对整个导航场景的三维隐式表示，合成关于未来视角的图像。包括多种特征，分别是未来视图的Nerf特征，和未来视图的渲染真实图像。分别将未来视图的Nerf特征和未来视图的GT CLIP特征做交叉熵损失和未来视图的渲染图像和未来视图的GT 图像进行交叉熵损失来建立对3D场景信息的保存。在推理过程中，对可选择的可导航点的未来图像进行预测和打分，选择最优打分的导航点作为下一步。但是该工作代码实现和论文内容不太一致，而且基于Nerf的方法需要太长的时间代价，4卡3090训练一个多星期，不太值得Follow。
> >
> > [2025-04-6]-[张宇]-[Facial Action Unit Detection by Adaptively Constraining Self-Attention and Causally Deconfounding Sample][IJCV 2024](https://arxiv.org/abs/2212.10071)
> >
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/ACD.png">
> > 
> > 论文简述:现有AU检测方法大多直接使用自注意力机制空间建模依赖，但容易受到伪相关性的干扰。所以本文从注意力约束和因果先验去混淆两个角度入手，提升 AU 检测性能。首先空间约束自注意（ACS），在原有自注意力结构上加入两类约束（先验和语义）：先验约束通过 AU 区域信息指导注意力分布，语义约束则根据样本标签相似性自适应调整注意力图，提升 AU 相关区域的权重，然后再用空间因果去混淆（SCD），引入扰动机制和因果效应建模，显式估计某一区域对 AU 预测的实际因果影响， 实验证明在 AU 复杂、标签不均衡的场景下更具优势。但是该方法仅限于空间维度，未对 AU 的时序因果演化进行建模，也未考虑模态间的因果关系，或许可以进一步引入时间因果图的建模思路，将面部区域在连续帧中的动态变化看作潜在干预因素，探究某些区域的早期活动是否导致后续 AU 的激活。
> >
> > [2025-04-6]-[何艺超]-[Large Language Models Are Reasoning Teachers][ACL 2023](https://arxiv.org/abs/2212.10071)
> >
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/Fine-tune-CoT.png">
> > 
> > 论文简述: 基于提示的CoT方法依赖于非常大的模型，基本都是几十B甚至几百B。作者提出Fine-tune-CoT，使用绝对大的大模型作为推理教师，获取特定任务下的CoT样本来微调0.3-10B模型。具体有三个步骤：Step 1. Reasoning generation【推理生成】，从具备CoT的模型中获取样本的推理链条。Step 2. Curation：筛选生成的样本并将其重新格式化为提示完成对。Step 3. Fine-tune：正常地基于下一个词预测任务。与此同时，方法可以选择性地生成多个推理链条，并使用随机采样从教师模型中生成多个推理解，以增加学生模型的训练数据。局限性：都是基于文本，多模态尚未探索，不过也很好转化。
> >
> > [2025-04-5]-[张雪松]-[UniGoal: Towards Universal Zero-shot Goal-oriented Navigation](https://arxiv.org/abs/2503.10630) arxiv2025.3.18
> >
> > <img width="512" alt="image" src="https://github.com/user-attachments/assets/3c612ef4-f138-4ba7-bd76-a293a2bd02f1">
> > 
> > 论文简述: 本文提出了UniGoal，一个面向通用**零样本目标导航ZSON**的框架，旨在提升模型在未知导航任务上的泛化能力。该方法采用多阶段预训练策略，先利用大规模图文数据学习通用感知与语言理解能力，再通过任务相关数据引导模型学习导航语义。同时引入多模态表示对齐机制，将语言、视觉和目标表示统一到共享语义空间，实现任务间的知识迁移和出色的导航表现。
> 
> > [2025-04-5]-[陈银]-[MMA-DFER: MultiModal Adaptation of unimodal models for Dynamic Facial Expression Recognition in-the-wild] CVPRW 2024
> >
> > <img width="512" alt="image" src="https://raw.githubusercontent.com/cyinen/imge/master/20250319183145.png">
> >
> > 论文简述: 不同与自监督预训练或者将FER模型扩展到DFER, 本文提出了一种新的方法，将预训练的MAE-Face和AudioMAE模型用到多模态的DFER上面。通过引入prompts适配各自的模态，并通过Fusion Bottleneck实现夸模态的融合，并在最后引入时序Transformer进行时序建模。本文提出的方法在DFER和MAFW上都实现了很高的性能，但是视觉模型仍依赖在规模人脸数据集上预训练。
>
> > [2025-03-31]-[于洋晨]-[Multimodal Emotion Recognition with Target Speaker-Based Facial Embeddings][2025 ICASSP](https://ieeexplore.ieee.org/abstract/document/10888205)
> > <img width="512" alt="image" src="https://github.com/yyc-hfut/PaperReading/blob/main/TSB.jpg">
> >
> > 论文简述: 继FacialMMT（2023 ACL）之后，今年很多基于Speaker's Face去做视频模态的特征增强任务。但目前来说，整体效果一般，单模态的视频效果仍然较差，其原因可能是在对话场景中，真实情感更多的表露在语言中，而表情中表露的真实情感更少，或者说简单的facial expression recognize任务很难察觉表观下的真实情感。例如，苦笑，尴尬的笑，冷笑，表观上看起来，都是笑容，但是其真实情感可能还是要结合具体语境进行判断，很难依靠面部表情识别其真实的，在多模态场景下的情感。
> 
> >   [2025-03-30]-[贾朋]-[TalkingGaussian: Structure-Persistent 3D Talking Head Synthesis via Gaussian Splatting](https://arxiv.org/abs/2404.15264)
> > <img width="512" alt="image" src="https://raw.githubusercontent.com/2351548518/images/main/20250120/20250330162410057.png">
> >
> > 论文简述: 作者提出TalkingGaussian，基于点的高斯泼溅技术，通过对持久高斯图元施加平滑连续的形变来表征面部运动，无需像既有方法那样学习复杂的外观变化。在此形变范式下，作者发现面部-口腔运动的不一致性会影响细微发音动作的学习。为此，作者将模型设计为分别处理面部和口腔区域的双分支结构，从而通过简化学习任务来重建更精确的口腔运动与结构。
> 
> > [2025-3-30] -[何艺超] - [MoE-LLaVA: Mixture of Experts for Large Vision-Language Models] - [Arxiv](http://arxiv.org/abs/2401.15947)
> > <img width='512' alt='image' src="https://github.com/ReadingPapers/Report/blob/main/Images/MoE-LLaVA.png"/>
> >
> > 论文简述：作者设计MoE-Tuning，对应的模型称作MoE-LLaVA 。该架构通过稀疏激活的专家机制，在保持计算成本不变的情况下可以扩展模型参数量。效果来说，MoE-LLaVA 只有大约 3B 的稀疏激活参数，但是可以在各种视觉理解数据集上表现出与 LLaVA-1.5-7B 相当的性能。MoE-LLaVA的核心是采用三阶段训练策略，第一阶段只训练视觉projector来进行多模态的对齐；第二阶段使用更复杂的指令，包括图像逻辑推理和文本识别等任务，这些任务使得模型具有更强的多模态理解【本质是指令微调】；第三阶段把第二阶段的ffn权重迁移至MoE的ffn中，模型上MoE ffn和原始ffn交替，以得到参数量削减的稀疏模型。
>
> > > [2025-3-30] -[张雪松] - [NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models]
> > <img width='512' alt='image' src="https://github.com/user-attachments/assets/a538981c-9345-4045-925b-4a702552a497"/>
> > 
> > 论文简述：本文提出NavGPT-2，一种结合大型视觉语言模型（VLM）与导航策略网络的方法，旨在提升视觉语言导航（VLN）任务的性能。通过冻结预训练的LLM，利用Q-former将多视角图像编码为固定长度的视觉标记，并注入方向信息生成导航提示。NavGPT-2采用两阶段训练：第一阶段微调Q-former生成导航推理，第二阶段结合拓扑图策略网络进行动作预测。实验表明，该方法在数据效率和性能上优于现有VLN专家模型，同时保留了LLM的解释能力，能够生成人类可理解的导航推理。
> 
> > [2025-3-30] -[徐赟博] - [VLFM: Vision-Language Frontier Mapsfor Zero-Shot Semantic Navigation] - [ICRA2024 最佳论文](https://arxiv.org/pdf/2312.03275)
> > <img width='512' alt='image' src="https://github.com/ReadingPapers/Report/blob/main/Images/VLFM.png"/>
> >
> > 论文简述：本文提供了一种在目标导航任务中的零样本方法。作者认为在人类在未知环境中进行导航所依靠的是环境中的语义知识来推断环境的布局。而大模型在训练过程中富含的先验语义知识可以用来促进Zero Shot导航能力。该方法提出基于空间语义相似度构建Value Map， 类似于SLAM，但核心是每个位置的视觉观察和目标物体（该任务最终的目的是找到对应的一个物体而非指令）的余弦相似度得分，同时，在生成语义得分的同时，另外又增加了置信度通道，由观察位置和视角光轴中心的距离决定，用来决定语义分数通道的更新方式。因为智能体在模拟环境中移动的时候观察全景图像存在视角重叠的问题，靠近光轴中心的被完全更新，边缘的以更小的权重更新。论文其他部分和连续导航存在差距，没完全精读。
>  >
>>[2025-03-29]-张宇-[PE-CLIP: A Parameter-Efficient Fine-Tuning of Vision Language Models for Dynamic Facial Expression Recognition]-[arxiv](https://arxiv.org/abs/2503.16945)
><img width='512' alt='image' src="https://github.com/ReadingPapers/Report/blob/main/Images/PE-CLIP.png"/>
>
>论文简述：目前用clip做dfer任务的通常需全参数微调，会破坏原有参数导致严重记忆遗忘问题。本篇工作也就是针对这一问题，设计了一种参数高效微调框架，通过时序动态adapter（TDA）和共享adapter（ShA）增强CLIP的时序建模能力与跨模态对齐。TDA通过GRU的动态缩放机制自动学习时序变化（也就是通过常规的adapter（但其中的激活函数用的是GELU）在clip的内部将多帧融合，不需要再经过时序模型），ShA以轻量瓶颈结构同步优化视觉与文本特征（这个adapter通过架构图也能看出是共享参数的）。最终通过面部动作单元（AU）的语义描述作为监督信号实现分类。最终实验表明，PE-CLIP在DFEW和FERV39K数据集上只有9M可训练参数，而且表现也不错，DFEW上的WAR达到了75.35%（目前在FER任务上对clip进行高效参数微调是一个不错的想法）。
>
>>[2025-03-29]-陈银-[CoPL:Parameter-Efficient Collaborative Prompt Learning for Audio-Visual Tasks] MM 2024
><img width='512' alt='image' src="https://raw.githubusercontent.com/cyinen/imge/master/20250328095606.png"/>
>
>论文简述： 将预训练的uni-modal模型迁移到多模态的任务能够减少训练的cost, 但是现有的方法主要关注的是多模态的融合，忽视了忽视模态特定的微调环节。为了解决这个，本文提出了高效的协作式提示学习（CoPL）来微调 uni-modal和多模态特征，包括模态特定的prompt与任务耦合的prompts, 并通过prompt bank实现基于实例的特征提取。最后使用MSE做模态对齐。在下游的几个音视频任务上以更少的参数实现了更好的性能。
>
>>[2025-03-22]-陈银-[Align before Adapt: Leveraging Entity-to-Region Alignments for Generalizable Video Action Recognition] CVPR 2024
><img width='512' alt='image' src="https://raw.githubusercontent.com/cyinen/imge/master/20250312181304.png"/>
>
>论文简述：本文提出了 Align Befor Adapt， 在Adapt 视频表征学习之前，先利用每一帧与实体-区域对齐。通过对区域感知图像嵌入与离线构建的文本语料库进行匹配，实现了实体对齐。利用已对齐的实体，将其文本嵌入作为查询输入到基于Transformer的视频适配器中，这有助于将视频中最重要的实体语义提取为向量表示。<font color="#366092">该范式在适配过程中重用了视觉语言预训练（VLP）的视觉-语言对齐机制，并尝试通过底层实体来解释动作。这种方法通过弥合复杂活动语义的差距来促进对动作的理解，特别是在面对不熟悉或未见过的类别时。</font>
>
>>[2025-03-22]-张雪松-[ETPNav: Evolving Topological Planning for
Vision-Language Navigation in Continuous
Environments](https://ieeexplore.ieee.org/abstract/document/10495141/)（TPAMI2024）
><img width="512" alt="image" src="https://github.com/user-attachments/assets/3d838dcf-6172-4b37-9737-f535762dc538" />
>
> 论文简述：连续和离散环境的VLN任务最主要的区别在于。连续环境中的agent无法依赖prior的graph map的结构进行high-level的导航，只能进行low-level的导航。为此，[之前的工作](https://github.com/YicongHong/Discrete-Continuous-VLN)已经设计了一个Candidate Waypoints Predictor来预测下一个可导航点，使得agent在连续的环境中来执行highlevel的导航动作。本文基于此，构建一个graphmap来预测下一个最可能的可导航点，然后使用一个参数化的控制器执行低水平的action从而实现导航任务。除此之外，（1）之前的Waypoints Predictor都是基于RGB-D的，而本文发现仅用depth效果更好。（2）本文对避障进行了简单的处理。最后，本文大幅提升了agent的导航表现，为连续环境下的VLN任务提供了一个strong baseline。

>>[2025-02-23]-张雪松-[NaVILA: Legged Robot Vision-Language-Action-Model for Navigation](https://navila-bot.github.io/)
><img width="512" alt="image" src="https://github.com/user-attachments/assets/37d1283a-2e83-4693-a6d2-b6ccafce7cf6" />
> 
> 论文简述：本文（应该是首次）将视觉语言导航任务成功落地到基于腿式机器人（宇树四足Go2和人形H1机器人）的室内室外导航场景。本文的方法分为两阶段，第一阶段是预训练视觉语言模型VILA（包含视觉encoder，一个projector，和一个LLM），其中预训练资源包括仿真环境中的VLN数据集，处理后的youtube中的视频，具身问答数据集等，有监督的微调阶段不冻结LLM的参数。第二阶段是使用单阶段的基于强化学习的动作学习策略，将第一阶段的语言输出等信息转换为四足的DoFs的低水平动作。实验结果，以单个view作为输入超过了gridmm在仿真环境的表现，在自己造的真实场景下也达到了将近50%的成功率。首次投ICLR2025被拒 https://openreview.net/forum?id=gkDRrvqeWF
> 
> >[2025-02-09]-[何艺超]-本周未精读
> >
>>[2025-02-09]-陈银-[HumanOmni: A Large Vision-Speech Language Model for Human-Centric Video Understanding]
><img width="512" alt="image" src="https://github.com/user-attachments/assets/af0134ac-3211-45a4-b14b-1fc320bd2eb7" />
>
> 论文简述：本文提出了一种以人为中心的视觉语音大模型，并构建了一个包含240万个人中心视频片段和详细caption的数据库，以及超过1400万个指令对。在“情感识别、面部表情描述和动作理解”方面，模型表现优异的性能。数据集的构建流程如下: 场景检测和分割->过滤低分辨率 clip->使用 Qwen2-VL 去掉相似的 clip->最终使用自动标注标注出人脸和身体的 bounding box->然后用两个多模态 VLM 模型进行caption，并使用大预言模型进行汇总共同点,去除幻觉，按照特定模板生成 Instrucitons。除此之外，还手动对其中的 50K 同时包含视觉和语音的video clips 进行了标注，用于后续的微调。模型的训练分为三阶段，视觉对齐，语音对齐，以及多模态交互。最终在DFEW， MAFW 上的 WAR分别达到了 82.46%，68.40%.
> 
> > [2025-01-26]-陈银-[A Multi-Task Learning Framework for Emotion  Recognition Using 2D Continuous Space]-TAFFC
>
>><img width="512" alt="image" src="https://github.com/user-attachments/assets/c41e9e42-aeb2-445d-8893-85fb7f112603" />
> 
> 论文简述：本文提出了一种基于深度信念网络（DBN）框架的多任务学习方法，用于情绪识别。该方法将情绪类别识别作为主要任务，将激活度和情感价的识别作为次要任务，通过两种策略——基于类别水平的分类和基于连续水平的回归——将次要任务整合到情绪识别系统中。实验在IEMOCAP和SEMAINE两个数据库上进行，结果表明，与仅使用基线特征的方法相比，该多任务学习框架在未加权准确率上取得了显著提升，证明了利用激活度和情感价信息在情绪识别中的有效性。
> > [2025-01-26]-程浩-本周未精读
> 
> > [2025-01-26]-[张雪松]-本周未精读
>
> > [2025-01-26]-[聂建涛]-本周未精读
>
> > [2025-01-26]-[于洋晨]-本周未精读
> 
> > [2025-01-26]-[贾朋]-本周未精读
> > 
> > [2025-01-26]-[徐赟博]-本周未精读
> > 
> > [2025-01-26]-[何艺超]-[Omni-Emotion: Extending Video MLLM with Detailed Face and Audio Modeling for Multimodal Emotion Analysis](https://arxiv.org/pdf/2501.09502v1)
> > 
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/Omni-Emotion%20MLLM.png">
> >
> > 论文简述：Emotion-LLaMA的后续工作，和Emotion-LLaMA非常相像，细节上更加精细，并且在MER-OV，EMER，recognition三个任务上都做了实验，十分全面，首先训练过程从二阶段调整成三阶段，即分步把音频对齐，视频对齐和细粒度微调，而不是视频音频一块对齐。在数据集的处理阶段，相较于emotion-llama完全借助大模型和AU知识生成caption，这里没有使用AU知识，而是借鉴了EmoLLM使用到的FaceXFormer针对年龄，性别，以及人脸特征提取，虽然instruction也是借助大模型提取，但是增加了gpt-3.5和人工的审查机制（emotion-llama没有审查），最终得到24137和3500的细粒度，粗粒度数据。然后模型设计这块，没有什么创新，单纯讨论了一下project在针对全画幅特征和人脸特征的融合机制，分别是：逐帧拼接，交叉注意力计算，以及全视频最终特征拼接。
> 
> >  [2025-1-26]-[张宇]-[CRoF: CLIP-based Robust Few-shot Learning on Noisy Labels]-[arxiv](https://arxiv.org/abs/2412.12793)
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/corf.png">
> >
> > 论文简述: 本篇工作是改进CLIP去适应噪声下的小样本学习，主要的出发点对准的是噪声这个特性，考虑到clip会把相似的类别误判的主要原因是：1）标签噪声导致2）clip的硬匹配造成，所以解决这一现状让clip在多噪声的环境下还能有较高得泛化性只能从第二点出发，设置加权平滑的软匹配，即top-k加权策略（目标如果被占cos得分的第一，赋值为1，如果在top-k中，那么通过加权把它的得分赋值到排名的最高，如果超出了范围，那么得分就是0），这也是本文主要的创新点之一，其他两点在我看来没有那么新颖感觉像是拼凑上去的（1.通过gpt-o1来进行prompt的生成，一个类别生成五组2.微调clip的两端，使用adapter+残差连接）这三点后面的消融中也都感觉很独立，所以在我看来，多噪声环境下的小样本学习主要还是靠clip的软匹配来获益的，总的来说这三点可以看成是一种策略，即插即用的训练策略，比较轻盈高效。
> 
> > [2025-1-19]-程浩-赶论文，未精~~度~~读
> > 
> > [2025-1-19]-[何艺超]-加紧多模态自监督的数据集微调数据传输等+家中琐事，本周没有来得及精读
> > 
> > [2025-1-19]-[张宇]-[本周没有精读]
> 
> > [2025-01-19]-[贾朋]-[Emotional Conversation- Empowering Talking Faces with Cohesive Expression, Gaze and Pose Generation](https://arxiv.org/abs/2406.07895)
> > <img width="512" alt="image" src="https://raw.githubusercontent.com/2351548518/images/main/20241219/20250113171534297.png">
> >
> > 论文简述: 提出了一个两阶段音频驱动的说话人脸生成框架，该框架采用 3D 面部标志作为中间变量，通过自监督学习实现了表情、注视和姿势的协调。作者将此任务分解为两个步骤，即语音到landmark的合成和landmaek到人脸的生成。语音到landmark合成：给定参考图像，提取归一化的landmarks、gaze和 head pose，并预测由输入语音和情感标签驱动的每帧运动。landmark到面部生成：每帧的facial landmarks被映射到潜在关键点，然后将其输入到预训练face-vid2vid模型中以生成最终的面部。
> 
> >[2025-1-18]-[徐赟博]-[AeroVerse: UAV-Agent Benchmark Suite for Simulating, Pre-training, Finetuning, and Evaluating Aerospace Embodied World Models]
> >
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/AeroVerse.png">
> >
> >论文简述：本文是构建无人机领域的新尝试。仿真器提供进行连续导航的条件。在两个典型城市（京沪）和学校以及居民区两个场景下搜集了大量的RGB图像和对应的深度图像，多个方向的环境caption。下游涵盖了场景描述，空间推理，导航决策等五个任务。提供了多个不同特点的数据集-以无人机为中心，以环境描述为中心，以空间推理为中心。论文看上去为无人机导航提供了很多可以直接用的插件。
> > 
> > [2025-1-17]-[聂建涛]-[Guided Interpretable Facial Expression Recognition via Spatial Action Unit Cues]-[paper](https://arxiv.org/pdf/2402.00281)
> >
> >  <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/Guided%20Interpretable%20Facial%20Expression%20Recognition%20via%20Spatial%20Action%20Unit%20Cues.png">
> >
> >论文简述：本文提出了一种新的学习策略，通过将空间动作单元（AU）线索显式地融入分类器训练，从而训练出深度可解释的模型。作者采用输入图像、表情标签和面部标记，结合AU代码本构建AU热图，以此作为空间线索，约束分类器的空间层特征与AU热图相关联，使用复合损失函数进行训练，以实现同时正确分类图像并生成与AU图相关的层内可视化注意力。
>
> >[2025-1-17]-[张雪松]-[NAVILA: LEGGED ROBOT VISION-LANGUAGEACTION MODEL FOR NAVIGATION](https://navila-bot.github.io/)
>> <img width="512" alt="image" src="https://github.com/user-attachments/assets/2c909803-ef4c-4f8b-8530-667796075ed7" />
>>
>>论文简述：本文主要关注腿式机器人的视觉语言导航。和之前的VLN任务相比，本任务的主要挑战在于，将自然语言指令迁移(translate)到机器人的腿部动作并不简单。暂时略读，一些demo比较炫酷，而且是面向实际应用场景的，代码似乎可用，准备未来fellow！
> 
>>[2025-1-17]-[陈银]-[Lifting Scheme-Based Implicit Disentanglement of Emotion-Related Facial Dynamics in the Wild]
>><img width="512" alt="image" src="https://github.com/user-attachments/assets/16272c26-1dd5-4807-a938-02079914fdc1" />
>>
>>论文简述：在自然场景的视频中，情感相关的表情往往被时间上和空间上的非情感相关表情和全局背景信息所稀释，这使得情感识别变得困难。本文提出了一种新的隐式面部动态解缠框架，通过小波提升方案（lifting scheme），在不使用显式操作或外部引导的情况下，隐式地将情感相关动态信息从全局背景信息中分离出来，减轻了情感无关帧的负面影响。
>
> 
> >[2025-1-13]-[徐赟博]-[ENVEDIT: Environment Editing for Vision-and-Language Navigation]
> ><img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/Envedit.png">
> >
> >论文简述：本文提出了一种对VLN的数据增强技术，在保留关键语义信息的基础上，改变房间的风格、外观、对象类别来减少训练集和测试集之间的分布迁移问题。通过条件可控生成的技术进行环境的编辑，其中注入生成模型的控制条件是语义分割的mask，这样可以保留到环境中的语义信息（和一定程度的结构信息）。在生成数据和原始数据的可视化对比中，作者发现具有高度的对应关系。在导航能力的定量对比中，这种数据增强技术可以同时在Seen和Unseen场景下提升明显
> >
> >[2025-01-13]-[于洋晨]-上周多次期末考试以及课程报告，未精度
>
> > [2025-01-12]-[贾朋]-[SyncTalk- The Devil is in the Synchronization for Talking Head Synthesis](https://ziqiaopeng.github.io/synctalk/)
> > <img width="512" alt="image" src="https://raw.githubusercontent.com/2351548518/images/main/20241219/20250109155213773.png">
> >
> > 论文简述: 传统的GAN很难保持一致的面部身份，而NeRF方法虽然可以解决这个问题，但通常会产生不匹配的嘴唇运动、面部表情不足和不稳定的头部姿势。作者提出了 SyncTalk这种基于 NeRF 的方法有效地保持了主体身份，增强了头部说话合成的同步性和真实感。SyncTalk 采用Face-Sync Controller将嘴唇运动与语音保持一致，并使用 3DMM 模型来捕捉准确的面部表情。大量的实验和用户研究表明，SyncTalk 在同步性和真实性方面优于最先进的方法。
> 
> > [2025-01-12]-[张雪松]-[Robustness Analysis of Video-Language Models Against Visual and Language Perturbations](https://proceedings.neurips.cc/paper_files/paper/2022/hash/de6ff07cbd222c10d694c2b2f732aceb-Abstract-Datasets_and_Benchmarks.html)
> > <img width="512" alt="image" src="https://github.com/user-attachments/assets/65b0509c-6b61-409a-a64e-63d0ff171650">
> >
> > 论文简述: 本文首次对视频-语言模型在真实世界扰动下的鲁棒性进行了广泛的研究。作者提出了两个大规模基准数据集（MSRVTT-P和YouCook2-P），并利用90种视觉和35种文本扰动进行测试，从而揭示出一些有趣的初步发现：1）当仅文本被扰动时，模型比仅视频被扰动时更为鲁棒；2）经过预训练的模型比从头训练的模型更鲁棒；3）模型更多地关注场景和物体，而不是动作和运动。
> 
> > [2024-1-12]-[张宇]-[Waffling around for Performance: Visual Classification with  Random Words and Broad Concepts]-[ICCV23](https://openaccess.thecvf.com/content/ICCV2023/html/Roth_Waffling_Around_for_Performance_Visual_Classification_with_Random_Words_and_ICCV_2023_paper.html)
> >
> >  <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/wallfclip.png">
> >
> >论文简述：LLM+CLIP的工作，但是探究的点很新颖，是探究性工作，一般LLM+CLIP的工作是使用类名单词作为强约束，多组符合常理的描述句子跟在类名单词后面作为细粒度的区分，但是这样的话很有可能是多组文本平均带来的性能提升，因为可能VLM不会真正理解那些一个类别多个描述的人为认定的细粒度描述语句（后面作者在实验中也证实了，如果它是从语言描述的细粒度程度上收益，那么按理说多组文本取max替代一般工作的mean方式也会表现不错，但是实验显示了换成max性能会大幅下降），这就更证实了作者的要探究的观点（胡乱的随机词作为类名之后的细粒度描述也能表现很好），所以也属于改进zero-shot的工作，prompt形式是“A photo of a {concept}: a {c}, which (is/has/etc) {随机单词或字符}.”所以还是有类名单词作为强约束的（因为后面消融也显示了只有随机字符作为监督性能不佳），为什么随机单词也能有区分呢？->可以把它看做是不同形式的噪声集成，可以理解成数据加噪，性能超越了使用专门设计的细粒度语言描述的工作。
>
> > [2024-1-12]-程浩- $\texttt{赶实验赶论文，未精读}$ \
> > [2025-1-12]-[聂建涛]-[Detecting Facial Action Units From Global-Local Fine-Grained Expressions]-[paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160018)
> >
> >  <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/Detecting%20Facial%20Action%20Units%20From%20Global-Local%20Fine-Grained%20Expressions.png">
> >
> >论文简述：论文提出了一种新的面部动作单位（AU）检测方法GLEE-Net，通过全局-局部面部表情嵌入技术，缓解了因AU注释需要专业知识导致的身份过拟合问题，同时利用了未被标记的的面部表情数据集来辅助AU检测。GLEE-Net框架包含三个分支，分别提取与身份无关的表情特征，其中全局分支消除身份影响建模整体面部表情，局部分支关注特定面部区域。该方法先在面部表情数据集上预训练得到身份无关的表情嵌入，然后在小量的AU数据集上进行微调。此外，引入了3D全局分支通过3D人脸重建提取表情系数以加强2D AU描述，最终使用基于Transformer的多标签分类器融合所有表示进行AU检测。
>
> > [2025-01-11]-何艺超-[Contrastive Instruction Tuning]-[ACL 2024 Findings](http://arxiv.org/abs/2402.11138)
> > <img width="512" alt="image" src=https://github.com/ReadingPapers/Report/blob/main/Images/COIN.png>
> >
> > 论文简述：作者针对指令微调在具体任务中过于依赖指令而在用户指令输入的单词错误，拼写错误，语法错误，语义错误中的表现不佳的问题，提出一种基于对比学习的指令微调，正样本对是原始指令和原始输入输出&原始指令改写指令和原始输入输出，然后负样本对是原始指令和原始输入输出&与任务无关指令和原始输入输出，或者是原始指令和原始输入输出&原始指令和与任务无关输入输出，同时将对比学习损失和LLM生成需要用的交叉熵损失结合，通过超参控制二者结合的比例，这种方法可以增强模型针对同一样本对于不同指令的语义理解，从而提高下游准确性。
> > 
> > [2025-01-11]-陈银-[Label-Guided Dynamic Spatial-Temporal Fusion for  Video-Based Facial Expression Recognition](https://ieeexplore.ieee.org/abstract/document/10552397)-[TMM]
> >
> > <img width="512" alt="image" src=https://raw.githubusercontent.com/cyinen/imge/master/20250108133119.png>
>>
>>论文简评：考虑到标签分布提供了图像的分类保真度，本文使用表情的分布来引导时空的融合。把 video的标签作为 label, 为每一个帧分配视频的 label, 并设计辅助损失函数来监督。利用学习到的标签分布计算一个动态权重，并用把权重用于时空融合。==有一种把监督信号加到中间，打破信息瓶颈的味道。==实验验证了为每一帧分配不同的权重是很重要的，能够让模型把注意力关注到更重要的帧上面，从而利用更有用的信息。**这个方向还有待深入研究**。
> 
> > [2025-1-5]-程浩-放假+实验+写论文=未精读\
> >
> > [2025-1-5]-[何艺超]-准备最后两门考试，未精读
> >
> > [2025-1-05]-[张雪松]-[ AeroVerse: UAV-Agent Benchmark Suite for Simulating, Pre-training, Finetuning, and Evaluating Aerospace Embodied World Models]-[paper](https://arxiv.org/pdf/2408.15511)
> >
> > <img width="512" alt="image" src="https://github.com/user-attachments/assets/10fec248-3237-46da-aeb7-2588ba9525cd">
> >
> > 论文简述：本文提出AeroVerse，作为无人机智能体领域的全新基准套件，包含AeroSimulator仿真平台、首个大规模真实图文预训练数据集AerialAgent-Ego10k与虚拟图文姿态对齐数据集CyberAgent-Ego500k，以及支持五类下游任务（场景感知、空间推理、导航探索、任务规划和动作决策）的微调数据集SkyAgent系列。此外，通过GPT-4开发的SkyAgent-Eval，实现更全面和客观的任务评估，有效揭示了现有2D/3D视觉语言模型在无人机任务中的潜力与局限性.（暂未开源，简单了解不算严格意义上的精读）
> >
> > [2025-1-05]-[徐赟博]-[SAME: Learning Generic Language-Guided Visual Navigation with State-Adaptive Mixture of Experts]
> >
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/SAME.png">
> >
> > 论文简述：本文提出了一种使用MOE的架构统一不同视觉语言导航任务的学习。作者首先使用了单独学习的方式，对不同语言引导程度的导航任务进行学习，发现这种方式会导致不同的任务之间产生负迁移的现象。于是，作者在Cross-Attention层引入MoE来促进多任务学习。但是，作者发现，直接进行moe的效果并不好，分析原因可能发生在不同导航任务的语言引导程度上（Zero/fine/coarse grined），于是作者在MoE层，预测路由概率时加入了与任务以及多模态相关的信息。促进了七种导航任务的多任务学习。实验做得不太完善，进行对比的很多工作都是没有在某个数据集上训练过的，但是也是在VLN中使用MoE的先河。
> >
> > [2025-01-05]-[贾朋]-[LES-Talker: Fine-Grained Emotion Editing for Talking Head Generation in Linear Emotion Space](https://peterfanfan.github.io/LES-Talker/)
> > <img width="512" alt="image" src="https://raw.githubusercontent.com/2351548518/images/main/20241219/20250105215301644.png">
> >
> > 论文简述: 虽然现有的Talking head模型在粗粒度情感编辑方面取得了进展，但仍然缺乏具有高可解释性的细粒度情感编辑模型。作者提出了 LES-Talker，一种具有高可解释性的Talking head生成模型，以实现不同情感类型、情感水平和AU的细粒度情感编辑，提出了基于AU的线性情感空间（LES）定义，将情感变换描述为向量变换。设计了跨维度注意力网络（CDAN）来挖掘 LES 表示和 3D 模型表示之间的相关性，使 LES 表示能够指导 3D 模型的可控变形。
> 
> > [2025-1-5]-[张宇]-准备最后两门考试，未精度
>
> > [2025-1-5]-[聂建涛]-[FaceMixup: Enhancing Facial Expression Recognition through Mixed Face Regularization]-[paper](https://arxiv.org/pdf/2405.20259)
> >
> >  <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/FaceMixup.png">
> >
> >论文简述：论文提出了一种新的面部数据增强方法，名为FaceMixup，旨在通过混合面部组件正则化来提高面部表情识别的性能。其主要思路是通过对不同类别的面部图像进行裁剪和替换操作来创建新的混合面部图像，并在训练过程中考虑这些类别信息。FaceMixup方法通过混合面部组件进行正则化，生成额外的训练样本，以此丰富数据集并提高深度学习模型的泛化能力。
>
> > [2025-01-05]-[陈银]-这周在改论文，未精度


为简化排版，2024年小组精读论文已经保存在[这里](https://github.com/ReadingPapers/Report/blob/main/Archived/2024%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E5%BD%92%E6%A1%A3.md)







