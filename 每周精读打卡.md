######### 模版 #############

> week1 同一周在同一个level的bar上
> 
> [日期]-[first]-[论文标题]-[来源]（附带链接，链接可选，时间倒序）\
> >
> > 论文框架图（有助于一眼就能想起论文内容），宽度设置为512
> >
> > 论文简述 （一两句话总结精华，切勿过长）
> 
> [日期]-[second]-[论文标题]-[来源]（附带链接，链接可选，时间倒序）\
> >
> > 论文框架图（有助于一眼就能想起论文内容）
> >
> > 论文简述 （一两句话总结精华，切勿过长）
> 

> week2
> 
######### 模版 #############

## 必须是【全文】精读的论文，如实记录

> ==> 时间倒序，本次更新插入位置 <==
>
> > [2025-04-13]-[何艺超]-[R1-Omni: Explainable Omni-Multimodal Emotion Recognition with Reinforcement Learning](https://arxiv.org/abs/2503.05379)
> >
> > 论文简述: 3月10号第二版刚挂在arxiv的工作，这篇工作首次将具有可验证奖励的强化学习（RLVR）应用于情绪识别的多模态大型语言模型，RLVR的思路来源于deepseek，在情感的子任务上可以让即使是没有详细数据的训练数据【如DFEW，MAFW只有category的标签】，也可以让模型根据这些训练数据获得推理的能力，但是从实验结果来看，分类的准确率和目前主流方法的准确率（如视觉指令微调的大模型，还有传统的分类方法）比差距还是很大。但是和EMER进行SFT的方法比要好一些，这里的原因还是EMER数据集的322条数据太少了，即使是指令微调了，也不会调的很彻底，总的来说是给了一个大模型做情感识别的新的方式方法：即通过没有描述的数据让大模型具备推理到情感标签的能力。
> 
> > [2025-04-08]-程浩-补上周：由于赶论文deadline，无精读
>
> > [2025-04-07]-[贾朋]-[InsTaG: Learning Personalized 3D Talking Head from Few-Second Video](https://arxiv.org/abs/2502.20387)
> > <img width="512" alt="image" src="https://raw.githubusercontent.com/2351548518/images/main/20250120/20250407135714879.png">
> >
> > 论文简述: 作者提出InsTaG，基于轻量级3D高斯泼溅人物特定合成器与通用运动先验，在保持高度个性化和高效的同时，实现了高质量快速适配。大多数NeRF类方法采用针对特定人物的训练，给定包含目标说话者的视频片段，整个针对特定人物的模型会被训练以记忆目标说话头部，对未见过的身份不具备泛化能力，需要大量高质量视频帧和长时间训练来适应每个新身份。作者通过预训练身份无关的3D动作场作为先验知识并将其与新身份对齐以加速适配，提出了基于高斯泼溅的即时说话头像合成框架InsTaG。
> 
> >[2025-04-06]-[于洋晨]-[因实验与假期，本周无精读]
> 
> > [2025-04-06]-[聂建涛]-[An Efficient Attribute-Preserving Framework for Face Swapping]-[TMM2024](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10400952)
> >
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/An%20Efficient%20Attribute-Preserving%20Framework%20for%20Face%20Swapping.png">
> >
> >论文简述：本文提出了一种高效的属性保持框架AP-Swap，用于面部交换，能够在保持身份一致的同时，有效保留目标面部的关键属性如头部姿势、表情和视线方向。方法包含两个创新模块，专门用于保留关键的面部属性。首先通过全局残余属性保留编码器（GRAPE）自适应地从目标人脸中提取全局完整的属性特征；其次，除了源面部图像和目标面部图像的常规网络流之外，引入了一个考虑到目标面部关键点的网络流。这个额外的网络流支持我们的面部关键点引导特征纠缠模块（LFEM），该模块通过执行基于地标的属性保留（LBAP）操作，有效地保留了细粒度的面部属性。
> >
> > [2025-04-06]-[徐赟博]-[Lookahead Exploration with Neural Radiance Representation for Continuous Vision-Language Navigation]-[CVPR2024](https://arxiv.org/pdf/2404.01943)
> >
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/HNR.png">
> >
> >论文简述：本文提出了一种基于Nerf合成未来视图的方法进行连续环境下的视觉语言导航的方法。首先，从多视角图像中抽取Feature Cloud 类似于稀疏点云。然后在连续导航的统一设定下，在每个导航轮次通过特征云再通过Nerf的可学习MLP建立对整个导航场景的三维隐式表示，合成关于未来视角的图像。包括多种特征，分别是未来视图的Nerf特征，和未来视图的渲染真实图像。分别将未来视图的Nerf特征和未来视图的GT CLIP特征做交叉熵损失和未来视图的渲染图像和未来视图的GT 图像进行交叉熵损失来建立对3D场景信息的保存。在推理过程中，对可选择的可导航点的未来图像进行预测和打分，选择最优打分的导航点作为下一步。但是该工作代码实现和论文内容不太一致，而且基于Nerf的方法需要太长的时间代价，4卡3090训练一个多星期，不太值得Follow。
> >
> > [2025-04-6]-[张宇]-[Facial Action Unit Detection by Adaptively Constraining Self-Attention and Causally Deconfounding Sample][IJCV 2024](https://arxiv.org/abs/2212.10071)
> >
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/ACD.png">
> > 
> > 论文简述:现有AU检测方法大多直接使用自注意力机制空间建模依赖，但容易受到伪相关性的干扰。所以本文从注意力约束和因果先验去混淆两个角度入手，提升 AU 检测性能。首先空间约束自注意（ACS），在原有自注意力结构上加入两类约束（先验和语义）：先验约束通过 AU 区域信息指导注意力分布，语义约束则根据样本标签相似性自适应调整注意力图，提升 AU 相关区域的权重，然后再用空间因果去混淆（SCD），引入扰动机制和因果效应建模，显式估计某一区域对 AU 预测的实际因果影响， 实验证明在 AU 复杂、标签不均衡的场景下更具优势。但是该方法仅限于空间维度，未对 AU 的时序因果演化进行建模，也未考虑模态间的因果关系，或许可以进一步引入时间因果图的建模思路，将面部区域在连续帧中的动态变化看作潜在干预因素，探究某些区域的早期活动是否导致后续 AU 的激活。
> >
> > [2025-04-6]-[何艺超]-[Large Language Models Are Reasoning Teachers][ACL 2023](https://arxiv.org/abs/2212.10071)
> >
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/Fine-tune-CoT.png">
> > 
> > 论文简述: 基于提示的CoT方法依赖于非常大的模型，基本都是几十B甚至几百B。作者提出Fine-tune-CoT，使用绝对大的大模型作为推理教师，获取特定任务下的CoT样本来微调0.3-10B模型。具体有三个步骤：Step 1. Reasoning generation【推理生成】，从具备CoT的模型中获取样本的推理链条。Step 2. Curation：筛选生成的样本并将其重新格式化为提示完成对。Step 3. Fine-tune：正常地基于下一个词预测任务。与此同时，方法可以选择性地生成多个推理链条，并使用随机采样从教师模型中生成多个推理解，以增加学生模型的训练数据。局限性：都是基于文本，多模态尚未探索，不过也很好转化。
> >
> > [2025-04-5]-[张雪松]-[UniGoal: Towards Universal Zero-shot Goal-oriented Navigation](https://arxiv.org/abs/2503.10630) arxiv2025.3.18
> >
> > <img width="512" alt="image" src="https://github.com/user-attachments/assets/3c612ef4-f138-4ba7-bd76-a293a2bd02f1">
> > 
> > 论文简述: 本文提出了UniGoal，一个面向通用**零样本目标导航ZSON**的框架，旨在提升模型在未知导航任务上的泛化能力。该方法采用多阶段预训练策略，先利用大规模图文数据学习通用感知与语言理解能力，再通过任务相关数据引导模型学习导航语义。同时引入多模态表示对齐机制，将语言、视觉和目标表示统一到共享语义空间，实现任务间的知识迁移和出色的导航表现。
> 
> > [2025-04-5]-[陈银]-[MMA-DFER: MultiModal Adaptation of unimodal models for Dynamic Facial Expression Recognition in-the-wild] CVPRW 2024
> >
> > <img width="512" alt="image" src="https://raw.githubusercontent.com/cyinen/imge/master/20250319183145.png">
> >
> > 论文简述: 不同与自监督预训练或者将FER模型扩展到DFER, 本文提出了一种新的方法，将预训练的MAE-Face和AudioMAE模型用到多模态的DFER上面。通过引入prompts适配各自的模态，并通过Fusion Bottleneck实现夸模态的融合，并在最后引入时序Transformer进行时序建模。本文提出的方法在DFER和MAFW上都实现了很高的性能，但是视觉模型仍依赖在规模人脸数据集上预训练。
>
> > [2025-03-31]-[于洋晨]-[Multimodal Emotion Recognition with Target Speaker-Based Facial Embeddings][2025 ICASSP](https://ieeexplore.ieee.org/abstract/document/10888205)
> > <img width="512" alt="image" src="https://github.com/yyc-hfut/PaperReading/blob/main/TSB.jpg">
> >
> > 论文简述: 继FacialMMT（2023 ACL）之后，今年很多基于Speaker's Face去做视频模态的特征增强任务。但目前来说，整体效果一般，单模态的视频效果仍然较差，其原因可能是在对话场景中，真实情感更多的表露在语言中，而表情中表露的真实情感更少，或者说简单的facial expression recognize任务很难察觉表观下的真实情感。例如，苦笑，尴尬的笑，冷笑，表观上看起来，都是笑容，但是其真实情感可能还是要结合具体语境进行判断，很难依靠面部表情识别其真实的，在多模态场景下的情感。
> 
> >   [2025-03-30]-[贾朋]-[TalkingGaussian: Structure-Persistent 3D Talking Head Synthesis via Gaussian Splatting](https://arxiv.org/abs/2404.15264)
> > <img width="512" alt="image" src="https://raw.githubusercontent.com/2351548518/images/main/20250120/20250330162410057.png">
> >
> > 论文简述: 作者提出TalkingGaussian，基于点的高斯泼溅技术，通过对持久高斯图元施加平滑连续的形变来表征面部运动，无需像既有方法那样学习复杂的外观变化。在此形变范式下，作者发现面部-口腔运动的不一致性会影响细微发音动作的学习。为此，作者将模型设计为分别处理面部和口腔区域的双分支结构，从而通过简化学习任务来重建更精确的口腔运动与结构。
> 
> > [2025-3-30] -[何艺超] - [MoE-LLaVA: Mixture of Experts for Large Vision-Language Models] - [Arxiv](http://arxiv.org/abs/2401.15947)
> > <img width='512' alt='image' src="https://github.com/ReadingPapers/Report/blob/main/Images/MoE-LLaVA.png"/>
> >
> > 论文简述：作者设计MoE-Tuning，对应的模型称作MoE-LLaVA 。该架构通过稀疏激活的专家机制，在保持计算成本不变的情况下可以扩展模型参数量。效果来说，MoE-LLaVA 只有大约 3B 的稀疏激活参数，但是可以在各种视觉理解数据集上表现出与 LLaVA-1.5-7B 相当的性能。MoE-LLaVA的核心是采用三阶段训练策略，第一阶段只训练视觉projector来进行多模态的对齐；第二阶段使用更复杂的指令，包括图像逻辑推理和文本识别等任务，这些任务使得模型具有更强的多模态理解【本质是指令微调】；第三阶段把第二阶段的ffn权重迁移至MoE的ffn中，模型上MoE ffn和原始ffn交替，以得到参数量削减的稀疏模型。
>
> > > [2025-3-30] -[张雪松] - [NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models]
> > <img width='512' alt='image' src="https://github.com/user-attachments/assets/a538981c-9345-4045-925b-4a702552a497"/>
> > 
> > 论文简述：本文提出NavGPT-2，一种结合大型视觉语言模型（VLM）与导航策略网络的方法，旨在提升视觉语言导航（VLN）任务的性能。通过冻结预训练的LLM，利用Q-former将多视角图像编码为固定长度的视觉标记，并注入方向信息生成导航提示。NavGPT-2采用两阶段训练：第一阶段微调Q-former生成导航推理，第二阶段结合拓扑图策略网络进行动作预测。实验表明，该方法在数据效率和性能上优于现有VLN专家模型，同时保留了LLM的解释能力，能够生成人类可理解的导航推理。
> 
> > [2025-3-30] -[徐赟博] - [VLFM: Vision-Language Frontier Mapsfor Zero-Shot Semantic Navigation] - [ICRA2024 最佳论文](https://arxiv.org/pdf/2312.03275)
> > <img width='512' alt='image' src="https://github.com/ReadingPapers/Report/blob/main/Images/VLFM.png"/>
> >
> > 论文简述：本文提供了一种在目标导航任务中的零样本方法。作者认为在人类在未知环境中进行导航所依靠的是环境中的语义知识来推断环境的布局。而大模型在训练过程中富含的先验语义知识可以用来促进Zero Shot导航能力。该方法提出基于空间语义相似度构建Value Map， 类似于SLAM，但核心是每个位置的视觉观察和目标物体（该任务最终的目的是找到对应的一个物体而非指令）的余弦相似度得分，同时，在生成语义得分的同时，另外又增加了置信度通道，由观察位置和视角光轴中心的距离决定，用来决定语义分数通道的更新方式。因为智能体在模拟环境中移动的时候观察全景图像存在视角重叠的问题，靠近光轴中心的被完全更新，边缘的以更小的权重更新。论文其他部分和连续导航存在差距，没完全精读。
>  >
>>[2025-03-29]-张宇-[PE-CLIP: A Parameter-Efficient Fine-Tuning of Vision Language Models for Dynamic Facial Expression Recognition]-[arxiv](https://arxiv.org/abs/2503.16945)
><img width='512' alt='image' src="https://github.com/ReadingPapers/Report/blob/main/Images/PE-CLIP.png"/>
>
>论文简述：目前用clip做dfer任务的通常需全参数微调，会破坏原有参数导致严重记忆遗忘问题。本篇工作也就是针对这一问题，设计了一种参数高效微调框架，通过时序动态adapter（TDA）和共享adapter（ShA）增强CLIP的时序建模能力与跨模态对齐。TDA通过GRU的动态缩放机制自动学习时序变化（也就是通过常规的adapter（但其中的激活函数用的是GELU）在clip的内部将多帧融合，不需要再经过时序模型），ShA以轻量瓶颈结构同步优化视觉与文本特征（这个adapter通过架构图也能看出是共享参数的）。最终通过面部动作单元（AU）的语义描述作为监督信号实现分类。最终实验表明，PE-CLIP在DFEW和FERV39K数据集上只有9M可训练参数，而且表现也不错，DFEW上的WAR达到了75.35%（目前在FER任务上对clip进行高效参数微调是一个不错的想法）。
>
>>[2025-03-29]-陈银-[CoPL:Parameter-Efficient Collaborative Prompt Learning for Audio-Visual Tasks] MM 2024
><img width='512' alt='image' src="https://raw.githubusercontent.com/cyinen/imge/master/20250328095606.png"/>
>
>论文简述： 将预训练的uni-modal模型迁移到多模态的任务能够减少训练的cost, 但是现有的方法主要关注的是多模态的融合，忽视了忽视模态特定的微调环节。为了解决这个，本文提出了高效的协作式提示学习（CoPL）来微调 uni-modal和多模态特征，包括模态特定的prompt与任务耦合的prompts, 并通过prompt bank实现基于实例的特征提取。最后使用MSE做模态对齐。在下游的几个音视频任务上以更少的参数实现了更好的性能。
>
>>[2025-03-22]-陈银-[Align before Adapt: Leveraging Entity-to-Region Alignments for Generalizable Video Action Recognition] CVPR 2024
><img width='512' alt='image' src="https://raw.githubusercontent.com/cyinen/imge/master/20250312181304.png"/>
>
>论文简述：本文提出了 Align Befor Adapt， 在Adapt 视频表征学习之前，先利用每一帧与实体-区域对齐。通过对区域感知图像嵌入与离线构建的文本语料库进行匹配，实现了实体对齐。利用已对齐的实体，将其文本嵌入作为查询输入到基于Transformer的视频适配器中，这有助于将视频中最重要的实体语义提取为向量表示。<font color="#366092">该范式在适配过程中重用了视觉语言预训练（VLP）的视觉-语言对齐机制，并尝试通过底层实体来解释动作。这种方法通过弥合复杂活动语义的差距来促进对动作的理解，特别是在面对不熟悉或未见过的类别时。</font>
>
>>[2025-03-22]-张雪松-[ETPNav: Evolving Topological Planning for
Vision-Language Navigation in Continuous
Environments](https://ieeexplore.ieee.org/abstract/document/10495141/)（TPAMI2024）
><img width="512" alt="image" src="https://github.com/user-attachments/assets/3d838dcf-6172-4b37-9737-f535762dc538" />
>
> 论文简述：连续和离散环境的VLN任务最主要的区别在于。连续环境中的agent无法依赖prior的graph map的结构进行high-level的导航，只能进行low-level的导航。为此，[之前的工作](https://github.com/YicongHong/Discrete-Continuous-VLN)已经设计了一个Candidate Waypoints Predictor来预测下一个可导航点，使得agent在连续的环境中来执行highlevel的导航动作。本文基于此，构建一个graphmap来预测下一个最可能的可导航点，然后使用一个参数化的控制器执行低水平的action从而实现导航任务。除此之外，（1）之前的Waypoints Predictor都是基于RGB-D的，而本文发现仅用depth效果更好。（2）本文对避障进行了简单的处理。最后，本文大幅提升了agent的导航表现，为连续环境下的VLN任务提供了一个strong baseline。

>>[2025-02-23]-张雪松-[NaVILA: Legged Robot Vision-Language-Action-Model for Navigation](https://navila-bot.github.io/)
><img width="512" alt="image" src="https://github.com/user-attachments/assets/37d1283a-2e83-4693-a6d2-b6ccafce7cf6" />
> 
> 论文简述：本文（应该是首次）将视觉语言导航任务成功落地到基于腿式机器人（宇树四足Go2和人形H1机器人）的室内室外导航场景。本文的方法分为两阶段，第一阶段是预训练视觉语言模型VILA（包含视觉encoder，一个projector，和一个LLM），其中预训练资源包括仿真环境中的VLN数据集，处理后的youtube中的视频，具身问答数据集等，有监督的微调阶段不冻结LLM的参数。第二阶段是使用单阶段的基于强化学习的动作学习策略，将第一阶段的语言输出等信息转换为四足的DoFs的低水平动作。实验结果，以单个view作为输入超过了gridmm在仿真环境的表现，在自己造的真实场景下也达到了将近50%的成功率。首次投ICLR2025被拒 https://openreview.net/forum?id=gkDRrvqeWF
> 
> >[2025-02-09]-[何艺超]-本周未精读
> >
>>[2025-02-09]-陈银-[HumanOmni: A Large Vision-Speech Language Model for Human-Centric Video Understanding]
><img width="512" alt="image" src="https://github.com/user-attachments/assets/af0134ac-3211-45a4-b14b-1fc320bd2eb7" />
>
> 论文简述：本文提出了一种以人为中心的视觉语音大模型，并构建了一个包含240万个人中心视频片段和详细caption的数据库，以及超过1400万个指令对。在“情感识别、面部表情描述和动作理解”方面，模型表现优异的性能。数据集的构建流程如下: 场景检测和分割->过滤低分辨率 clip->使用 Qwen2-VL 去掉相似的 clip->最终使用自动标注标注出人脸和身体的 bounding box->然后用两个多模态 VLM 模型进行caption，并使用大预言模型进行汇总共同点,去除幻觉，按照特定模板生成 Instrucitons。除此之外，还手动对其中的 50K 同时包含视觉和语音的video clips 进行了标注，用于后续的微调。模型的训练分为三阶段，视觉对齐，语音对齐，以及多模态交互。最终在DFEW， MAFW 上的 WAR分别达到了 82.46%，68.40%.
> 
> > [2025-01-26]-陈银-[A Multi-Task Learning Framework for Emotion  Recognition Using 2D Continuous Space]-TAFFC
>
>><img width="512" alt="image" src="https://github.com/user-attachments/assets/c41e9e42-aeb2-445d-8893-85fb7f112603" />
> 
> 论文简述：本文提出了一种基于深度信念网络（DBN）框架的多任务学习方法，用于情绪识别。该方法将情绪类别识别作为主要任务，将激活度和情感价的识别作为次要任务，通过两种策略——基于类别水平的分类和基于连续水平的回归——将次要任务整合到情绪识别系统中。实验在IEMOCAP和SEMAINE两个数据库上进行，结果表明，与仅使用基线特征的方法相比，该多任务学习框架在未加权准确率上取得了显著提升，证明了利用激活度和情感价信息在情绪识别中的有效性。
> > [2025-01-26]-程浩-本周未精读
> 
> > [2025-01-26]-[张雪松]-本周未精读
>
> > [2025-01-26]-[聂建涛]-本周未精读
>
> > [2025-01-26]-[于洋晨]-本周未精读
> 
> > [2025-01-26]-[贾朋]-本周未精读
> > 
> > [2025-01-26]-[徐赟博]-本周未精读
> > 
> > [2025-01-26]-[何艺超]-[Omni-Emotion: Extending Video MLLM with Detailed Face and Audio Modeling for Multimodal Emotion Analysis](https://arxiv.org/pdf/2501.09502v1)
> > 
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/Omni-Emotion%20MLLM.png">
> >
> > 论文简述：Emotion-LLaMA的后续工作，和Emotion-LLaMA非常相像，细节上更加精细，并且在MER-OV，EMER，recognition三个任务上都做了实验，十分全面，首先训练过程从二阶段调整成三阶段，即分步把音频对齐，视频对齐和细粒度微调，而不是视频音频一块对齐。在数据集的处理阶段，相较于emotion-llama完全借助大模型和AU知识生成caption，这里没有使用AU知识，而是借鉴了EmoLLM使用到的FaceXFormer针对年龄，性别，以及人脸特征提取，虽然instruction也是借助大模型提取，但是增加了gpt-3.5和人工的审查机制（emotion-llama没有审查），最终得到24137和3500的细粒度，粗粒度数据。然后模型设计这块，没有什么创新，单纯讨论了一下project在针对全画幅特征和人脸特征的融合机制，分别是：逐帧拼接，交叉注意力计算，以及全视频最终特征拼接。
> 
> >  [2025-1-26]-[张宇]-[CRoF: CLIP-based Robust Few-shot Learning on Noisy Labels]-[arxiv](https://arxiv.org/abs/2412.12793)
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/corf.png">
> >
> > 论文简述: 本篇工作是改进CLIP去适应噪声下的小样本学习，主要的出发点对准的是噪声这个特性，考虑到clip会把相似的类别误判的主要原因是：1）标签噪声导致2）clip的硬匹配造成，所以解决这一现状让clip在多噪声的环境下还能有较高得泛化性只能从第二点出发，设置加权平滑的软匹配，即top-k加权策略（目标如果被占cos得分的第一，赋值为1，如果在top-k中，那么通过加权把它的得分赋值到排名的最高，如果超出了范围，那么得分就是0），这也是本文主要的创新点之一，其他两点在我看来没有那么新颖感觉像是拼凑上去的（1.通过gpt-o1来进行prompt的生成，一个类别生成五组2.微调clip的两端，使用adapter+残差连接）这三点后面的消融中也都感觉很独立，所以在我看来，多噪声环境下的小样本学习主要还是靠clip的软匹配来获益的，总的来说这三点可以看成是一种策略，即插即用的训练策略，比较轻盈高效。
> 
> > [2025-1-19]-程浩-赶论文，未精~~度~~读
> > 
> > [2025-1-19]-[何艺超]-加紧多模态自监督的数据集微调数据传输等+家中琐事，本周没有来得及精读
> > 
> > [2025-1-19]-[张宇]-[本周没有精读]
> 
> > [2025-01-19]-[贾朋]-[Emotional Conversation- Empowering Talking Faces with Cohesive Expression, Gaze and Pose Generation](https://arxiv.org/abs/2406.07895)
> > <img width="512" alt="image" src="https://raw.githubusercontent.com/2351548518/images/main/20241219/20250113171534297.png">
> >
> > 论文简述: 提出了一个两阶段音频驱动的说话人脸生成框架，该框架采用 3D 面部标志作为中间变量，通过自监督学习实现了表情、注视和姿势的协调。作者将此任务分解为两个步骤，即语音到landmark的合成和landmaek到人脸的生成。语音到landmark合成：给定参考图像，提取归一化的landmarks、gaze和 head pose，并预测由输入语音和情感标签驱动的每帧运动。landmark到面部生成：每帧的facial landmarks被映射到潜在关键点，然后将其输入到预训练face-vid2vid模型中以生成最终的面部。
> 
> >[2025-1-18]-[徐赟博]-[AeroVerse: UAV-Agent Benchmark Suite for Simulating, Pre-training, Finetuning, and Evaluating Aerospace Embodied World Models]
> >
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/AeroVerse.png">
> >
> >论文简述：本文是构建无人机领域的新尝试。仿真器提供进行连续导航的条件。在两个典型城市（京沪）和学校以及居民区两个场景下搜集了大量的RGB图像和对应的深度图像，多个方向的环境caption。下游涵盖了场景描述，空间推理，导航决策等五个任务。提供了多个不同特点的数据集-以无人机为中心，以环境描述为中心，以空间推理为中心。论文看上去为无人机导航提供了很多可以直接用的插件。
> > 
> > [2025-1-17]-[聂建涛]-[Guided Interpretable Facial Expression Recognition via Spatial Action Unit Cues]-[paper](https://arxiv.org/pdf/2402.00281)
> >
> >  <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/Guided%20Interpretable%20Facial%20Expression%20Recognition%20via%20Spatial%20Action%20Unit%20Cues.png">
> >
> >论文简述：本文提出了一种新的学习策略，通过将空间动作单元（AU）线索显式地融入分类器训练，从而训练出深度可解释的模型。作者采用输入图像、表情标签和面部标记，结合AU代码本构建AU热图，以此作为空间线索，约束分类器的空间层特征与AU热图相关联，使用复合损失函数进行训练，以实现同时正确分类图像并生成与AU图相关的层内可视化注意力。
>
> >[2025-1-17]-[张雪松]-[NAVILA: LEGGED ROBOT VISION-LANGUAGEACTION MODEL FOR NAVIGATION](https://navila-bot.github.io/)
>> <img width="512" alt="image" src="https://github.com/user-attachments/assets/2c909803-ef4c-4f8b-8530-667796075ed7" />
>>
>>论文简述：本文主要关注腿式机器人的视觉语言导航。和之前的VLN任务相比，本任务的主要挑战在于，将自然语言指令迁移(translate)到机器人的腿部动作并不简单。暂时略读，一些demo比较炫酷，而且是面向实际应用场景的，代码似乎可用，准备未来fellow！
> 
>>[2025-1-17]-[陈银]-[Lifting Scheme-Based Implicit Disentanglement of Emotion-Related Facial Dynamics in the Wild]
>><img width="512" alt="image" src="https://github.com/user-attachments/assets/16272c26-1dd5-4807-a938-02079914fdc1" />
>>
>>论文简述：在自然场景的视频中，情感相关的表情往往被时间上和空间上的非情感相关表情和全局背景信息所稀释，这使得情感识别变得困难。本文提出了一种新的隐式面部动态解缠框架，通过小波提升方案（lifting scheme），在不使用显式操作或外部引导的情况下，隐式地将情感相关动态信息从全局背景信息中分离出来，减轻了情感无关帧的负面影响。
>
> 
> >[2025-1-13]-[徐赟博]-[ENVEDIT: Environment Editing for Vision-and-Language Navigation]
> ><img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/Envedit.png">
> >
> >论文简述：本文提出了一种对VLN的数据增强技术，在保留关键语义信息的基础上，改变房间的风格、外观、对象类别来减少训练集和测试集之间的分布迁移问题。通过条件可控生成的技术进行环境的编辑，其中注入生成模型的控制条件是语义分割的mask，这样可以保留到环境中的语义信息（和一定程度的结构信息）。在生成数据和原始数据的可视化对比中，作者发现具有高度的对应关系。在导航能力的定量对比中，这种数据增强技术可以同时在Seen和Unseen场景下提升明显
> >
> >[2025-01-13]-[于洋晨]-上周多次期末考试以及课程报告，未精度
>
> > [2025-01-12]-[贾朋]-[SyncTalk- The Devil is in the Synchronization for Talking Head Synthesis](https://ziqiaopeng.github.io/synctalk/)
> > <img width="512" alt="image" src="https://raw.githubusercontent.com/2351548518/images/main/20241219/20250109155213773.png">
> >
> > 论文简述: 传统的GAN很难保持一致的面部身份，而NeRF方法虽然可以解决这个问题，但通常会产生不匹配的嘴唇运动、面部表情不足和不稳定的头部姿势。作者提出了 SyncTalk这种基于 NeRF 的方法有效地保持了主体身份，增强了头部说话合成的同步性和真实感。SyncTalk 采用Face-Sync Controller将嘴唇运动与语音保持一致，并使用 3DMM 模型来捕捉准确的面部表情。大量的实验和用户研究表明，SyncTalk 在同步性和真实性方面优于最先进的方法。
> 
> > [2025-01-12]-[张雪松]-[Robustness Analysis of Video-Language Models Against Visual and Language Perturbations](https://proceedings.neurips.cc/paper_files/paper/2022/hash/de6ff07cbd222c10d694c2b2f732aceb-Abstract-Datasets_and_Benchmarks.html)
> > <img width="512" alt="image" src="https://github.com/user-attachments/assets/65b0509c-6b61-409a-a64e-63d0ff171650">
> >
> > 论文简述: 本文首次对视频-语言模型在真实世界扰动下的鲁棒性进行了广泛的研究。作者提出了两个大规模基准数据集（MSRVTT-P和YouCook2-P），并利用90种视觉和35种文本扰动进行测试，从而揭示出一些有趣的初步发现：1）当仅文本被扰动时，模型比仅视频被扰动时更为鲁棒；2）经过预训练的模型比从头训练的模型更鲁棒；3）模型更多地关注场景和物体，而不是动作和运动。
> 
> > [2024-1-12]-[张宇]-[Waffling around for Performance: Visual Classification with  Random Words and Broad Concepts]-[ICCV23](https://openaccess.thecvf.com/content/ICCV2023/html/Roth_Waffling_Around_for_Performance_Visual_Classification_with_Random_Words_and_ICCV_2023_paper.html)
> >
> >  <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/wallfclip.png">
> >
> >论文简述：LLM+CLIP的工作，但是探究的点很新颖，是探究性工作，一般LLM+CLIP的工作是使用类名单词作为强约束，多组符合常理的描述句子跟在类名单词后面作为细粒度的区分，但是这样的话很有可能是多组文本平均带来的性能提升，因为可能VLM不会真正理解那些一个类别多个描述的人为认定的细粒度描述语句（后面作者在实验中也证实了，如果它是从语言描述的细粒度程度上收益，那么按理说多组文本取max替代一般工作的mean方式也会表现不错，但是实验显示了换成max性能会大幅下降），这就更证实了作者的要探究的观点（胡乱的随机词作为类名之后的细粒度描述也能表现很好），所以也属于改进zero-shot的工作，prompt形式是“A photo of a {concept}: a {c}, which (is/has/etc) {随机单词或字符}.”所以还是有类名单词作为强约束的（因为后面消融也显示了只有随机字符作为监督性能不佳），为什么随机单词也能有区分呢？->可以把它看做是不同形式的噪声集成，可以理解成数据加噪，性能超越了使用专门设计的细粒度语言描述的工作。
>
> > [2024-1-12]-程浩- $\texttt{赶实验赶论文，未精读}$ \
> > [2025-1-12]-[聂建涛]-[Detecting Facial Action Units From Global-Local Fine-Grained Expressions]-[paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10160018)
> >
> >  <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/Detecting%20Facial%20Action%20Units%20From%20Global-Local%20Fine-Grained%20Expressions.png">
> >
> >论文简述：论文提出了一种新的面部动作单位（AU）检测方法GLEE-Net，通过全局-局部面部表情嵌入技术，缓解了因AU注释需要专业知识导致的身份过拟合问题，同时利用了未被标记的的面部表情数据集来辅助AU检测。GLEE-Net框架包含三个分支，分别提取与身份无关的表情特征，其中全局分支消除身份影响建模整体面部表情，局部分支关注特定面部区域。该方法先在面部表情数据集上预训练得到身份无关的表情嵌入，然后在小量的AU数据集上进行微调。此外，引入了3D全局分支通过3D人脸重建提取表情系数以加强2D AU描述，最终使用基于Transformer的多标签分类器融合所有表示进行AU检测。
>
> > [2025-01-11]-何艺超-[Contrastive Instruction Tuning]-[ACL 2024 Findings](http://arxiv.org/abs/2402.11138)
> > <img width="512" alt="image" src=https://github.com/ReadingPapers/Report/blob/main/Images/COIN.png>
> >
> > 论文简述：作者针对指令微调在具体任务中过于依赖指令而在用户指令输入的单词错误，拼写错误，语法错误，语义错误中的表现不佳的问题，提出一种基于对比学习的指令微调，正样本对是原始指令和原始输入输出&原始指令改写指令和原始输入输出，然后负样本对是原始指令和原始输入输出&与任务无关指令和原始输入输出，或者是原始指令和原始输入输出&原始指令和与任务无关输入输出，同时将对比学习损失和LLM生成需要用的交叉熵损失结合，通过超参控制二者结合的比例，这种方法可以增强模型针对同一样本对于不同指令的语义理解，从而提高下游准确性。
> > 
> > [2025-01-11]-陈银-[Label-Guided Dynamic Spatial-Temporal Fusion for  Video-Based Facial Expression Recognition](https://ieeexplore.ieee.org/abstract/document/10552397)-[TMM]
> >
> > <img width="512" alt="image" src=https://raw.githubusercontent.com/cyinen/imge/master/20250108133119.png>
>>
>>论文简评：考虑到标签分布提供了图像的分类保真度，本文使用表情的分布来引导时空的融合。把 video的标签作为 label, 为每一个帧分配视频的 label, 并设计辅助损失函数来监督。利用学习到的标签分布计算一个动态权重，并用把权重用于时空融合。==有一种把监督信号加到中间，打破信息瓶颈的味道。==实验验证了为每一帧分配不同的权重是很重要的，能够让模型把注意力关注到更重要的帧上面，从而利用更有用的信息。**这个方向还有待深入研究**。
> 
> > [2025-1-5]-程浩-放假+实验+写论文=未精读\
> >
> > [2025-1-5]-[何艺超]-准备最后两门考试，未精读
> >
> > [2025-1-05]-[张雪松]-[ AeroVerse: UAV-Agent Benchmark Suite for Simulating, Pre-training, Finetuning, and Evaluating Aerospace Embodied World Models]-[paper](https://arxiv.org/pdf/2408.15511)
> >
> > <img width="512" alt="image" src="https://github.com/user-attachments/assets/10fec248-3237-46da-aeb7-2588ba9525cd">
> >
> > 论文简述：本文提出AeroVerse，作为无人机智能体领域的全新基准套件，包含AeroSimulator仿真平台、首个大规模真实图文预训练数据集AerialAgent-Ego10k与虚拟图文姿态对齐数据集CyberAgent-Ego500k，以及支持五类下游任务（场景感知、空间推理、导航探索、任务规划和动作决策）的微调数据集SkyAgent系列。此外，通过GPT-4开发的SkyAgent-Eval，实现更全面和客观的任务评估，有效揭示了现有2D/3D视觉语言模型在无人机任务中的潜力与局限性.（暂未开源，简单了解不算严格意义上的精读）
> >
> > [2025-1-05]-[徐赟博]-[SAME: Learning Generic Language-Guided Visual Navigation with State-Adaptive Mixture of Experts]
> >
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/SAME.png">
> >
> > 论文简述：本文提出了一种使用MOE的架构统一不同视觉语言导航任务的学习。作者首先使用了单独学习的方式，对不同语言引导程度的导航任务进行学习，发现这种方式会导致不同的任务之间产生负迁移的现象。于是，作者在Cross-Attention层引入MoE来促进多任务学习。但是，作者发现，直接进行moe的效果并不好，分析原因可能发生在不同导航任务的语言引导程度上（Zero/fine/coarse grined），于是作者在MoE层，预测路由概率时加入了与任务以及多模态相关的信息。促进了七种导航任务的多任务学习。实验做得不太完善，进行对比的很多工作都是没有在某个数据集上训练过的，但是也是在VLN中使用MoE的先河。
> >
> > [2025-01-05]-[贾朋]-[LES-Talker: Fine-Grained Emotion Editing for Talking Head Generation in Linear Emotion Space](https://peterfanfan.github.io/LES-Talker/)
> > <img width="512" alt="image" src="https://raw.githubusercontent.com/2351548518/images/main/20241219/20250105215301644.png">
> >
> > 论文简述: 虽然现有的Talking head模型在粗粒度情感编辑方面取得了进展，但仍然缺乏具有高可解释性的细粒度情感编辑模型。作者提出了 LES-Talker，一种具有高可解释性的Talking head生成模型，以实现不同情感类型、情感水平和AU的细粒度情感编辑，提出了基于AU的线性情感空间（LES）定义，将情感变换描述为向量变换。设计了跨维度注意力网络（CDAN）来挖掘 LES 表示和 3D 模型表示之间的相关性，使 LES 表示能够指导 3D 模型的可控变形。
> 
> > [2025-1-5]-[张宇]-准备最后两门考试，未精度
>
> > [2025-1-5]-[聂建涛]-[FaceMixup: Enhancing Facial Expression Recognition through Mixed Face Regularization]-[paper](https://arxiv.org/pdf/2405.20259)
> >
> >  <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/FaceMixup.png">
> >
> >论文简述：论文提出了一种新的面部数据增强方法，名为FaceMixup，旨在通过混合面部组件正则化来提高面部表情识别的性能。其主要思路是通过对不同类别的面部图像进行裁剪和替换操作来创建新的混合面部图像，并在训练过程中考虑这些类别信息。FaceMixup方法通过混合面部组件进行正则化，生成额外的训练样本，以此丰富数据集并提高深度学习模型的泛化能力。
>
> > [2025-01-05]-[陈银]-这周在改论文，未精度


为简化排版，2024年小组精读论文已经保存在[这里](https://github.com/ReadingPapers/Report/blob/main/Archived/2024%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E5%BD%92%E6%A1%A3.md)







