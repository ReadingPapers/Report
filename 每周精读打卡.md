
######### 模版 #############

> [日期]-[总结人]-[论文标题]-[来源]（附带链接，链接可选，时间倒序）
>  
> > 论文框架图（有助于一眼就能想起论文内容）
> 
> > 论文简述 （一两句话总结精华，切勿过长）
> >

######### 模版 #############

> ==> 时间倒序，本次更新插入位置 <==
>
> > [2023-3-22]-[聂建涛]-[High-resolution face swapping via latent semantics disentanglement]-[Paper]([https://researchers.mq.edu.au/en/publications/mind-the-gap-improving-success-rate-of-vision-and-language-naviga](https://ink.library.smu.edu.sg/cgi/viewcontent.cgi?params=/context/sis_research/article/9535/&path_info=Xu_High_Resolution_Face_Swapping_via_Latent_Semantics_Disentanglement_CVPR_2022_paper.pdf))
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/blob/main/Images/High-resolution%20face%20swapping%20via%20latent%20semantics%20disentanglement.png">
> 
> >论文简述:
> > 本文提出了一种基于预训练的GAN模型的高分辨率面部交换方法。该方法通过分离源图像和目标图像在生成器中的潜在语义，明确地分离了潜在的语义，从浅层获得结构属性，从深层获得外观属性，实现结构属性和外观属性的单独处理。

>[2023-3-24]-[何艺超]-[MERBench: A Unified Evaluation Benchmark for Multimodal Emotion Recognition]-[Paper](https://arxiv.org/abs/2401.03429),[知乎](https://zhuanlan.zhihu.com/p/688678130)
>
> >论文简述：
> >
> >多模态情感识别领域模型方法研究的不一致性（数据集不一致，模型不一致，实验设置不一致）带来的难以公平评测不同模型能力的问题，后来的研究者难以轻松地选择适合自己的模型研究，这种不一致限制了多模态情感识别的发展，于是制作了MER2023 benchmark数据集，和一套模型测试方法，并在众多已有的模型上测试，给出了多指标的测试结果，方便后期工作者了解不同模型的性能。

> [2023-3-23]-[崔凯]-[PR-PL: A Novel Prototypical Representation based Pairwise Learning Framework for Emotion Recognition Using EEG Signals]-[Paper](https://ieeexplore.ieee.org/abstract/document/10160130)， [GitHub](https://github.com/KAZABANA/PR-PL)
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/assets/127180964/2564838f-603d-4d52-9bab-86edecee3564">
> 
> >论文简述：
> >
> >我们提出了一种新颖的迁移学习框架，该框架具有基于原型表示的成对学习（PR-PL）。学习区分性和广义的脑电图特征以揭示个体之间的情绪，并将情绪识别任务制定为成对学习，以提高模型对噪声标签的耐受性。更具体地说，开发了一种原型学习来编码脑电图数据固有的与情感相关的语义结构，并在考虑源域和目标域的特征可分离性的情况下将个体的脑电图特征对齐到共享的公共特征空间。

> [2023-3-23]-[王会雅]-[Facial micro-expression spotting and recognition using time contrasted feature with visual memory]-[Arxiv](https://arxiv.org/pdf/1902.03514) 
>
> >论文简述:
> >
> >本文使用时间网络与使用DeepLab v1的空间网络给出的更高层次微表情图像编码进行对比得出微表情相关的特征，GRU模块利用这种对比特征来记忆和预测微表情片段时间框架中的微表情的类别和强度，此方法不仅提高了从不明显的面部运动中识别微表情的能力，相对于传统识别方法还具有更高的准确性。
> >
> >
> [2023-3-23]-[陈银]-[Unmasked Teacher: Towards Training-Efficient Video Foundation Model]-[Paper](https://arxiv.org/abs/2303.16058)， [GitHub](https://github.com/OpenGVLab/unmasked_teacher) 
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/assets/56926538/80a93820-cdf3-4ba7-b690-23374bf454f0">
> >
> >论文简述:
> >
> > 基于像素重建的 VideoMAE预训练方式学习到的特征与下游任务存在较大的 gap, 而 CLIP 基于图像文本预训练包含更 hifh-level 的语义信息。本文提出一个新的框架，将 CLIP 预训练的 Image encoder 作为 Teacher, 指导 video model (Student)的学习， 使student model 既具备了时空捕捉能力，又学到了更接近下游任务的 high-level 知识。在下游的多个任务上达到 sota, 并且 student的性能超过了 teacher.

> [2023-3-22]-[张雪松]-[Mind the Gap: Improving Success Rate of Vision-and-Language
Navigation by Revisiting Oracle Success Routes]-[Paper](https://researchers.mq.edu.au/en/publications/mind-the-gap-improving-success-rate-of-vision-and-language-naviga)， [知乎](https://zhuanlan.zhihu.com/p/688058784](https://zhuanlan.zhihu.com/p/687858582))
> > <img width="512" alt="image" src="https://github.com/ReadingPapers/Report/assets/68311986/74ed96aa-e4c5-417b-aaac-0dc0fdd1ca64">
> 
> >论文简述:
> > 本文基于已有方法提供的指令和轨迹(不是智能体主动采样得到的)，设计了一个基于tansformer多模块的框架，旨在找到轨迹中和指令描述匹配的目标位置，从而提高导航成功率，减少SR和OSR之间的差距。

> [2023-3-20]-[张宇]-[EmoCLIP: A Vision-Language Method for Zero-Shot Video Facial Expression Recognition]-[Arxiv](https://arxiv.org/abs/2310.16640) [知乎](https://zhuanlan.zhihu.com/p/688058784)
>
> >论文简述:
> >
> >在CLIP的基础上进行改进实现对DFER进行zero-shot，加入ViT模型模拟时间维度，首个在DFER上提出使用样本级视频-文本数据集进行训练，在其余流行的DFER数据集上进行zero-shot进行测试，区别于VLM在DFER上CLIPER和DFER-CLIP的类级学习范式，微调了CLIP的image端和text端，使其对人脸表情以及表情描述文本更加敏感。

> [2023-3-19]-[程浩]-[Self-supervised Cross-modal Pretraining for Speech Emotion Recognition and Sentiment Analysis]-[EMNLP 2022](https://aclanthology.org/2022.findings-emnlp.375.pdf)
>
> >论文简述:\
> >利用Textual modality现成的一个类Bert但更General的模型RoBERTa和Acoustic modality中一个较General的模型HuBERT分别作为两个模态的Embedding, 接着在特征层面进行Mask and Reconstruction的学习，模型对于两个模态分别设计了两个Transformer模块，并利用Cross-attention机制进行模态间的特征融合，最终在IEMOCAP和MOSEI两个数据集上达到了SOTA。


>  [2023-3-17]-[崔凯]-[TSception: Capturing Temporal Dynamics and Spatial Asymmetry From EEG for Emotion Recognition]-[ieee.org](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9762054)
>

> > 论文简述
> >
> > 针对EEG信号的high temporal resolution和the asymmetric spatial activations属性，作者设计了具有多尺度卷积核Dynamic Temporal Layer以及Asymmetric Spatial Layer，以此来捕捉EEG的时间动态性和空间不对称性，实现更准确和更具备泛化的情绪识别。

> [2023-3-13]-[陈银]-[A $^3$ lign-DFER: Pioneering Comprehensive Dynamic Affective Alignment for Dynamic Facial Expression Recognition with CLIP]-[Arxiv](https://arxiv.org/abs/2403.04294) [知乎](https://zhuanlan.zhihu.com/p/686840722)
>  

> > 论文简述：
> > 
> > 在CLIP的基础上，提出多个模块，从affective, dynamic和bidirectional三个角度实现了动态情感对齐，达到了较高的performance。相比于之前的prompt learning ， 把text embeding增加了一个时间上的维度。在clip-based模型里达到了SOTA。







