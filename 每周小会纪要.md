
### 2024å¹´
ç¡®å®šæŠ•ç¨¿æ¥å—åå†æ‰“[x]

- 10.11 å¾èµŸåš ä½•è‰ºè¶…
  - å¾èµŸåš - åˆ†äº«äº†ä¸€ç¯‡generalçš„å¤šæ¨¡æ€å·¥ä½œï¼Œç ”ç©¶é‡åŒ–è§†è§‰ç‰¹å¾çš„èƒ½åŠ›ä»¥åŠä¸ºä¸‹æ¸¸ä»»åŠ¡é€‰æ‹©æœ€ä¼˜è§†è§‰ç‰¹å¾çš„æ–¹å¼ã€‚-[paper](https://openreview.net/pdf?id=SZm3hxmksx)

- 5.31 é™ˆé“¶ å¾èµŸåš
  - é™ˆé“¶-åˆ†äº«äº†ä¸€ç‰‡å­¦æœ¯è®ºæ–‡ï¼Œå…³äºè§†è§‰å¤šä»»åŠ¡ä¸­çš„è·¨æ¨¡æ€å¯¹é½ã€‚ [paper](https://ojs.aaai.org/index.php/AAAI/article/view/29540) [PPT](http://htmlpreview.github.io/?https://github.com/ReadingPapers/Report/blob/main/Slide/20240531_%E5%A4%9A%E6%A8%A1%E6%80%81%E5%AF%B9%E9%BD%90.html)
    
  - [x] æœ¬å‘¨ç¼ºå¸­æˆå‘˜ï¼šä½•è‰ºè¶…ï¼ˆè¯·å‡ï¼‰ï¼Œæœ€ä½³æé—®äººï¼šè‚å»ºæ¶›ï¼Œå¾èµŸåš
  - [x] æœ¬å‘¨çŸ¥ä¹ä¸“æ æŠ•ç¨¿æ–‡ç« : [çŸ¥ä¹](https://zhuanlan.zhihu.com/p/700999452?) é™ˆé“¶
- 5.24	è‚å»ºæ¶›
  
  è‚å»ºæ¶›-åˆ†äº«ä¸€ç¯‡å…³äºæœªé…å¯¹å›¾åƒåˆ°å›¾åƒçš„è½¬æ¢æ–¹æ³•ã€‚[paper](https://openaccess.thecvf.com/content/WACV2023/papers/Torbunov_UVCGAN_UNet_Vision_Transformer_Cycle-Consistent_GAN_for_Unpaired_Image-to-Image_Translation_WACV_2023_paper.pdf),[ppt](https://docs.google.com/presentation/d/1TboboQMHXjPTiplZEZXLOhjKiql68sTc/edit#slide=id.p1)

  è´¾æœ‹-åˆ†äº«äº†ä¸€ç¯‡KANçš„è®ºæ–‡ã€‚[paper](https://arxiv.org/html/2404.19756v1),[ppt](https://docs.google.com/presentation/d/1M1C9UOIrVSs4CR42GfBmOGQ7q9unCNFk/edit?usp=drive_link)

  - [x] æœ¬å‘¨ç¼ºå¸­è®¨è®ºçš„ç»„å‘˜ï¼šæ— ï¼Œæœ¬æ¬¡æœ€ä½³æé—®äººï¼š
  - [x] æœ¬å‘¨çŸ¥ä¹ä¸“æ æŠ•ç¨¿æ–‡ç« :[UVCGANï¼šUNet Vision Transformer cycle-consistent GAN for unpaired image-to-image translation](https://zhuanlan.zhihu.com/p/699968769), è‚å»ºæ¶›   note: æœºç¿»ä¸¥é‡ï¼Œ â€œç§‘å­¦ä»¿çœŸâ€ï¼Œâ€œç§‘å­¦æ¨¡æ‹Ÿâ€ğŸ˜‚
  - [x] æœ¬å‘¨çŸ¥ä¹ä¸“æ æŠ•ç¨¿æ–‡ç« ï¼š[KAN: Kolmogorovâ€“Arnold Networks](https://zhuanlan.zhihu.com/p/699939831) , è´¾æœ‹

- 5.17 äºæ´‹æ™¨  å´”å‡¯\
  å´”å‡¯-åˆ†äº«ä¸€ç¯‡å…³äºå¯¹æ¯”å­¦ä¹ çš„è®ºæ–‡å¹¶æ±‡æŠ¥è¿‘æœŸå·¥ä½œã€‚[PPT](https://docs.google.com/presentation/d/1I3aUK5ntGIGGEVg7OET9vYTDzxRtAPA9/edit#slide=id.p1)

  äºæ´‹æ™¨-åˆ†äº«äº†ä¸€ç¯‡å…³äºMaskedçš„è®ºæ–‡ï¼Œå¹¶å¯¹ä¸€äº›ä»£ç ç»†èŠ‚è¿›è¡Œæ¢è®¨ã€‚[PPT](https://docs.google.com/presentation/d/1I3aUK5ntGIGGEVg7OET9vYTDzxRtAPA9/edit#slide=id.p1)
  
  - [x] æœ¬å‘¨ç¼ºå¸­è®¨è®ºçš„ç»„å‘˜ï¼šç¨‹æµ©ï¼Œæœ¬æ¬¡æœ€ä½³æé—®äººï¼šé™ˆé“¶
  - [x] æœ¬å‘¨çŸ¥ä¹ä¸“æ æŠ•ç¨¿æ–‡ç« : [çŸ¥ä¹](https://zhuanlan.zhihu.com/p/698099502), å´”å‡¯
  - [x] æœ¬å‘¨çŸ¥ä¹ä¸“æ æŠ•ç¨¿æ–‡ç« ï¼š[çŸ¥ä¹](https://zhuanlan.zhihu.com/p/699582594), äºæ´‹æ™¨

- 5.10 å¼ å®‡ ç¨‹æµ©\
  ç¨‹æµ©-æ¢³ç†RNNã€CNNå’Œself-attentionæ¶æ„ï¼Œä»‹ç»SSMæ¨¡å‹ï¼Œè¿›è€Œä»‹ç»MambaåŸæ–‡.[Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://docs.google.com/presentation/d/1F0Co522rU__xKtRG7Zqe7NEAe36qqB4J/edit#slide=id.p1)

  å¼ å®‡-åˆ†äº«äº†ä¸€ç¯‡æ–‡æœ¬è¾…åŠ©è¯†åˆ«å·¥ä½œçš„è®ºæ–‡ï¼Œæ¶‰åŠåˆ°æ–‡æœ¬å’Œå›¾åƒä¸¤ä¸ªæ¨¡æ€ï¼ŒPPT-[2024.5.10-zhangyu-FER-former Multi-modal Transformer for Facial Expression Recognition](https://docs.google.com/presentation/d/15vC7855D_LFRTcQ8N1DpxB_z08ab0gHy/edit?usp=drive_link&ouid=105607574390576704351&rtpof=true&sd=true)
  
  - [x] æœ¬å‘¨ç¼ºå¸­è®¨è®ºçš„ç»„å‘˜ï¼šæ— ï¼Œæœ¬æ¬¡æœ€ä½³æé—®äººï¼šé™ˆé“¶
  - [ ] æœ¬å‘¨çŸ¥ä¹ä¸“æ æŠ•ç¨¿æ–‡ç« : [Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://zhuanlan.zhihu.com/p/697357883), ç¨‹æµ©
  - [x] æœ¬å‘¨çŸ¥ä¹ä¸“æ æŠ•ç¨¿æ–‡ç« ï¼š[è®ºæ–‡é˜…è¯»ï¼šFER-former: Multi-modal Transformer for Facial Expression Recognition](https://zhuanlan.zhihu.com/p/696439106)ï¼Œä½œè€…ï¼šå¼ å®‡

- 4.26 å¾èµŸåšï¼Œå¼ é›ªæ¾   
  å¼ é›ªæ¾-åˆ†äº«ä¸€äº›è®¨è®º[â€œè§†è§‰æ¨¡æ€åœ¨VQAï¼ŒEQAå’ŒVLNç­‰ä»»åŠ¡ä¸Šçš„é‡è¦æ€§â€çš„æ–‡ç« ï¼ŒPPT](https://docs.google.com/presentation/d/11yrPMOLRm0U9azxm2Jb-WfHceKVcVEFG/edit?usp=drive_web&ouid=105694431737350823330&rtpof=true)   
  å¾èµŸåš-ä»‹ç»äº†ä¸€ç¯‡é€šè¿‡å› æœå­¦ä¹ æ¥åšè§†è§‰è¯­è¨€å¯¼èˆªçš„è®ºæ–‡[https://arxiv.org/pdf/2404.10241]ï¼Œé¡ºå¸¦ä»‹ç»äº†åé—¨è°ƒæ•´å’Œå‰é—¨è°ƒæ•´ç­‰ç›¸å…³çš„å› æœæ¨ç†çš„èƒŒæ™¯çŸ¥è¯†ã€‚(https://docs.google.com/presentation/d/1hGlhDh9FRN-nT-h7oFAr_ia06mCyio6r/edit?usp=drive_web&ouid=108523051968570972506&rtpof=true)
  - [x] æœ¬å‘¨ç¼ºå¸­è®¨è®ºçš„ç»„å‘˜ï¼šæ— ï¼Œæœ¬æ¬¡æœ€ä½³æé—®äººï¼šå¼ é›ªæ¾ï¼Œè‚å»ºæ¶›
  - [x] æœ¬å‘¨çŸ¥ä¹ä¸“æ æŠ•ç¨¿æ–‡ç« : [è®ºæ–‡åˆ†äº«ï¼šè§†è§‰æ¨¡æ€åœ¨VQAï¼ŒVLNï¼ŒEQAç­‰ä»»åŠ¡ä¸­çœŸçš„æœ‰ç”¨å—ï¼Ÿ](https://zhuanlan.zhihu.com/p/694329491), å¼ é›ªæ¾
  - [x] æœ¬å‘¨çŸ¥ä¹ä¸“æ æŠ•ç¨¿æ–‡ç« : [åŸºäºå› æœå­¦ä¹ çš„è§†è§‰è¯­è¨€å¯¼èˆª]ï¼ˆhttps://zhuanlan.zhihu.com/p/694377132ï¼‰, å¾èµŸåš
  
- 4.19	é™ˆé“¶ ä½•è‰ºè¶…
  
  é™ˆé“¶-åˆ†äº«ä¸€ç¯‡å…³äºè§†é¢‘è’¸é¦çš„æ–‡ç« ã€‚[paper][https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Fine-Grained_Face_Swapping_via_Regional_GAN_Inversion_CVPR_2023_paper.pdf](https://arxiv.org/pdf/2212.04500.pdf),[ppt](https://docs.google.com/presentation/d/1RBsssFE5lp-Iq2TPvlMXAbftLgv5wvLM/edit?usp=drive_link&ouid=117223158485938146370&rtpof=true&sd=true)

  ä½•è‰ºè¶…-åˆ†äº«ä¸€ç¯‡åŸºäºå¯¹è¯å’Œæƒ…å¢ƒå»ºæ¨¡çš„å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æçš„æ–‡ç« ã€‚[paper](https://arxiv.org/abs/2205.02455),[ppt](https://docs.google.com/presentation/d/1AL8QtHZo965MSr1Mhs6AvvgiAs-iifmP/edit#slide=id.p3)
  - [x] æœ¬å‘¨ç¼ºå¸­è®¨è®ºçš„ç»„å‘˜ï¼šæ— ï¼Œæœ¬æ¬¡æœ€ä½³æé—®äººï¼šç¨‹æµ©
  - [ ] æœ¬å‘¨çŸ¥ä¹ä¸“æ æŠ•ç¨¿æ–‡ç« : [PTH-NET](https://zhuanlan.zhihu.com/p/693939726?), é™ˆé“¶ï¼Œæœªæ”¶åˆ°æŠ•ç¨¿
  - [x] æœ¬å‘¨çŸ¥ä¹ä¸“æ æŠ•ç¨¿æ–‡ç« ï¼š[COGMEN](https://zhuanlan.zhihu.com/p/692989727)ï¼Œä½•è‰ºè¶…
 
      
- 4.12	è‚å»ºæ¶›
  
  è‚å»ºæ¶›-åˆ†äº«ä¸€ç¯‡å…³äºåŒºåŸŸGANåæ¼”(RGI)æ–¹æ³•åœ¨ç»†ç²’åº¦çº§åˆ«çš„æ¢è„¸ã€‚[paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Fine-Grained_Face_Swapping_via_Regional_GAN_Inversion_CVPR_2023_paper.pdf),[ppt](https://docs.google.com/presentation/d/18mptJJIrZ_3WrVLHL9qSd5Bf4XcPop8u/edit#slide=id.p5)

  - [x] æœ¬å‘¨ç¼ºå¸­è®¨è®ºçš„ç»„å‘˜ï¼šæ— ï¼Œæœ¬æ¬¡æœ€ä½³æé—®äººï¼šé™ˆé“¶
  - [x] æœ¬å‘¨çŸ¥ä¹ä¸“æ æŠ•ç¨¿æ–‡ç« :[Fine-Grained Face Swapping via Regional GAN Inversion](https://zhuanlan.zhihu.com/p/693977227?), è‚å»ºæ¶›

- 4.05	å´”å‡¯ ç‹ä¼šé›…
  
  å´”å‡¯-åˆ†äº«ä¸€ç‰‡è®ºæ–‡å’Œè¿‘æœŸçš„ç ”ç©¶ã€‚è®ºæ–‡ä¸»è¦å†…å®¹æ˜¯åŸºäºè„‘ç”µä¿¡å·çš„åŠ¨æ€ä¸ç¡®å®šæ€§å’Œä¸å¯¹ç§°æ€§å»è®¾è®¡æ¨¡å‹ã€Š[Tsception](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9762054)ã€‹,[ppt](https://docs.google.com/presentation/d/13yxf6I43vWx6IluOKf6ajXWniQ4kv8PR/edit#slide=id.p1)\
  ç‹ä¼šé›…-è®ºæ–‡é˜…è¯»åˆ†äº«ï¼Œè®ºæ–‡æå‡ºäº†ä¸¤ç§å¾®è¡¨æƒ…è¯†åˆ«æ–¹æ³•[Spontaneous Facial Micro-Expression Recognition using 3D Spatiotemporal Convolutional Neural Networks.pptx](https://docs.google.com/presentation/d/1rVP3C3CYJbvDkcPxy-fRLl9Yxp38Sblt/edit#slide=id.p1)

  - [x] æœ¬å‘¨ç¼ºå¸­è®¨è®ºçš„ç»„å‘˜ï¼šæ— ï¼Œæœ¬æ¬¡æœ€ä½³æé—®äººï¼š
  - [x] æœ¬å‘¨çŸ¥ä¹ä¸“æ æŠ•ç¨¿æ–‡ç« :[MAViL: Masked Audio-Video Learners](https://zhuanlan.zhihu.com/p/690067209),ç¨‹æµ©

- 3.29	ç¨‹æµ©
  
  åˆ†äº«æœ€è¿‘ç ”ç©¶è¿›å±•ï¼Œå’Œä¸¤ç¯‡è®ºæ–‡ã€‚[Mar 29 2024-chenghao-Multimodal Emotion Classification through A-V-T Self-Supervised Pre-training](https://docs.google.com/presentation/d/1BCs9gJBQT6WDJNJvgFemcnAdVXwj420-/edit#slide=id.p1)
  - [x] æœ¬å‘¨ç¼ºå¸­è®¨è®ºçš„ç»„å‘˜ï¼šæ— ï¼Œæœ¬æ¬¡æœ€ä½³æé—®äººï¼šé™ˆé“¶
  - [x] æœ¬å‘¨çŸ¥ä¹ä¸“æ æŠ•ç¨¿æ–‡ç« :æ ‡é¢˜ã€Š[TSception-Capturing Temporal Dynamics and Spatial Asymmetry From EEG for Emotion Recognition](https://zhuanlan.zhihu.com/p/689915437)ã€‹,ä½œè€…ï¼šå´”å‡¯

- 3.22	å¼ å®‡  å¼ é›ªæ¾

   å¼ å®‡-åˆ†äº«è®ºæ–‡é˜…è¯»ï¼Œé€šè¿‡åˆ©ç”¨æ ·æœ¬çº§æ•°æ®å¾®è°ƒCLIPå®ç°DFERçš„zero-shotã€‚[2024.3.22-zhangyu-EmoCLIPï¼šA Vision-Language Method for Zero-Shot Video Facial Expression Recognition.pptx](https://docs.google.com/presentation/d/1975ZbUKVx5SspaQMcvliktTzeW3Oj72H/edit#slide=id.p19)\
   å¼ é›ªæ¾-åˆ†äº«ä¸€ç¯‡[VLNè®ºæ–‡](https://zhuanlan.zhihu.com/p/687858582)é˜…è¯»ã€‚[2024.3.22-zhangxuesong-è§†è§‰è¯­è¨€å¯¼èˆªè®ºæ–‡ç®€è¯»ä¸å®éªŒè¿›å±•åˆ†äº«](https://docs.google.com/presentation/d/11n3r1AjDne3mC-SHHOa5dtCaxiVamIoD/edit#slide=id.p10)
  - [x] æœ¬å‘¨ç¼ºå¸­è®¨è®ºçš„ç»„å‘˜ï¼šæ— ï¼Œæœ¬æ¬¡æœ€ä½³æé—®äººï¼šç¨‹æµ©
  - [x] æœ¬å‘¨çŸ¥ä¹ä¸“æ æŠ•ç¨¿æ–‡ç« ï¼š[è§†è§‰è¯­è¨€å¯¼èˆªï¼ˆå‡å°‘å¯¼èˆªä¸­SRå’ŒOSRçš„å·®è·ï¼‰](https://zhuanlan.zhihu.com/p/687858582)ï¼Œä½œè€…ï¼šå¼ é›ªæ¾
  - [x] æœ¬å‘¨çŸ¥ä¹ä¸“æ æŠ•ç¨¿æ–‡ç« ï¼š[è®ºæ–‡é˜…è¯» EmoCLIP: A Vision-Language Method for Zero-Shot Video Facial Expression Recognition](https://zhuanlan.zhihu.com/p/688058784)ï¼Œä½œè€…ï¼šå¼ å®‡
                
- 3.15	é™ˆé“¶

  æ±‡æŠ¥æœ€è¿‘ç ”ç©¶è¿›å±•ï¼Œå¦‚ä½•å€ŸåŠ© Image+Vdieo çš„ pretrain æå‡ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚[20240311-chenyin_[Image+Video pretraining for Facial Expression Recogniiton].pptx](https://docs.google.com/presentation/d/1Dz8lLmPg-xF-MkoRCYIs4Ug-gJk4tDBm/edit#slide=id.p23)
  - [ ] æœ¬å‘¨ç¼ºå¸­è®¨è®ºçš„ç»„å‘˜ï¼šæ— ï¼Œæœ¬æ¬¡æœ€ä½³æé—®äººï¼š
  - [x] æœ¬å‘¨çŸ¥ä¹ä¸“æ æŠ•ç¨¿æ–‡ç« ï¼šæ ‡é¢˜ã€Š[A3lign-DFER: Pioneering Comprehensive Dynamic Affective Alignment for Dynamic Facial Expression Recognition with CLIP](https://zhuanlan.zhihu.com/p/686840722ï¼‰ã€‹ï¼Œä½œè€…ï¼šé™ˆé“¶
        
åˆ†å‰²çº¿ ---å·²æ‰§è¡Œï¼ˆæ—¶é—´å€’åºï¼‰ï¼Œå¾…æ‰§è¡Œï¼ˆæ—¶é—´é¡ºåºï¼‰--- åˆ†å‰²çº¿
---

  
  
-  5.3	ç¨‹æµ© å¼ å®‡

-  5.10 å´”å‡¯ äºæ´‹æ™¨
  
-  5.17	è‚å»ºæ¶› è´¾æœ‹
  
-  5.24	é™ˆé“¶ ä½•è‰ºè¶…
  
-  5.31	å¼ é›ªæ¾ å¾èµŸåš
  
-  6.7	ç¨‹æµ© å¼ å®‡
  
-  6.14	å´”å‡¯ äºæ´‹æ™¨
  
-  6.21	è‚å»ºæ¶› è´¾æœ‹
  
-  6.28	é™ˆé“¶ ä½•è‰ºè¶…
  
-  7.5	å¼ é›ªæ¾ å¾èµŸåš
  
-  7.12	ç¨‹æµ© å¼ å®‡
  
-  7.19	å´”å‡¯ äºæ´‹æ™¨
  
-  7.28	è‚å»ºæ¶› è´¾æœ‹
