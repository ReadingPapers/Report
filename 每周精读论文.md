
######### 模版 #############

> [日期]-[总结人]-[论文标题]-[来源]（附带链接，链接可选）
>  
> > 论文框架图（有助于一眼就能想起论文内容）
> 
> > 论文简述 （一两句话总结精华，切勿过长）
> > 
######### 模版 #############


> [2023-3-13]-[陈银]-[A $^3$ lign-DFER: Pioneering Comprehensive Dynamic Affective Alignment for Dynamic Facial Expression Recognition with CLIP]-[Arxiv](https://arxiv.org/abs/2403.04294) [知乎](https://zhuanlan.zhihu.com/p/686840722)
>  

> > 论文简述：
> > 
> > 在CLIP的基础上，提出多个模块，从affective, dynamic和bidirectional三个角度实现了动态情感对齐，达到了较高的performance。相比于之前的prompt learning ， 把text embeding增加了一个时间上的维度。在clip-based模型里达到了SOTA。


>  [2023-3-17]-[崔凯]-[TSception: Capturing Temporal Dynamics and Spatial Asymmetry From EEG for Emotion Recognition]-[ieee.org](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9762054)
>

> > 论文简述
> >
> > 针对EEG信号的high temporal resolution和the asymmetric spatial activations属性，作者设计了具有多尺度卷积核Dynamic Temporal Layer以及Asymmetric Spatial Layer，以此来捕捉EEG的时间动态性和空间不对称性，实现更准确和更具备泛化的情绪识别。

> [2023-3-19]-[程浩]-[Self-supervised Cross-modal Pretraining for Speech Emotion Recognition and Sentiment Analysis]-[EMNLP 2022](https://aclanthology.org/2022.findings-emnlp.375.pdf)
>
> >论文简述:\
> >利用Text modality现成的一个类Bert但更genral的模型RoBERTa和Acoustic modality中一个Genral的模型HuBERT分别作为两个模态的Embedding, 接着在特征层面进行Mask and Reconstruction的学习，模型对于两个模态分别设计了两个Transformer模块，并利用Cross-attention机制进行模态间的特征融合，最终在IEMOCAP和MOSEI两个数据集上达到了SOTA。

> [2023-3-20]-[张宇]-[EmoCLIP: A Vision-Language Method for Zero-Shot Video Facial Expression Recognition]-[Arxiv]([https://arxiv.org/abs/2403.04294][知乎]([https://zhuanlan.zhihu.com/p/686840722](https://zhuanlan.zhihu.com/p/688058784))
>
> >论文简述:
> >在CLIP的基础上进行改进实现对DFER进行zero-shot，加入ViT模型模拟时间维度，首个在DFER上提出使用样本级视频-文本数据集进行训练，在其余流行的DFER数据集上进行zero-shot进行测试，区别于VLM在DFER上CLIPER和DFER-CLIP的类级学习范式，微调了CLIP的image端和text端，使其对人脸表情以及表情描述文本更加敏感。
